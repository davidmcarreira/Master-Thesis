\documentclass[class=report, crop=false, a4paper, 12pt]{standalone}

%Packages import
\usepackage{../pkgs}


\begin{document}
\section{FaceNet}
----> Introduction
System that directly learns a mapping from face images to a compact \red{Euclidean space} where distances correspond to a measure of face similarities.
 
Performs face recognition, verification and clustering with standard techniques with \red{Facenet embeddings as feature vectors}.

Solves the problem of illumination and pose variance.

The model architecture consists of a batch input, a deep CNN followed by L2 normalization (modifies the values in a way that the sum of the squares will be always up to 1, also called least squares), resulting in the face embedding. This is followed by the triplet loss during training.

This article has proofs that what distinguishes a face recognition method from a verification method is the treatment of the output.


----> Methods
- They use a DCNN and based the methods in 2 architectures: Zeiler&Fergus and Inception.

- Most important part is the end-to-end learning of the whole system.

- The model strives to embed an image into a feature space, such that the squared distance between \textbf{all faces of the same identity} is small, independently of the imaging conditions (very important), whereas different identities are further.

- Failure point, they didn't compare to other losses

- Facenet contributes with a method for triplet selection, since generating all possible triplets would have a great amount of results that would satisfy the constraint (Eq. 1) but would still pass through the network, generating overhead and slower convergence.


----> Triplet selection
- To ensure a fast convergence it is crucial to select triplets that violate the triplet constraint by selecting anchors and hard positives such as they satisfy the argmax of the square of distances and similarly hard negatives in order to verify the argmin of the same metric.

- Computing the argmin and argmax across the whole data set is not feasible (big amount of data). Additionally, it might lead to poor training, as mislabelled and poorly imaged faces would dominate hard positives and negatives. 

- Two obvious choices: offline (select triplets every n steps and computing the argmin and argmax)  and online (selecting within a mini-batch) generation

- The article focus on online generation usign large mini-batches

-------


- Dimensionality - number of input variables or features for a dataset. This Article (https://www.sciencedirect.com/science/article/abs/pii/0031320390900089) also defines it as the number of connections (i.e weights) and number of input units.

- Embeddings - In general, embedding is the mapping of a discrete/categorical variable to a vector of continuous numbers. Neural network embeddings are a low-dimensional space into which high-dimensional categorical/discrete vectors can be translated to. Neural networks embeddings are low dimensional, learned continuous vector representations of discrete variables. It places similar inputs close together in the embedding space. They reduce the dimensionality of categorical variables and solve the one hot encoding limitations (each category has a one hot encoding, therefore, high dimensionality; it doesn't place similar entities closer to another in the vector space; and the main issue is that the transformation does not rely on any supervisions). The embeddings are learned through supervised training and form the weights of the network, which are adjusted to minimize the loss on the task.

Bottleneck layer - layer with less nodes than the previous one. It's used to reduce the dimensionality of the feature maps in the case of the CNN or the input features in general.

Parameters - what is learned by the model (weights and biases, weights of linear and logistic regressions or cluster centroid in clustering)

Feature maps, representations and channels are equivalent terms just as kernel, filter and mask are synonymous of each other.

Holdout dataset (also called test set, hold out or hold-out) - the purpose of it is to provide an unbiased estimate of the model performance  after it has been trained and validated.

Disjoint sets have no elements in common

This article is a good reference to prove that LFW is the de-facto academic test for face verification.

ROC (Receiver Operating Characteristic) curve - plots Validation Rate (True Positive Rate or Recall) vs False Acceptance Rate (False Positive Rate) and shows the performance at all classification thresholds.

-------

Large-margin softmax loss
    - Angular softmax loss (SphereFace)
    - Large margin cosine loss (CosFDdace)
    - Additive angular margin loss (Arcface)
    - Fair loss (?)




--------
\section{QMagFace}
Face recognition systems have to deal with large variabilities, such as changes in illumination, pose, expressions.

Previous works either do not employ face image quality information during comparison or include quality estimates that are not adequate for the comparison process task. In the first case, there's a loss of valuable information while in the second one the limiting factor is the employed estimates for the quality of the pictures.

QMagFace combines a quality-aware comparison function with MagFace loss (magnitude-aware angular margin loss). 

The authors of the article experimented over 4 face recognition databases and 6 benchmarks.
\end{document}