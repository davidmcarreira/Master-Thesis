@article{4308320,
  title = {Polynomial Theory of Complex Systems},
  author = {Ivakhnenko, A. G.},
  year = {1971},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics},
  volume = {SMC-1},
  number = {4},
  pages = {364--378},
  doi = {10.1109/TSMC.1971.4308320}
}

@article{6795724,
  title = {Backpropagation Applied to Handwritten Zip Code Recognition},
  author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
  year = {1989},
  journal = {Neural Computation},
  volume = {1},
  number = {4},
  pages = {541--551},
  doi = {10.1162/neco.1989.1.4.541}
}

@article{ansariHumanDetectionTechniques2021,
  title = {Human Detection Techniques for Real Time Surveillance: A Comprehensive Survey},
  shorttitle = {Human Detection Techniques for Real Time Surveillance},
  author = {Ansari, Mohd. Aquib and Singh, Dushyant Kumar},
  year = {2021},
  month = mar,
  journal = {Multimedia Tools and Applications},
  volume = {80},
  number = {6},
  pages = {8759--8808},
  issn = {1380-7501, 1573-7721},
  doi = {10.1007/s11042-020-10103-4},
  abstract = {Real-time detection of humans is an evolutionary research topic. It is an essential and prominent component of various vision based applications. Detection of humans in real-time video sequences is an arduous and challenging task due to various constraints like cluttered environment, occlusion, noise, etc. Many researchers are doing their research in this area and have published the number of researches so far. Determining humans in visual monitoring system is prominent for different types of applications like person detection and identification, fall detection for an elder person, abnormal surveillance, gender classification, crowd analysis, person gait characterization, etc. The main objective of this paper is to provide a comprehensive survey of the various challenges and modern developments seen for human detection methodologies in day vision. This paper consists of an overview of different human detection techniques and their classification based on various underlying factors. The algorithmic technicalities with their applicability to these techniques are deliberated in detail in the manuscript. Different humanitarian imperative factors have also been highlighted for comparative analysis of each human detection methodology. Our survey shows the difference between current research and future requirements.},
  langid = {english},
  file = {/home/david/Zotero/storage/TN63GH4G/Ansari and Singh - 2021 - Human detection techniques for real time surveilla.pdf}
}

@article{chellapillaHighPerformanceConvolutional,
  title = {High {{Performance Convolutional Neural Networks}} for {{Document Processing}}},
  author = {Chellapilla, Kumar and Puri, Sidd and Simard, Patrice},
  abstract = {Convolutional neural networks (CNNs) are well known for producing state-of-the-art recognizers for document processing [1]. However, they can be difficult to implement and are usually slower than traditional multi-layer perceptrons (MLPs). We present three novel approaches to speeding up CNNs: a) unrolling convolution, b) using BLAS (basic linear algebra subroutines), and c) using GPUs (graphic processing units). Unrolled convolution converts the processing in each convolutional layer (both forward-propagation and back-propagation) into a matrix-matrix product. The matrix-matrix product representation of CNNs makes their implementation as easy as MLPs. BLAS is used to efficiently compute matrix products on the CPU. We also present a pixel shader based GPU implementation of CNNs. Results on character recognition problems indicate that unrolled convolution with BLAS produces a dramatic 2.4X-3.0X speedup. The GPU implementation is even faster and produces a 3.1X-4.1X speedup.},
  langid = {english},
  file = {/home/david/Zotero/storage/ZHBI9YQT/Chellapilla et al. - High Performance Convolutional Neural Networks for.pdf}
}

@inproceedings{ciresanCommitteeNeuralNetworks2011,
  title = {A Committee of Neural Networks for Traffic Sign Classification},
  booktitle = {The 2011 {{International Joint Conference}} on {{Neural Networks}}},
  author = {Cire{\c s}an, Dan and Meier, Ueli and Masci, Jonathan and Schmidhuber, J{\"u}rgen},
  year = {2011},
  month = jul,
  pages = {1918--1921},
  issn = {2161-4407},
  doi = {10.1109/IJCNN.2011.6033458},
  abstract = {We describe the approach that won the preliminary phase of the German traffic sign recognition benchmark with a better-than-human recognition rate of 98.98\%.We obtain an even better recognition rate of 99.15\% by further training the nets. Our fast, fully parameterizable GPU implementation of a Convolutional Neural Network does not require careful design of pre-wired feature extractors, which are rather learned in a supervised way. A CNN/MLP committee further boosts recognition performance.},
  keywords = {Biological neural networks,Convolutional codes,Error analysis,Image color analysis,Kernel,Neurons,Training},
  file = {/home/david/Zotero/storage/9VGNNCCB/Cireşan et al. - 2011 - A committee of neural networks for traffic sign cl.pdf;/home/david/Zotero/storage/8QJLFKCB/stamp.html}
}

@article{ciresanDeepBigSimple2010,
  title = {Deep, {{Big}}, {{Simple Neural Nets}} for {{Handwritten Digit Recognition}}},
  author = {Cire{\c s}an, Dan Claudiu and Meier, Ueli and Gambardella, Luca Maria and Schmidhuber, J{\"u}rgen},
  year = {2010},
  month = dec,
  journal = {Neural Computation},
  volume = {22},
  number = {12},
  pages = {3207--3220},
  issn = {0899-7667},
  doi = {10.1162/NECO_a_00052},
  abstract = {Good old online backpropagation for plain multilayer perceptrons yields a very low 0.35\% error rate on the MNIST handwritten digits benchmark. All we need to achieve this best result so far are many hidden layers, many neurons per layer, numerous deformed training images to avoid overfitting, and graphics cards to greatly speed up learning.}
}

@article{ciresanDeepNeuralNetworks2012,
  title = {Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images},
  author = {Ciresan, D.C. and Giusti, A. and Gambardella, L.M. and Schmidhuber, J.},
  year = {2012},
  journal = {NIPS},
  volume = {25},
  pages = {2852--2860},
  note = {Export Date: 26 January 2023; Cited By: 92}
}

@inproceedings{ciresanMitosisDetectionBreast2013,
  title = {Mitosis {{Detection}} in {{Breast Cancer Histology Images}} with {{Deep Neural Networks}}},
  booktitle = {Medical {{Image Computing}} and {{Computer-Assisted Intervention}} \textendash{} {{MICCAI}} 2013},
  author = {Cire{\c s}an, Dan C. and Giusti, Alessandro and Gambardella, Luca M. and Schmidhuber, J{\"u}rgen},
  editor = {Mori, Kensaku and Sakuma, Ichiro and Sato, Yoshinobu and Barillot, Christian and Navab, Nassir},
  year = {2013},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {411--418},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-40763-5_51},
  abstract = {We use deep max-pooling convolutional neural networks to detect mitosis in breast histology images. The networks are trained to classify each pixel in the images, using as context a patch centered on the pixel. Simple postprocessing is then applied to the network output. Our approach won the ICPR 2012 mitosis detection competition, outperforming other contestants by a significant margin.},
  isbn = {978-3-642-40763-5},
  langid = {english},
  keywords = {Convolutional Neural Network,Deep Neural Network,Ground Truth,Input Image,Mitotic Nucleus},
  file = {/home/david/Zotero/storage/2XVR4SD2/Cireşan et al. - 2013 - Mitosis Detection in Breast Cancer Histology Image.pdf}
}

@article{Dreyfus1973383,
  type = {Article},
  title = {The Computational Solution of Optimal Control Problems with Time Lag},
  author = {Dreyfus, Stuart E.},
  year = {1973},
  journal = {IEEE Transactions on Automatic Control},
  volume = {18},
  number = {4},
  pages = {383--385},
  doi = {10.1109/TAC.1973.1100330},
  publication_stage = {Final},
  source = {Scopus},
  note = {Cited by: 32}
}

@article{elizondoLinearSeparabilityProblem2006,
  title = {The {{Linear Separability Problem}}: {{Some Testing Methods}}},
  shorttitle = {The {{Linear Separability Problem}}},
  author = {Elizondo, D.},
  year = {2006},
  month = mar,
  journal = {IEEE Transactions on Neural Networks},
  volume = {17},
  number = {2},
  pages = {330--344},
  issn = {1045-9227},
  doi = {10.1109/TNN.2005.860871},
  abstract = {The notion of linear separability is used widely in machine learning research. Learning algorithms that use this concept to learn include neural networks (single layer perceptron and recursive deterministic perceptron), and kernel machines (support vector machines). This paper presents an overview of several of the methods for testing linear separability between two classes. The methods are divided into four groups: Those based on linear programming, those based on computational geometry, one based on neural networks, and one based on quadratic programming. The Fisher linear discriminant method is also presented. A section on the quantification of the complexity of classification problems is included.},
  langid = {english},
  file = {/home/david/Zotero/storage/JX227D5E/Elizondo - 2006 - The Linear Separability Problem Some Testing Meth.pdf}
}

@article{farleySimulationSelforganizingSystems1954,
  title = {Simulation of Self-Organizing Systems by Digital Computer},
  author = {Farley, B. and Clark, W.},
  year = {1954},
  journal = {Transactions of the IRE Professional Group on Information Theory},
  volume = {4},
  number = {4},
  pages = {76--84},
  doi = {10.1109/TIT.1954.1057468}
}

@article{haenleinBriefHistoryArtificial2019,
  title = {A {{Brief History}} of {{Artificial Intelligence}}: {{On}} the {{Past}}, {{Present}}, and {{Future}} of {{Artificial Intelligence}}},
  shorttitle = {A {{Brief History}} of {{Artificial Intelligence}}},
  author = {Haenlein, Michael and Kaplan, Andreas},
  year = {2019},
  month = jul,
  journal = {California Management Review},
  volume = {61},
  pages = {000812561986492},
  doi = {10.1177/0008125619864925},
  abstract = {This introduction to this special issue discusses artificial intelligence (AI), commonly defined as ``a system's ability to interpret external data correctly, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation.'' It summarizes seven articles published in this special issue that present a wide variety of perspectives on AI, authored by several of the world's leading experts and specialists in AI. It concludes by offering a comprehensive outlook on the future of AI, drawing on micro-, meso-, and macro-perspectives.},
  file = {/home/david/Zotero/storage/GFIB753Q/Haenlein and Kaplan - 2019 - A Brief History of Artificial Intelligence On the.pdf}
}

@book{hebbOrganizationBehaviorNeuropsychological1949,
  title = {The {{Organization}} of {{Behavior}}: {{A Neuropsychological Theory}}},
  shorttitle = {The {{Organization}} of {{Behavior}}},
  author = {Hebb, Donald Olding},
  year = {1949},
  publisher = {{Wiley}},
  abstract = {Description du ph\'enom\`ene neuropsychologique r\'egissant le laps de temps entre le stimulus et la r\'eponse, le neurophysiologique et le psychologique. Que ce passe-t-il durant cet interval, dans le cervau humain?},
  googlebooks = {dZ0eDiLTwuEC},
  isbn = {978-0-471-36727-7},
  langid = {english}
}

@article{hochreiterLongShortTermMemory1997,
  title = {Long {{Short-Term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  month = nov,
  journal = {Neural Computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  issn = {0899-7667},
  doi = {10.1162/neco.1997.9.8.1735},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}
}

@article{ivakhnenkoCyberneticPredictingDevices,
  title = {Cybernetic {{Predicting Devices}}},
  author = {Ivakhnenko, A G and Lapa, V G},
  langid = {english},
  file = {/home/david/Zotero/storage/5DSPZAUF/Ivakhnenko and Lapa - Cybernetic Predicting Devices.pdf}
}

@book{josephContributionsPerceptronTheory1960,
  title = {Contributions to {{Perceptron Theory}}},
  author = {Joseph, Roger David},
  year = {1960},
  publisher = {{Cornell Aeronautical Laboratory}},
  googlebooks = {O9JUAAAAYAAJ},
  langid = {english}
}

@inproceedings{krizhevskyImageNetClassificationDeep2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  year = {2012},
  volume = {25},
  publisher = {{Curran Associates, Inc.}},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7\textbackslash\% and 18.9\textbackslash\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
  file = {/home/david/Zotero/storage/PI6EC7PZ/Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf}
}

@article{lecunDeepLearning2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year = {2015},
  month = may,
  journal = {Nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature14539},
  langid = {english},
  file = {/home/david/Zotero/storage/FUTB2UE9/nature14539.pdf}
}

@article{lecunGradientBasedLearningApplied1998,
  title = {Gradient-{{Based Learning Applied}} to {{Document Recognition}}},
  author = {LeCun, Yann and Bottou, Leon and Bengio, Yoshua and Ha, Patrick},
  year = {1998},
  langid = {english},
  file = {/home/david/Zotero/storage/VQIWJ4R7/LeCun et al. - 1998 - Gradient-Based Learning Applied to Document Recogn.pdf}
}

@phdthesis{linnainmaa1970representation,
  title = {The Representation of the Cumulative Rounding Error of an Algorithm as a {{Taylor}} Expansion of the Local Rounding Errors},
  author = {Linnainmaa, Seppo},
  year = {1970},
  school = {Master's Thesis (in Finnish), Univ. Helsinki}
}

@misc{liSurveyConvolutionalNeural2020,
  title = {A {{Survey}} of {{Convolutional Neural Networks}}: {{Analysis}}, {{Applications}}, and {{Prospects}}},
  shorttitle = {A {{Survey}} of {{Convolutional Neural Networks}}},
  author = {Li, Zewen and Yang, Wenjie and Peng, Shouheng and Liu, Fan},
  year = {2020},
  month = apr,
  number = {arXiv:2004.02806},
  eprint = {2004.02806},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  abstract = {Convolutional Neural Network (CNN) is one of the most significant networks in the deep learning field. Since CNN made impressive achievements in many areas, including but not limited to computer vision and natural language processing, it attracted much attention both of industry and academia in the past few years. The existing reviews mainly focus on the applications of CNN in different scenarios without considering CNN from a general perspective, and some novel ideas proposed recently are not covered. In this review, we aim to provide novel ideas and prospects in this fast-growing field as much as possible. Besides, not only two-dimensional convolution but also one-dimensional and multi-dimensional ones are involved. First, this review starts with a brief introduction to the history of CNN. Second, we provide an overview of CNN. Third, classic and advanced CNN models are introduced, especially those key points making them reach state-of-the-art results. Fourth, through experimental analysis, we draw some conclusions and provide several rules of thumb for function selection. Fifth, the applications of one-dimensional, two-dimensional, and multi-dimensional convolution are covered. Finally, some open issues and promising directions for CNN are discussed to serve as guidelines for future work.},
  archiveprefix = {arXiv},
  keywords = {68T07 (Primary) 68T10; 68T45 (Secondary),Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,I.2.10,I.2.6},
  note = {Comment: 21 pages, 33 figures, journal},
  file = {/home/david/Zotero/storage/C6FDB9FZ/Li et al. - 2020 - A Survey of Convolutional Neural Networks Analysi.pdf;/home/david/Zotero/storage/6PS2WQMA/2004.html}
}

@article{mccarthyPROPOSALDARTMOUTHSUMMER,
  title = {A {{PROPOSAL FOR THE DARTMOUTH SUMMER RESEARCH PROJECT ON ARTIFICIAL INTELLIGENCE}}},
  author = {McCarthy, J and Minsky, M L and Rochester, N and Corporation, I B M and Shannon, C E},
  langid = {english},
  file = {/home/david/Zotero/storage/3BPHNKVQ/McCarthy et al. - A PROPOSAL FOR THE DARTMOUTH SUMMER RESEARCH PROJE.pdf}
}

@article{mccullochLOGICALCALCULUSIDEAS,
  title = {A {{LOGICAL CALCULUS OF THE IDEAS IMMANENT IN NERVOUS ACTIVITY}}},
  author = {Mcculloch, Warren S and Pitts, Walter},
  langid = {english},
  file = {/home/david/Zotero/storage/7MBILWB3/Mcculloch and Pitts - A LOGICAL CALCULUS OF THE IDEAS IMMANENT IN NERVOU.pdf}
}

@book{minsky69perceptrons,
  title = {Perceptrons: {{An}} Introduction to Computational Geometry},
  author = {Minsky, Marvin and Papert, Seymour},
  year = {1969},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA, USA}},
  added-at = {2008-05-16T13:57:01.000+0200},
  description = {: mf : blob : \guillemotright{} bibtex},
  interhash = {d80d4948a422623047f1b800272c0389},
  intrahash = {06a5a6751b3e61408455fca2ed8d87fc},
  keywords = {linear-classification neural-networks seminal},
  timestamp = {2008-05-16T13:57:02.000+0200}
}

@inproceedings{newell1959report,
  title = {Report on a General Problem Solving Program},
  booktitle = {{{IFIP}} Congress},
  author = {Newell, Allen and Shaw, John C and Simon, Herbert A},
  year = {1959},
  volume = {256},
  pages = {64},
  organization = {{Pittsburgh, PA}}
}

@article{ohGPUImplementationNeural2004,
  title = {{{GPU}} Implementation of Neural Networks},
  author = {Oh, Kyoung-Su and Jung, Keechul},
  year = {2004},
  month = jun,
  journal = {Pattern Recognition},
  volume = {37},
  number = {6},
  pages = {1311--1314},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2004.01.013},
  abstract = {Graphics processing unit (GPU) is used for a faster artificial neural network. It is used to implement the matrix multiplication of a neural network to enhance the time performance of a text detection system. Preliminary results produced a 20-fold performance enhancement using an ATI RADEON 9700 PRO board. The parallelism of a GPU is fully utilized by accumulating a lot of input feature vectors and weight vectors, then converting the many inner-product operations into one matrix operation. Further research areas include benchmarking the performance with various hardware and GPU-aware learning algorithms.},
  keywords = {Graphics processing unit(GPU),Multi-layer perceptron,Neural network(NN),Text detection}
}

@inproceedings{rainaLargescaleDeepUnsupervised2009b,
  title = {Large-Scale Deep Unsupervised Learning Using Graphics Processors},
  booktitle = {Proceedings of the 26th {{Annual International Conference}} on {{Machine Learning}}},
  author = {Raina, Rajat and Madhavan, Anand and Ng, Andrew Y.},
  year = {2009},
  month = jun,
  series = {{{ICML}} '09},
  pages = {873--880},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1553374.1553486},
  abstract = {The promise of unsupervised learning methods lies in their potential to use vast amounts of unlabeled data to learn complex, highly nonlinear models with millions of free parameters. We consider two well-known unsupervised learning models, deep belief networks (DBNs) and sparse coding, that have recently been applied to a flurry of machine learning applications (Hinton \& Salakhutdinov, 2006; Raina et al., 2007). Unfortunately, current learning algorithms for both models are too slow for large-scale applications, forcing researchers to focus on smaller-scale models, or to use fewer training examples. In this paper, we suggest massively parallel methods to help resolve these problems. We argue that modern graphics processors far surpass the computational capabilities of multicore CPUs, and have the potential to revolutionize the applicability of deep unsupervised learning methods. We develop general principles for massively parallelizing unsupervised learning tasks using graphics processors. We show that these principles can be applied to successfully scaling up learning algorithms for both DBNs and sparse coding. Our implementation of DBN learning is up to 70 times faster than a dual-core CPU implementation for large models. For example, we are able to reduce the time required to learn a four-layer DBN with 100 million free parameters from several weeks to around a single day. For sparse coding, we develop a simple, inherently parallel algorithm, that leads to a 5 to 15-fold speedup over previous methods.},
  isbn = {978-1-60558-516-1},
  file = {/home/david/Zotero/storage/QDRZR76Z/Raina et al. - 2009 - Large-scale deep unsupervised learning using graph.pdf}
}

@inproceedings{ranzatoEfficientLearningSparse2006,
  title = {Efficient {{Learning}} of {{Sparse Representations}} with an {{Energy-Based Model}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {aurelio Ranzato, Marc' and Poultney, Christopher and Chopra, Sumit and Cun, Yann},
  year = {2006},
  volume = {19},
  publisher = {{MIT Press}},
  abstract = {We describe a novel unsupervised method for learning sparse, overcomplete features. The model uses a linear encoder, and a linear decoder preceded by a sparsifying non-linearity that turns a code vector into a quasi-binary sparse code vector. Given an input, the optimal code minimizes the distance between the output of the decoder and the input patch while being as similar as possible to the encoder output. Learning proceeds in a two-phase EM-like fashion: (1) compute the minimum-energy code vector, (2) adjust the parameters of the encoder and decoder so as to decrease the energy. The model produces "stroke detectors" when trained on handwritten numerals, and Gabor-like filters when trained on natural image patches. Inference and learning are very fast, requiring no preprocessing, and no expensive sampling. Using the proposed unsupervised method to initialize the first layer of a convolutional network, we achieved an error rate slightly lower than the best reported result on the MNIST dataset. Finally, an extension of the method is described to learn topographical filter maps.},
  file = {/home/david/Zotero/storage/ZAKI3IQG/Ranzato et al. - 2006 - Efficient Learning of Sparse Representations with .pdf}
}

@article{rochesterTestsCellAssembly1956,
  title = {Tests on a Cell Assembly Theory of the Action of the Brain, Using a Large Digital Computer},
  author = {Rochester, N. and Holland, J. and Haibt, L. and Duda, W.},
  year = {1956},
  journal = {IRE Transactions on Information Theory},
  volume = {2},
  number = {3},
  pages = {80--93},
  doi = {10.1109/TIT.1956.1056810}
}

@article{rosenblattPerceptronProbabilisticModel1958,
  title = {The Perceptron: {{A}} Probabilistic Model for Information Storage and Organization in the Brain.},
  author = {Rosenblatt, F.},
  year = {1958},
  journal = {Psychological Review},
  volume = {65},
  pages = {386--408},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1471(Electronic),0033-295X(Print)},
  doi = {10.1037/h0042519},
  abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {*Brain,*Cognition,*Memory,Nervous System}
}

@book{rosenblattPrinciplesNeurodynamicsPerceptrons1962,
  title = {Principles of {{Neurodynamics}}: {{Perceptrons}} and the {{Theory}} of {{Brain Mechanisms}}},
  shorttitle = {Principles of {{Neurodynamics}}},
  author = {Rosenblatt, Frank},
  year = {1962},
  publisher = {{Spartan Books}},
  googlebooks = {7FhRAAAAMAAJ},
  langid = {english}
}

@misc{rumelhart1986learning,
  title = {Learning Internal Representations by Error Propagation", in\textbackslash{{Parallel Distributed Processing}}", {{DE}} Rumelhart, {{JL McClelland}} Eds},
  author = {Rumelhart, DE and Hinton, GE and Williams, RJ},
  year = {1986},
  publisher = {{MIT Press, Cambridge}}
}

@article{schmidhuberDeepLearningNeural2015,
  title = {Deep Learning in Neural Networks: {{An}} Overview},
  shorttitle = {Deep Learning in Neural Networks},
  author = {Schmidhuber, J{\"u}rgen},
  year = {2015},
  month = jan,
  journal = {Neural Networks},
  volume = {61},
  pages = {85--117},
  issn = {08936080},
  doi = {10.1016/j.neunet.2014.09.003},
  abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
  langid = {english},
  file = {/home/david/Zotero/storage/PT65D7X6/Schmidhuber - 2015 - Deep learning in neural networks An overview.pdf}
}

@inproceedings{simardBestPracticesConvolutional2003,
  title = {Best Practices for Convolutional Neural Networks Applied to Visual Document Analysis},
  booktitle = {Seventh {{International Conference}} on {{Document Analysis}} and {{Recognition}}, 2003. {{Proceedings}}.},
  author = {Simard, P.Y. and Steinkraus, D. and Platt, J.C.},
  year = {2003},
  volume = {1},
  pages = {958--963},
  publisher = {{IEEE Comput. Soc}},
  address = {{Edinburgh, UK}},
  doi = {10.1109/ICDAR.2003.1227801},
  abstract = {Neural networks are a powerful technology for classification of visual inputs arising from documents. However, there is a confusing plethora of different neural network methods that are used in the literature and in industry. This paper describes a set of concrete best practices that document analysis researchers can use to get good results with neural networks. The most important practice is getting a training set as large as possible: we expand the training set by adding a new form of distorted data. The next most important practice is that convolutional neural networks are better suited for visual document tasks than fully connected networks. We propose that a simple ``do-it-yourself'' implementation of convolution with a flexible architecture is suitable for many visual document problems. This simple convolutional neural network does not require complex methods, such as momentum, weight decay, structuredependent learning rates, averaging layers, tangent prop, or even finely-tuning the architecture. The end result is a very simple yet general architecture which can yield state-of-the-art performance for document analysis. We illustrate our claims on the MNIST set of English digit images.},
  isbn = {978-0-7695-1960-9},
  langid = {english},
  file = {/home/david/Zotero/storage/M3Z6FERC/Simard et al. - 2003 - Best practices for convolutional neural networks a.pdf}
}

@article{stallkampManVsComputer2012,
  title = {Man vs. Computer: {{Benchmarking}} Machine Learning Algorithms for Traffic Sign Recognition},
  shorttitle = {Man vs. Computer},
  author = {Stallkamp, J. and Schlipsing, M. and Salmen, J. and Igel, C.},
  year = {2012},
  month = aug,
  journal = {Neural Networks},
  series = {Selected {{Papers}} from {{IJCNN}} 2011},
  volume = {32},
  pages = {323--332},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2012.02.016},
  abstract = {Traffic signs are characterized by a wide variability in their visual appearance in real-world environments. For example, changes of illumination, varying weather conditions and partial occlusions impact the perception of road signs. In practice, a large number of different sign classes needs to be recognized with very high accuracy. Traffic signs have been designed to be easily readable for humans, who perform very well at this task. For computer systems, however, classifying traffic signs still seems to pose a challenging pattern recognition problem. Both image processing and machine learning algorithms are continuously refined to improve on this task. But little systematic comparison of such systems exist. What is the status quo? Do today's algorithms reach human performance? For assessing the performance of state-of-the-art machine learning algorithms, we present a publicly available traffic sign dataset with more than 50,000 images of German road signs in 43 classes. The data was considered in the second stage of the German Traffic Sign Recognition Benchmark held at IJCNN 2011. The results of this competition are reported and the best-performing algorithms are briefly described. Convolutional neural networks (CNNs) showed particularly high classification accuracies in the competition. We measured the performance of human subjects on the same data\textemdash and the CNNs outperformed the human test persons.},
  langid = {english},
  keywords = {Benchmarking,Convolutional neural networks,Machine learning,Traffic sign recognition},
  file = {/home/david/Zotero/storage/F7DQ68KG/Stallkamp et al. - 2012 - Man vs. computer Benchmarking machine learning al.pdf;/home/david/Zotero/storage/W7ICGVVM/S0893608012000457.html}
}

@article{turingCOMPUTINGMACHINERYINTELLIGENCE1950,
  title = {I.\textemdash{{COMPUTING MACHINERY AND INTELLIGENCE}}},
  author = {Turing, A. M.},
  year = {1950},
  month = oct,
  journal = {Mind},
  volume = {LIX},
  number = {236},
  pages = {433--460},
  issn = {1460-2113, 0026-4423},
  doi = {10.1093/mind/LIX.236.433},
  langid = {english},
  file = {/storage/Master Thesis/Articles/lix-236-433.pdf}
}

@article{weizenbaumELIZAComputerProgram1966,
  title = {{{ELIZA}}\textemdash a Computer Program for the Study of Natural Language Communication between Man and Machine},
  author = {Weizenbaum, Joseph},
  year = {1966},
  month = jan,
  journal = {Communications of the ACM},
  volume = {9},
  number = {1},
  pages = {36--45},
  issn = {0001-0782},
  doi = {10.1145/365153.365168},
  file = {/home/david/Zotero/storage/3SAM3JPS/Weizenbaum - 1966 - ELIZA—a computer program for the study of natural .pdf}
}

@inproceedings{wengCresceptronSelforganizingNeural1992,
  title = {Cresceptron: A Self-Organizing Neural Network Which Grows Adaptively},
  shorttitle = {Cresceptron},
  booktitle = {[{{Proceedings}} 1992] {{IJCNN International Joint Conference}} on {{Neural Networks}}},
  author = {Weng, J. and Ahuja, N. and Huang, T.S.},
  year = {1992},
  month = jun,
  volume = {1},
  pages = {576-581 vol.1},
  doi = {10.1109/IJCNN.1992.287150},
  abstract = {Cresceptron uses a hierarchical framework to grow neural networks automatically, adaptively, and incrementally through learning. At every level of the hierarchy, new concepts are detected automatically and the network grows by creating new neurons and synapses which memorize the new concepts and their context. The training samples are generalized to other perceptually equivalent items through hierarchical tolerance of deviation. The neural network recognizes the learned items and their variations by hierarchically associating the learned knowledge with the input. It segments the recognized items from the input through back training along the response paths.{$<>$}},
  keywords = {Backpropagation,Electric breakdown,Humans,Input variables,Learning systems,Neural networks,Neurons,Unsupervised learning},
  file = {/home/david/Zotero/storage/PXM2V4HZ/Weng et al. - 1992 - Cresceptron a self-organizing neural network whic.pdf;/home/david/Zotero/storage/9EQZD6A9/stamp.html}
}

@article{xuMachineLearningConstruction2021,
  title = {Machine Learning in Construction: {{From}} Shallow to Deep Learning},
  shorttitle = {Machine Learning in Construction},
  author = {Xu, Yayin and Zhou, Ying and Sekula, Przemyslaw and Ding, Lieyun},
  year = {2021},
  month = may,
  journal = {Developments in the Built Environment},
  volume = {6},
  pages = {100045},
  issn = {26661659},
  doi = {10.1016/j.dibe.2021.100045},
  abstract = {The development of artificial intelligence technology is currently bringing about new opportunities in construction. Machine learning is a major area of interest within the field of artificial intelligence, playing a pivotal role in the process of making construction ``smart''. The application of machine learning in construction has the potential to open up an array of opportunities such as site supervision, automatic detection, and intelligent maintenance. However, the implementation of machine learning faces a range of challenges due to the difficulties in acquiring labeled data, especially when applied in a highly complex construction site environment. This paper reviews the history of machine learning development from shallow to deep learning and its applications in construction. The strengths and weaknesses of machine learning technology in construction have been analyzed in order to foresee the future direction of machine learning applications in this sphere. Furthermore, this paper presents suggestions which may benefit researchers in terms of combining specific knowledge domains in construction with machine learning algorithms so as to develop dedicated deep network models for the industry.},
  langid = {english},
  file = {/home/david/Zotero/storage/GTU5IIL7/1-s2.0-S2666165921000041-main.pdf}
}

@inproceedings{Yang2009,
  type = {Conference Paper},
  title = {Detecting Human Actions in Surveillance Videos},
  author = {Yang, Ming and Ji, Shuiwang and Xu, Wei and Wang, Jinjun and Lv, Fengjun and Yu, Kai and Gong, Yihong and Dikmen, Mert and Lin, Dennis J. and Huang, Thomas S.},
  year = {2009},
  series = {2009 {{TREC Video Retrieval Evaluation Notebook Papers}}},
  publication_stage = {Final},
  source = {Scopus},
  note = {Cited by: 26}
}

@article{zhangStudyArtificialIntelligence2021,
  title = {Study on Artificial Intelligence: {{The}} State of the Art and Future Prospects},
  shorttitle = {Study on Artificial Intelligence},
  author = {Zhang, Caiming and Lu, Yang},
  year = {2021},
  month = sep,
  journal = {Journal of Industrial Information Integration},
  volume = {23},
  pages = {100224},
  issn = {2452414X},
  doi = {10.1016/j.jii.2021.100224},
  abstract = {In the world, the technological and industrial revolution is accelerating by the widespread application of new generation information and communication technologies, such as AI, IoT (the Internet of Things), and blockchain technology. Artificial intelligence has attracted much attention from government, industry, and academia. In this study, popular articles published in recent years that relate to artificial intelligence are selected and explored. This study aims to provide a review of artificial intelligence based on industry information integration. It presents an overview of the scope of artificial intelligence using background, drivers, technologies, and applications, as well as logical opinions regarding the development of artificial intelligence. This paper may play a role in AIrelated research and should provide important insights for practitioners in the real world.The main contribu\- tion of this study is that it clarifies the state of the art of AI for future study.},
  langid = {english},
  file = {/storage/Master Thesis/Articles/1-s2.0-S2452414X21000248-main.pdf}
}
