@article{4308320,
  title = {Polynomial Theory of Complex Systems},
  author = {Ivakhnenko, A. G.},
  year = {1971},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics},
  volume = {SMC-1},
  number = {4},
  pages = {364--378},
  doi = {10.1109/TSMC.1971.4308320}
}

@article{6795724,
  title = {Backpropagation Applied to Handwritten Zip Code Recognition},
  author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
  year = {1989},
  journal = {Neural Computation},
  volume = {1},
  number = {4},
  pages = {541--551},
  doi = {10.1162/neco.1989.1.4.541}
}

@article{abdel-hamidConvolutionalNeuralNetworks2014,
  title = {Convolutional {{Neural Networks}} for {{Speech Recognition}}},
  author = {{Abdel-Hamid}, Ossama and Mohamed, Abdel-rahman and Jiang, Hui and Deng, Li and Penn, Gerald and Yu, Dong},
  year = {2014},
  month = oct,
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {22},
  number = {10},
  pages = {1533--1545},
  issn = {2329-9304},
  doi = {10.1109/TASLP.2014.2339736},
  abstract = {Recently, the hybrid deep neural network (DNN)-hidden Markov model (HMM) has been shown to significantly improve speech recognition performance over the conventional Gaussian mixture model (GMM)-HMM. The performance improvement is partially attributed to the ability of the DNN to model complex correlations in speech features. In this paper, we show that further error rate reduction can be obtained by using convolutional neural networks (CNNs). We first present a concise description of the basic CNN and explain how it can be used for speech recognition. We further propose a limited-weight-sharing scheme that can better model speech features. The special structure such as local connectivity, weight sharing, and pooling in CNNs exhibits some degree of invariance to small shifts of speech features along the frequency axis, which is important to deal with speaker and environment variations. Experimental results show that CNNs reduce the error rate by 6\%-10\% compared with DNNs on the TIMIT phone recognition and the voice search large vocabulary speech recognition tasks.},
  keywords = {Convolution,convolutional neural networks,Hidden Markov models,Limited Weight Sharing (LWS) scheme,Neural networks,pooling,Speech,Speech recognition,Training,Vectors},
  file = {/home/david/Zotero/storage/2GDDFR8V/Abdel-Hamid et al. - 2014 - Convolutional Neural Networks for Speech Recogniti.pdf;/home/david/Zotero/storage/HWAB7EPQ/stamp.html}
}

@article{ahonenFaceDescriptionLocal2006,
  title = {Face Description with Local Binary Patterns: {{Application}} to Face Recognition},
  shorttitle = {Face Description with Local Binary Patterns},
  author = {Ahonen, T. and Hadid, A. and Pietik{\"a}inen, M.},
  year = {2006},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {28},
  number = {12},
  pages = {2037--2041},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2006.244},
  abstract = {This paper presents a novel and efficient facial image representation based on local binary pattern (LBP) texture features. The face image is divided into several regions from which the LBP feature distributions are extracted and concatenated into an enhanced feature vector to be used as a face descriptor. The performance of the proposed method is assessed in the face recognition problem under different challenges. Other applications and several extensions are also discussed. \textcopyright{} 2006 IEEE.},
  langid = {english},
  keywords = {Component-based face recognition,Face misalignment,Facial image representation,Local binary pattern,Texture features},
  note = {Cited By :4611},
  file = {/home/david/Zotero/storage/3NCN79G3/display.html}
}

@inproceedings{ajitReviewConvolutionalNeural2020,
  title = {A {{Review}} of {{Convolutional Neural Networks}}},
  booktitle = {2020 {{International Conference}} on {{Emerging Trends}} in {{Information Technology}} and {{Engineering}} (Ic-{{ETITE}})},
  author = {Ajit, Arohan and Acharya, Koustav and Samanta, Abhishek},
  year = {2020},
  month = feb,
  pages = {1--5},
  doi = {10.1109/ic-ETITE47903.2020.049},
  abstract = {Before Convolutional Neural Networks gained popularity, computer recognition problems involved extracting features out of the data provided which was not adequately efficient or provided a high degree of accuracy. However in recent times, Convolutional Neural Networks have attempted to provide a higher level of efficiency and accuracy in all the fields in which it has been employed in most popular of which are Object Detection, Digit and Image Recognition. It employs a definitely algorithm of steps to follow including methods like Backpropagation, Convolutional Layers, Feature formation and Pooling. Also this article will also venture into use of various frameworks and tools that involve CNN model.},
  keywords = {Convolutional Neural Networks,Deep Learning,Digit Recognition},
  file = {/home/david/Zotero/storage/HGXLEVHN/Ajit et al. - 2020 - A Review of Convolutional Neural Networks.pdf;/home/david/Zotero/storage/GBMMLPKG/stamp.html}
}

@inproceedings{albieroDoesFaceRecognition2020,
  title = {Does {{Face Recognition Accuracy Get Better With Age}}? {{Deep Face Matchers Say No}}},
  shorttitle = {Does {{Face Recognition Accuracy Get Better With Age}}?},
  booktitle = {2020 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Albiero, Vitor and Bowyer, Kevin W. and Vangara, Kushal and King, Michael C.},
  year = {2020},
  month = mar,
  pages = {250--258},
  publisher = {{IEEE}},
  address = {{Snowmass Village, CO, USA}},
  doi = {10.1109/WACV45572.2020.9093357},
  urldate = {2023-02-28},
  abstract = {Previous studies generally agree that face recognition accuracy is higher for older persons than for younger persons. But most previous studies were before the wave of deep learning matchers, and most considered accuracy only in terms of the verification rate for genuine pairs. This paper investigates accuracy for age groups 16-29, 30-49 and 50-70, using three modern deep CNN matchers, and considers differences in the impostor and genuine distributions as well as verification rates and ROC curves. We find that accuracy is lower for older persons and higher for younger persons. In contrast, a pre deep learning matcher on the same dataset shows the traditional result of higher accuracy for older persons, although its overall accuracy is much lower than that of the deep learning matchers. Comparing the impostor and genuine distributions, we conclude that impostor scores have a larger effect than genuine scores in causing lower accuracy for the older age group. We also investigate the effects of training data across the age groups. Our results show that fine-tuning the deep CNN models on additional images of older persons actually lowers accuracy for the older age group. Also, we fine-tune and train from scratch two models using age-balanced training datasets, and these results also show lower accuracy for older age group. These results argue that the lower accuracy for the older age group is not due to imbalance in the original training data.},
  isbn = {978-1-72816-553-0},
  langid = {english},
  annotation = {22 citations (Semantic Scholar/DOI) [2023-02-28] 15 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/8PLWVIAL/Albiero et al. - 2020 - Does Face Recognition Accuracy Get Better With Age.pdf}
}

@article{alzubaidiReviewDeepLearning2021,
  title = {Review of Deep Learning: Concepts, {{CNN}} Architectures, Challenges, Applications, Future Directions},
  shorttitle = {Review of Deep Learning},
  author = {Alzubaidi, Laith and Zhang, Jinglan and Humaidi, Amjad J. and {Al-Dujaili}, Ayad and Duan, Ye and {Al-Shamma}, Omran and Santamar{\'i}a, J. and Fadhel, Mohammed A. and {Al-Amidie}, Muthana and Farhan, Laith},
  year = {2021},
  month = mar,
  journal = {Journal of Big Data},
  volume = {8},
  number = {1},
  pages = {53},
  issn = {2196-1115},
  doi = {10.1186/s40537-021-00444-8},
  urldate = {2023-02-09},
  abstract = {In the last few years, the deep learning (DL) computing paradigm has been deemed the Gold Standard in the machine learning (ML) community. Moreover, it has gradually become the most widely used computational approach in the field of ML, thus achieving outstanding results on several complex cognitive tasks, matching or even beating those provided by human performance. One of the benefits of DL is the ability to learn massive amounts of data. The DL field has grown fast in the last few years and it has been extensively used to successfully address a wide range of traditional applications. More importantly, DL has outperformed well-known ML techniques in many domains, e.g., cybersecurity, natural language processing, bioinformatics, robotics and control, and medical information processing, among many others. Despite it has been contributed several works reviewing the State-of-the-Art on DL, all of them only tackled one aspect of the DL, which leads to an overall lack of knowledge about it. Therefore, in this contribution, we propose using a more holistic approach in order to provide a more suitable starting point from which to develop a full understanding of DL. Specifically, this review attempts to provide a more comprehensive survey of the most important aspects of DL and including those enhancements recently added to the field. In particular, this paper outlines the importance of DL, presents the types of DL techniques and networks. It then presents convolutional neural networks (CNNs) which the most utilized DL network type and describes the development of CNNs architectures together with their main features, e.g., starting with the AlexNet network and closing with the High-Resolution network (HR.Net). Finally, we further present the challenges and suggested solutions to help researchers understand the existing research gaps. It is followed by a list of the major DL applications. Computational tools including FPGA, GPU, and CPU are summarized along with a description of their influence on DL. The paper ends with the evolution matrix, benchmark datasets, and summary and conclusion.},
  keywords = {Convolution neural network (CNN),Deep learning,Deep learning applications,Deep neural network architectures,FPGA,GPU,Image classification,Machine learning,Medical image analysis,Supervised learning,Transfer learning},
  file = {/home/david/Zotero/storage/ATPK2EMH/Alzubaidi et al. - 2021 - Review of deep learning concepts, CNN architectur.pdf;/home/david/Zotero/storage/ZQEACNRQ/s40537-021-00444-8.html}
}

@inproceedings{anKillingTwoBirds2022,
  title = {Killing {{Two Birds}} with {{One Stone}}: {{Efficient}} and {{Robust Training}} of {{Face Recognition CNNs}} by {{Partial FC}}},
  shorttitle = {Killing {{Two Birds}} with {{One Stone}}},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {An, Xiang and Deng, Jiankang and Guo, Jia and Feng, Ziyong and Zhu, XuHan and Yang, Jing and Liu, Tongliang},
  year = {2022},
  month = jun,
  pages = {4032--4041},
  publisher = {{IEEE}},
  address = {{New Orleans, LA, USA}},
  doi = {10.1109/CVPR52688.2022.00401},
  urldate = {2023-02-28},
  abstract = {Learning discriminative deep feature embeddings by using million-scale in-the-wild datasets and margin-based softmax loss is the current state-of-the-art approach for face recognition. However, the memory and computing cost of the Fully Connected (FC) layer linearly scales up to the number of identities in the training set. Besides, the largescale training data inevitably suffers from inter-class conflict and long-tailed distribution. In this paper, we propose a sparsely updating variant of the FC layer, named Partial FC (PFC). In each iteration, positive class centers and a random subset of negative class centers are selected to compute the margin-based softmax loss. All class centers are still maintained throughout the whole training process, but only a subset is selected and updated in each iteration. Therefore, the computing requirement, the probability of inter-class conflict, and the frequency of passive update on tail class centers, are dramatically reduced. Extensive experiments across different training data and backbones (e.g. CNN and ViT) confirm the effectiveness, robustness and efficiency of the proposed PFC. The source code is available at https://github.com/deepinsight/ insightface/tree/master/recognition.},
  isbn = {978-1-66546-946-3},
  langid = {english},
  annotation = {17 citations (Semantic Scholar/DOI) [2023-04-18] 6 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/2H73HNMX/An et al. - 2022 - Killing Two Birds with One Stone Efficient and Ro.pdf}
}

@misc{anPartialFCTraining2021,
  title = {Partial {{FC}}: {{Training}} 10 {{Million Identities}} on a {{Single Machine}}},
  shorttitle = {Partial {{FC}}},
  author = {An, Xiang and Zhu, Xuhan and Xiao, Yang and Wu, Lan and Zhang, Ming and Gao, Yuan and Qin, Bin and Zhang, Debing and Fu, Ying},
  year = {2021},
  month = jan,
  number = {arXiv:2010.05222},
  eprint = {2010.05222},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-28},
  abstract = {Face recognition has been an active and vital topic among computer vision community for a long time. Previous researches mainly focus on loss functions used for facial feature extraction network, among which the improvements of softmax-based loss functions greatly promote the performance of face recognition. However, the contradiction between the drastically increasing number of face identities and the shortage of GPU memories is gradually becoming irreconcilable. In this paper, we thoroughly analyze the optimization goal of softmax-based loss functions and the difficulty of training massive identities. We find that the importance of negative classes in softmax function in face representation learning is not as high as we previously thought. The experiment demonstrates no loss of accuracy when training with only 10\textbackslash\% randomly sampled classes for the softmax-based loss functions, compared with training with full classes using state-of-the-art models on mainstream benchmarks. We also implement a very efficient distributed sampling algorithm, taking into account model accuracy and training efficiency, which uses only eight NVIDIA RTX2080Ti to complete classification tasks with tens of millions of identities. The code of this paper has been made available https://github.com/deepinsight/insightface/tree/master/recognition/partial\_fc.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,{Computer Science - Distributed, Parallel, and Cluster Computing}},
  note = {Comment: 8 pages, 9 figures},
  file = {/home/david/Zotero/storage/SLM4MH5I/An et al. - 2021 - Partial FC Training 10 Million Identities on a Si.pdf;/home/david/Zotero/storage/V6WI4FW7/2010.html}
}

@article{ansariHumanDetectionTechniques2021,
  title = {Human Detection Techniques for Real Time Surveillance: A Comprehensive Survey},
  shorttitle = {Human Detection Techniques for Real Time Surveillance},
  author = {Ansari, Mohd. Aquib and Singh, Dushyant Kumar},
  year = {2021},
  month = mar,
  journal = {Multimedia Tools and Applications},
  volume = {80},
  number = {6},
  pages = {8759--8808},
  issn = {1380-7501, 1573-7721},
  doi = {10.1007/s11042-020-10103-4},
  urldate = {2023-01-17},
  abstract = {Real-time detection of humans is an evolutionary research topic. It is an essential and prominent component of various vision based applications. Detection of humans in real-time video sequences is an arduous and challenging task due to various constraints like cluttered environment, occlusion, noise, etc. Many researchers are doing their research in this area and have published the number of researches so far. Determining humans in visual monitoring system is prominent for different types of applications like person detection and identification, fall detection for an elder person, abnormal surveillance, gender classification, crowd analysis, person gait characterization, etc. The main objective of this paper is to provide a comprehensive survey of the various challenges and modern developments seen for human detection methodologies in day vision. This paper consists of an overview of different human detection techniques and their classification based on various underlying factors. The algorithmic technicalities with their applicability to these techniques are deliberated in detail in the manuscript. Different humanitarian imperative factors have also been highlighted for comparative analysis of each human detection methodology. Our survey shows the difference between current research and future requirements.},
  langid = {english},
  file = {/home/david/Zotero/storage/TN63GH4G/Ansari and Singh - 2021 - Human detection techniques for real time surveilla.pdf}
}

@inproceedings{baeDigiFace1MMillionDigital2023,
  title = {{{DigiFace-1M}}: 1 {{Million Digital Face Images}} for {{Face Recognition}}},
  shorttitle = {{{DigiFace-1M}}},
  booktitle = {2023 {{IEEE}}/{{CVF Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Bae, Gwangbin and {de La Gorce}, Martin and Baltrusaitis, Tadas and Hewitt, Charlie and Chen, Dong and Valentin, Julien and Cipolla, Roberto and Shen, Jingjing},
  year = {2023},
  month = jan,
  pages = {3515--3524},
  publisher = {{IEEE}},
  address = {{Waikoloa, HI, USA}},
  doi = {10.1109/WACV56688.2023.00352},
  urldate = {2023-02-28},
  abstract = {State-of-the-art face recognition models show impressive accuracy, achieving over 99.8\% on Labeled Faces in the Wild (LFW) dataset. Such models are trained on large-scale datasets that contain millions of real human face images collected from the internet. Web-crawled face images are severely biased (in terms of race, lighting, make-up, etc) and often contain label noise. More importantly, the face images are collected without explicit consent, raising ethical concerns. To avoid such problems, we introduce a largescale synthetic dataset for face recognition, obtained by rendering digital faces using a computer graphics pipeline1. We first demonstrate that aggressive data augmentation can significantly reduce the synthetic-to-real domain gap. Having full control over the rendering pipeline, we also study how each attribute (e.g., variation in facial pose, accessories and textures) affects the accuracy. Compared to SynFace, a recent method trained on GAN-generated synthetic faces, we reduce the error rate on LFW by 52.5\% (accuracy from 91.93\% to 96.17\%). By fine-tuning the network on a smaller number of real face images that could reasonably be obtained with consent, we achieve accuracy that is comparable to the methods trained on millions of real face images.},
  isbn = {978-1-66549-346-8},
  langid = {english},
  annotation = {7 citations (Semantic Scholar/DOI) [2023-04-18] 0 citations (Crossref) [2023-02-28]},
  note = {Datasets},
  file = {/home/david/Zotero/storage/8SM3XFTA/Bae et al. - 2023 - DigiFace-1M 1 Million Digital Face Images for Fac.pdf}
}

@misc{bansalDonTsCNNbased2017,
  title = {The {{Do}}'s and {{Don}}'ts for {{CNN-based Face Verification}}},
  author = {Bansal, Ankan and Castillo, Carlos and Ranjan, Rajeev and Chellappa, Rama},
  year = {2017},
  month = sep,
  number = {arXiv:1705.07426},
  eprint = {1705.07426},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-28},
  abstract = {While the research community appears to have developed a consensus on the methods of acquiring annotated data, design and training of CNNs, many questions still remain to be answered. In this paper, we explore the following questions that are critical to face recognition research: (i) Can we train on still images and expect the systems to work on videos? (ii) Are deeper datasets better than wider datasets? (iii) Does adding label noise lead to improvement in performance of deep networks? (iv) Is alignment needed for face recognition? We address these questions by training CNNs using CASIA-WebFace, UMDFaces, and a new video dataset and testing on YouTube- Faces, IJB-A and a disjoint portion of UMDFaces datasets. Our new data set, which will be made publicly available, has 22,075 videos and 3,735,476 human annotated frames extracted from them.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: 10 pages including references, added more experiments on deeper vs wider dataset (section 3.2)},
  file = {/home/david/Zotero/storage/G7GBWXVE/Bansal et al. - 2017 - The Do's and Don'ts for CNN-based Face Verificatio.pdf;/home/david/Zotero/storage/8KR44NIS/1705.html}
}

@book{barronrodriguezRemoteLearningGlobal2021,
  title = {Remote {{Learning During}} the {{Global School Lockdown}}: {{Multi-Country Lessons}}},
  shorttitle = {Remote {{Learning During}} the {{Global School Lockdown}}},
  author = {Barron Rodriguez, Maria and Cobo, Cristobal and {Munoz-Najar}, Alberto and Sanchez Ciarrusta, Inaki},
  year = {2021},
  month = aug,
  publisher = {{World Bank}},
  doi = {10.1596/36141},
  urldate = {2023-03-13},
  langid = {english},
  file = {/home/david/Zotero/storage/MDL9W3L6/Barron Rodriguez et al. - 2021 - Remote Learning During the Global School Lockdown.pdf}
}

@incollection{bartoliniImageQuerying2009,
  title = {Image {{Querying}}},
  booktitle = {Encyclopedia of {{Database Systems}}},
  author = {Bartolini, Ilaria},
  editor = {LIU, {\relax LING} and {\"O}ZSU, M. TAMER},
  year = {2009},
  pages = {1368--1374},
  publisher = {{Springer US}},
  address = {{Boston, MA}},
  doi = {10.1007/978-0-387-39940-9_1440},
  urldate = {2023-02-14},
  isbn = {978-0-387-39940-9},
  langid = {english},
  file = {/home/david/Zotero/storage/NFQV9564/Bartolini - 2009 - Image Querying.pdf}
}

@misc{bhagavatulaFasterRealtimeFacial2017,
  title = {Faster {{Than Real-time Facial Alignment}}: {{A 3D Spatial Transformer Network Approach}} in {{Unconstrained Poses}}},
  shorttitle = {Faster {{Than Real-time Facial Alignment}}},
  author = {Bhagavatula, Chandrasekhar and Zhu, Chenchen and Luu, Khoa and Savvides, Marios},
  year = {2017},
  month = sep,
  number = {arXiv:1707.05653},
  eprint = {1707.05653},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-15},
  abstract = {Facial alignment involves finding a set of landmark points on an image with a known semantic meaning. However, this semantic meaning of landmark points is often lost in 2D approaches where landmarks are either moved to visible boundaries or ignored as the pose of the face changes. In order to extract consistent alignment points across large poses, the 3D structure of the face must be considered in the alignment step. However, extracting a 3D structure from a single 2D image usually requires alignment in the first place. We present our novel approach to simultaneously extract the 3D shape of the face and the semantically consistent 2D alignment through a 3D Spatial Transformer Network (3DSTN) to model both the camera projection matrix and the warping parameters of a 3D model. By utilizing a generic 3D model and a Thin Plate Spline (TPS) warping function, we are able to generate subject specific 3D shapes without the need for a large 3D shape basis. In addition, our proposed network can be trained in an end-to-end framework on entirely synthetic data from the 300W-LP dataset. Unlike other 3D methods, our approach only requires one pass through the network resulting in a faster than real-time alignment. Evaluations of our model on the Annotated Facial Landmarks in the Wild (AFLW) and AFLW2000-3D datasets show our method achieves state-of-the-art performance over other 3D approaches to alignment.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: International Conference on Computer Vision (ICCV) 2017},
  file = {/home/david/Zotero/storage/7U2ZV4KG/Bhagavatula et al. - 2017 - Faster Than Real-time Facial Alignment A 3D Spati.pdf;/home/david/Zotero/storage/XS3RQH4A/1707.html}
}

@inproceedings{bilgeRedCarpetFight2021,
  title = {Red {{Carpet}} to {{Fight Club}}: {{Partially-supervised Domain Transfer}} for {{Face Recognition}} in {{Violent Videos}}},
  shorttitle = {Red {{Carpet}} to {{Fight Club}}},
  booktitle = {2021 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Bilge, Yunus Can and Yucel, Mehmet Kerim and Cinbis, Ramazan Gokberk and {Ikizler-Cinbis}, Nazli and Duygulu, Pinar},
  year = {2021},
  month = jan,
  pages = {3357--3368},
  publisher = {{IEEE}},
  address = {{Waikoloa, HI, USA}},
  doi = {10.1109/WACV48630.2021.00340},
  urldate = {2023-02-28},
  abstract = {In many real-world problems, there is typically a large discrepancy between the characteristics of data used in training versus deployment. A prime example is the analysis of aggression videos: in a criminal incidence, typically suspects need to be identified based on their clean portraitlike photos, instead of their prior video recordings. This results in three major challenges; large domain discrepancy between violence videos and ID-photos, the lack of video examples for most individuals and limited training data availability. To mimic such scenarios, we formulate a realistic domain-transfer problem, where the goal is to transfer the recognition model trained on clean posed images to the target domain of violent videos, where training videos are available only for a subset of subjects. To this end, we introduce the ``WildestFaces'' dataset, tailored to study cross-domain recognition under a variety of adverse conditions. We divide the task of transferring a recognition model from the domain of clean images to the violent videos into two sub-problems and tackle them using (i) stacked affine-transforms for classifier-transfer, (ii) attention-driven pooling for temporal-adaptation. We additionally formulate a self-attention based model for domain-transfer. We establish a rigorous evaluation protocol for this ``clean-toviolent'' recognition task, and present a detailed analysis of the proposed dataset and the methods. Our experiments highlight the unique challenges introduced by the WildestFaces dataset and the advantages of the proposed approach.},
  isbn = {978-1-66540-477-8},
  langid = {english},
  annotation = {1 citations (Semantic Scholar/DOI) [2023-02-28] 2 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/DJQ6H83K/Bilge et al. - 2021 - Red Carpet to Fight Club Partially-supervised Dom.pdf}
}

@article{brubakerDesignCascadesBoosted2008,
  title = {On the {{Design}} of {{Cascades}} of {{Boosted Ensembles}} for {{Face Detection}}},
  author = {Brubaker, S. Charles and Wu, Jianxin and Sun, Jie and Mullin, Matthew D. and Rehg, James M.},
  year = {2008},
  month = may,
  journal = {International Journal of Computer Vision},
  volume = {77},
  number = {1},
  pages = {65--86},
  issn = {1573-1405},
  doi = {10.1007/s11263-007-0060-1},
  abstract = {Cascades of boosted ensembles have become popular in the object detection community following their highly successful introduction in the face detector of Viola and Jones. Since then, researchers have sought to improve upon the original approach by incorporating new methods along a variety of axes (e.g. alternative boosting methods, feature sets, etc.). Nevertheless, key decisions about how many hypotheses to include in an ensemble and the appropriate balance of detection and false positive rates in the individual stages are often made by user intervention or by an automatic method that produces unnecessarily slow detectors. We propose a novel method for making these decisions, which exploits the shape of the stage ROC curves in ways that have been previously ignored. The result is a detector that is significantly faster than the one produced by the standard automatic method. When this algorithm is combined with a recycling method for reusing the outputs of early stages in later ones and with a retracing method that inserts new early rejection points in the cascade, the detection speed matches that of the best hand-crafted detector. We also exploit joint distributions over several features in weak learning to improve overall detector accuracy, and explore ways to improve training time by aggressively filtering features.}
}

@inproceedings{byunGeometricallyAdaptiveDictionary2022,
  title = {Geometrically {{Adaptive Dictionary Attack}} on {{Face Recognition}}},
  booktitle = {2022 {{IEEE}}/{{CVF Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Byun, Junyoung and Go, Hyojun and Kim, Changick},
  year = {2022},
  month = jan,
  pages = {3809--3818},
  publisher = {{IEEE}},
  address = {{Waikoloa, HI, USA}},
  doi = {10.1109/WACV51458.2022.00386},
  urldate = {2023-02-28},
  abstract = {CNN-based face recognition models have brought remarkable performance improvement, but they are vulnerable to adversarial perturbations. Recent studies have shown that adversaries can fool the models even if they can only access the models' hard-label output. However, since many queries are needed to find imperceptible adversarial noise, reducing the number of queries is crucial for these attacks. In this paper, we point out two limitations of existing decision-based black-box attacks. We observe that they waste queries for background noise optimization, and they do not take advantage of adversarial perturbations generated for other images. We exploit 3D face alignment to overcome these limitations and propose a general strategy for query-efficient black-box attacks on face recognition named Geometrically Adaptive Dictionary Attack (GADA). Our core idea is to create an adversarial perturbation in the UV texture map and project it onto the face in the image. It greatly improves query efficiency by limiting the perturbation search space to the facial area and effectively recycling previous perturbations. We apply the GADA strategy to two existing attack methods and show overwhelming performance improvement in the experiments on the LFW and CPLFW datasets. Furthermore, we also present a novel attack strategy that can circumvent query similarity-based stateful detection that identifies the process of query-based black-box attacks.},
  isbn = {978-1-66540-915-5},
  langid = {english},
  annotation = {1 citations (Semantic Scholar/DOI) [2023-04-07] 0 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/YYQHA486/Byun et al. - 2022 - Geometrically Adaptive Dictionary Attack on Face R.pdf}
}

@inproceedings{caoCeleb500KLargeTraining2018,
  title = {Celeb-{{500K}}: {{A Large Training Dataset}} for {{Face Recognition}}},
  shorttitle = {Celeb-{{500K}}},
  booktitle = {2018 25th {{IEEE International Conference}} on {{Image Processing}} ({{ICIP}})},
  author = {Cao, Jiajiong and Li, Yingming and Zhang, Zhongfei},
  year = {2018},
  month = oct,
  pages = {2406--2410},
  issn = {2381-8549},
  doi = {10.1109/ICIP.2018.8451704},
  abstract = {In this paper, we propose a large training dataset named Celeb-500K for face recognition, which contains 50M images from 500K persons. To better facilitate academic research, we clean Celeb-500K to obtain Celeb-500K-2R, which contains 25M aligned face images from 365K persons. Based on the developed dataset, we achieve state-of-the-art face recognition performance and reveal two important observations on face recognition study. First, metric learning methods have limited performance gain when the training dataset contains a large number of identities. Second, in order to develop an efficient training dataset, the number of identities is more important than the average image number of each identity from the perspective of face recognition performance. Extensive experimental results show the superiority of Celeb-500K and provide a strong support to the two observations.},
  keywords = {convolutional neural networks,Face,face dataset,Face detection,face recognition,Face recognition,large scale,Learning systems,Measurement,Performance gain,Training},
  file = {/home/david/Zotero/storage/K2EI4IAC/Cao et al. - 2018 - Celeb-500K A Large Training Dataset for Face Reco.pdf;/home/david/Zotero/storage/FIU9AJWA/stamp.html}
}

@misc{caoDomainBalancingFace2020,
  title = {Domain {{Balancing}}: {{Face Recognition}} on {{Long-Tailed Domains}}},
  shorttitle = {Domain {{Balancing}}},
  author = {Cao, Dong and Zhu, Xiangyu and Huang, Xingyu and Guo, Jianzhu and Lei, Zhen},
  year = {2020},
  month = mar,
  number = {arXiv:2003.13791},
  eprint = {2003.13791},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-02-28},
  abstract = {Long-tailed problem has been an important topic in face recognition task. However, existing methods only concentrate on the long-tailed distribution of classes. Differently, we devote to the long-tailed domain distribution problem, which refers to the fact that a small number of domains frequently appear while other domains far less existing. The key challenge of the problem is that domain labels are too complicated (related to race, age, pose, illumination, etc.) and inaccessible in real applications. In this paper, we propose a novel Domain Balancing (DB) mechanism to handle this problem. Specifically, we first propose a Domain Frequency Indicator (DFI) to judge whether a sample is from head domains or tail domains. Secondly, we formulate a light-weighted Residual Balancing Mapping (RBM) block to balance the domain distribution by adjusting the network according to DFI. Finally, we propose a Domain Balancing Margin (DBM) in the loss function to further optimize the feature space of the tail domains to improve generalization. Extensive analysis and experiments on several face recognition benchmarks demonstrate that the proposed method effectively enhances the generalization capacities and achieves superior performance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {45 citations (Semantic Scholar/arXiv) [2023-02-28]},
  note = {Comment: Accepted to CVPR2020},
  file = {/home/david/Zotero/storage/QVHD98K4/Cao et al. - 2020 - Domain Balancing Face Recognition on Long-Tailed .pdf;/home/david/Zotero/storage/N472ILBU/2003.html}
}

@inproceedings{caoPoseRobustFaceRecognition2018,
  title = {Pose-{{Robust Face Recognition}} via {{Deep Residual Equivariant Mapping}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Cao, Kaidi and Rong, Yu and Li, Cheng and Tang, Xiaoou and Loy, Chen Change},
  year = {2018},
  month = jun,
  pages = {5187--5196},
  publisher = {{IEEE}},
  address = {{Salt Lake City, UT, USA}},
  doi = {10.1109/CVPR.2018.00544},
  urldate = {2023-02-28},
  abstract = {Face recognition achieves exceptional success thanks to the emergence of deep learning. However, many contemporary face recognition models still perform relatively poor in processing profile faces compared to frontal faces. A key reason is that the number of frontal and profile training faces are highly imbalanced - there are extensively more frontal training samples compared to profile ones. In addition, it is intrinsically hard to learn a deep representation that is geometrically invariant to large pose variations. In this study, we hypothesize that there is an inherent mapping between frontal and profile faces, and consequently, their discrepancy in the deep representation space can be bridged by an equivariant mapping. To exploit this mapping, we formulate a novel Deep Residual EquivAriant Mapping (DREAM) block, which is capable of adaptively adding residuals to the input deep representation to transform a profile face representation to a canonical pose that simplifies recognition. The DREAM block consistently enhances the performance of profile face recognition for many strong deep networks, including ResNet models, without deliberately augmenting training data of profile faces. The block is easy to use, light-weight, and can be implemented with a negligible computational overhead 1.},
  isbn = {978-1-5386-6420-9},
  langid = {english},
  annotation = {130 citations (Semantic Scholar/DOI) [2023-02-28] 83 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/DVIUYH7I/Cao et al. - 2018 - Pose-Robust Face Recognition via Deep Residual Equ.pdf}
}

@article{caoReviewNeuralNetworks2018,
  title = {A Review on Neural Networks with Random Weights},
  author = {Cao, Weipeng and Wang, Xizhao and Ming, Zhong and Gao, Jinzhu},
  year = {2018},
  month = jan,
  journal = {Neurocomputing},
  volume = {275},
  pages = {278--287},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2017.08.040},
  urldate = {2023-02-09},
  abstract = {In big data fields, with increasing computing capability, artificial neural networks have shown great strength in solving data classification and regression problems. The traditional training of neural networks depends generally on the error back propagation method to iteratively tune all the parameters. When the number of hidden layers increases, this kind of training has many problems such as slow convergence, time consuming, and local minima. To avoid these problems, neural networks with random weights (NNRW) are proposed in which the weights between the hidden layer and input layer are randomly selected and the weights between the output layer and hidden layer are obtained analytically. Researchers have shown that NNRW has much lower training complexity in comparison with the traditional training of feed-forward neural networks. This paper objectively reviews the advantages and disadvantages of NNRW model, tries to reveal the essence of NNRW, gives our comments and remarks on NNRW, and provides some useful guidelines for users to choose a mechanism to train a feed-forward neural network.},
  langid = {english},
  keywords = {Feed-forward neural networks,Neural networks with random weights,Training mechanism},
  file = {/home/david/Zotero/storage/2DW6TFC3/Cao et al. - 2018 - A review on neural networks with random weights.pdf;/home/david/Zotero/storage/Z7SHKWPE/S0925231217314613.html}
}

@misc{caoVGGFace2DatasetRecognising2018,
  title = {{{VGGFace2}}: {{A}} Dataset for Recognising Faces across Pose and Age},
  shorttitle = {{{VGGFace2}}},
  author = {Cao, Qiong and Shen, Li and Xie, Weidi and Parkhi, Omkar M. and Zisserman, Andrew},
  year = {2018},
  month = may,
  number = {arXiv:1710.08092},
  eprint = {1710.08092},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-27},
  abstract = {In this paper, we introduce a new large-scale face dataset named VGGFace2. The dataset contains 3.31 million images of 9131 subjects, with an average of 362.6 images for each subject. Images are downloaded from Google Image Search and have large variations in pose, age, illumination, ethnicity and profession (e.g. actors, athletes, politicians). The dataset was collected with three goals in mind: (i) to have both a large number of identities and also a large number of images for each identity; (ii) to cover a large range of pose, age and ethnicity; and (iii) to minimize the label noise. We describe how the dataset was collected, in particular the automated and manual filtering stages to ensure a high accuracy for the images of each identity. To assess face recognition performance using the new dataset, we train ResNet-50 (with and without Squeeze-and-Excitation blocks) Convolutional Neural Networks on VGGFace2, on MS- Celeb-1M, and on their union, and show that training on VGGFace2 leads to improved recognition performance over pose and age. Finally, using the models trained on these datasets, we demonstrate state-of-the-art performance on all the IARPA Janus face recognition benchmarks, e.g. IJB-A, IJB-B and IJB-C, exceeding the previous state-of-the-art by a large margin. Datasets and models are publicly available.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: This paper has been accepted by IEEE Conference on Automatic Face and Gesture Recognition (F\&G), 2018. (Oral)},
  file = {/home/david/Zotero/storage/N773889N/Cao et al. - 2018 - VGGFace2 A dataset for recognising faces across p.pdf;/home/david/Zotero/storage/93K97L3G/1710.html}
}

@inproceedings{changDataUncertaintyLearning2020,
  title = {Data {{Uncertainty Learning}} in {{Face Recognition}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Chang, Jie and Lan, Zhonghao and Cheng, Changmao and Wei, Yichen},
  year = {2020},
  month = jun,
  pages = {5709--5718},
  publisher = {{IEEE}},
  address = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.00575},
  urldate = {2023-02-28},
  abstract = {Modeling data uncertainty is important for noisy images, but seldom explored for face recognition. The pioneer work [35] considers uncertainty by modeling each face image embedding as a Gaussian distribution. It is quite effective. However, it uses fixed feature (mean of the Gaussian) from an existing model. It only estimates the variance and relies on an ad-hoc and costly metric. Thus, it is not easy to use. It is unclear how uncertainty affects feature learning.},
  isbn = {978-1-72817-168-5},
  langid = {english},
  annotation = {129 citations (Semantic Scholar/DOI) [2023-02-28] 75 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/M43CGG4I/Chang et al. - 2020 - Data Uncertainty Learning in Face Recognition.pdf}
}

@misc{changFacePoseNetMakingCase2017,
  title = {{{FacePoseNet}}: {{Making}} a {{Case}} for {{Landmark-Free Face Alignment}}},
  shorttitle = {{{FacePoseNet}}},
  author = {Chang, Fengju and Tran, Anh Tuan and Hassner, Tal and Masi, Iacopo and Nevatia, Ram and Medioni, Gerard},
  year = {2017},
  month = aug,
  number = {arXiv:1708.07517},
  eprint = {1708.07517},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-15},
  abstract = {We show how a simple convolutional neural network (CNN) can be trained to accurately and robustly regress 6 degrees of freedom (6DoF) 3D head pose, directly from image intensities. We further explain how this FacePoseNet (FPN) can be used to align faces in 2D and 3D as an alternative to explicit facial landmark detection for these tasks. We claim that in many cases the standard means of measuring landmark detector accuracy can be misleading when comparing different face alignments. Instead, we compare our FPN with existing methods by evaluating how they affect face recognition accuracy on the IJB-A and IJB-B benchmarks: using the same recognition pipeline, but varying the face alignment method. Our results show that (a) better landmark detection accuracy measured on the 300W benchmark does not necessarily imply better face recognition accuracy. (b) Our FPN provides superior 2D and 3D face alignment on both benchmarks. Finally, (c), FPN aligns faces at a small fraction of the computational cost of comparably accurate landmark detectors. For many purposes, FPN is thus a far faster and far more accurate face alignment method than using facial landmark detectors.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/david/Zotero/storage/K9942R9K/Chang et al. - 2017 - FacePoseNet Making a Case for Landmark-Free Face .pdf;/home/david/Zotero/storage/IAMXLQDD/1708.html}
}

@article{chellapillaHighPerformanceConvolutional,
  title = {High {{Performance Convolutional Neural Networks}} for {{Document Processing}}},
  author = {Chellapilla, Kumar and Puri, Sidd and Simard, Patrice},
  abstract = {Convolutional neural networks (CNNs) are well known for producing state-of-the-art recognizers for document processing [1]. However, they can be difficult to implement and are usually slower than traditional multi-layer perceptrons (MLPs). We present three novel approaches to speeding up CNNs: a) unrolling convolution, b) using BLAS (basic linear algebra subroutines), and c) using GPUs (graphic processing units). Unrolled convolution converts the processing in each convolutional layer (both forward-propagation and back-propagation) into a matrix-matrix product. The matrix-matrix product representation of CNNs makes their implementation as easy as MLPs. BLAS is used to efficiently compute matrix products on the CPU. We also present a pixel shader based GPU implementation of CNNs. Results on character recognition problems indicate that unrolled convolution with BLAS produces a dramatic 2.4X-3.0X speedup. The GPU implementation is even faster and produces a 3.1X-4.1X speedup.},
  langid = {english},
  file = {/home/david/Zotero/storage/ZHBI9YQT/Chellapilla et al. - High Performance Convolutional Neural Networks for.pdf}
}

@inproceedings{chenAdversarialNetworkCross2019,
  title = {{{R}}{$^3$} {{Adversarial Network}} for {{Cross Model Face Recognition}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Chen, Ken and Wu, Yichao and Qin, Haoyu and Liang, Ding and Liu, Xuebo and Yan, Junjie},
  year = {2019},
  month = jun,
  pages = {9860--9868},
  issn = {2575-7075},
  doi = {10.1109/CVPR.2019.01010},
  abstract = {In this paper, we raise a new problem, namely cross model face recognition (CMFR), which has considerable economic and social significance. The core of this problem is to make features extracted from different models comparable. However, the diversity, mainly caused by different application scenarios, frequent version updating, and all sorts of service platforms, obstructs interaction among different models and poses a great challenge. To solve this problem, from the perspective of Bayesian modelling, we propose R3 Adversarial Network (R3AN) which consists of three paths: Reconstruction, Representation and Regression. We also introduce adversarial learning into the reconstruction path for better performance. Comprehensive experiments on public datasets demonstrate the feasibility of interaction among different models with the proposed framework. When updating the gallery, R3AN conducts the feature transformation nearly 10 times faster than ResNet-101. Meanwhile, the transformed feature distribution is very close to that of target model, and its error rate is incredibly reduced by approximately 75\% compared with a naive transformation model. Furthermore, we show that face feature can be deciphered into original face image roughly by the reconstruction path, which may give valuable hints for improving the original face recognition models.},
  keywords = {and Body Pose,Face,Gesture,Representation Learning},
  annotation = {26 citations (Semantic Scholar/DOI) [2023-02-28]},
  file = {/home/david/Zotero/storage/NM7ZPWB9/Chen et al. - 2019 - R³ Adversarial Network for Cross Model Face Recogn.pdf;/home/david/Zotero/storage/AG99ENGB/stamp.html}
}

@inproceedings{chenFaceAlignmentKernel2019,
  title = {Face {{Alignment With Kernel Density Deep Neural Network}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Chen, Lisha and Su, Hui and Ji, Qiang},
  year = {2019},
  month = oct,
  pages = {6991--7001},
  publisher = {{IEEE}},
  address = {{Seoul, Korea (South)}},
  doi = {10.1109/ICCV.2019.00709},
  urldate = {2023-04-15},
  abstract = {Deep neural networks achieve good performance in many computer vision problems such as face alignment. However, when the testing image is challenging due to low resolution, occlusion or adversarial attacks, the accuracy of a deep neural network suffers greatly. Therefore, it is important to quantify the uncertainty in its predictions. A probabilistic neural network with Gaussian distribution over the target is typically used to quantify uncertainty for regression problems. However, in real-world problems especially computer vision tasks, the Gaussian assumption is too strong. To model more general distributions, such as multimodal or asymmetric distributions, we propose to develop a kernel density deep neural network. Specifically, for face alignment, we adapt state-of-the-art hourglass neural network into a probabilistic neural network framework with landmark probability map as its output. The model is trained by maximizing the conditional log likelihood. To exploit the output probability map, we extend the model to multistage so that the logits map from the previous stage can feed into the next stage to progressively improve the landmark detection accuracy. Extensive experiments on benchmark datasets against state-of-the-art unconstrained deep learning method demonstrate that the proposed kernel density network achieves comparable or superior performance in terms of prediction accuracy. It further provides aleatoric uncertainty estimation in predictions.},
  isbn = {978-1-72814-803-8},
  langid = {english},
  file = {/home/david/Zotero/storage/MYL6YAJ5/Chen et al. - 2019 - Face Alignment With Kernel Density Deep Neural Net.pdf}
}

@article{chengjunliuGaborFeatureBased2002,
  title = {Gabor Feature Based Classification Using the Enhanced Fisher Linear Discriminant Model for Face Recognition},
  author = {{Chengjun Liu} and {H. Wechsler}},
  year = {2002},
  month = apr,
  journal = {IEEE Transactions on Image Processing},
  volume = {11},
  number = {4},
  pages = {467--476},
  issn = {1941-0042},
  doi = {10.1109/TIP.2002.999679}
}

@misc{chengSurveillanceFaceRecognition2018,
  title = {Surveillance {{Face Recognition Challenge}}},
  author = {Cheng, Zhiyi and Zhu, Xiatian and Gong, Shaogang},
  year = {2018},
  month = aug,
  number = {arXiv:1804.09691},
  eprint = {1804.09691},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-28},
  abstract = {Face recognition (FR) is one of the most extensively investigated problems in computer vision. Significant progress in FR has been made due to the recent introduction of the larger scale FR challenges, particularly with constrained social media web images, e.g. high-resolution photos of celebrity faces taken by professional photo-journalists. However, the more challenging FR in unconstrained and low-resolution surveillance images remains largely under-studied. To facilitate more studies on developing FR models that are effective and robust for low-resolution surveillance facial images, we introduce a new Surveillance Face Recognition Challenge, which we call the QMUL-SurvFace benchmark. This new benchmark is the largest and more importantly the only true surveillance FR benchmark to our best knowledge, where low-resolution images are not synthesised by artificial down-sampling of native high-resolution images. This challenge contains 463,507 face images of 15,573 distinct identities captured in real-world uncooperative surveillance scenes over wide space and time. As a consequence, it presents an extremely challenging FR benchmark. We benchmark the FR performance on this challenge using five representative deep learning face recognition models, in comparison to existing benchmarks. We show that the current state of the arts are still far from being satisfactory to tackle the under-investigated surveillance FR problem in practical forensic scenarios. Face recognition is generally more difficult in an open-set setting which is typical for surveillance scenarios, owing to a large number of non-target people (distractors) appearing open spaced scenes. This is evidently so that on the new Surveillance FR Challenge, the top-performing CentreFace deep learning FR model on the MegaFace benchmark can now only achieve 13.2\% success rate (at Rank-20) at a 10\% false alarm rate.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: The QMUL-SurvFace challenge is publicly available at https://qmul-survface.github.io/},
  file = {/home/david/Zotero/storage/HWXQKI8W/Cheng et al. - 2018 - Surveillance Face Recognition Challenge.pdf;/home/david/Zotero/storage/AHXGLMGU/1804.html}
}

@misc{chenMobileFaceNetsEfficientCNNs2018,
  title = {{{MobileFaceNets}}: {{Efficient CNNs}} for {{Accurate Real-Time Face Verification}} on {{Mobile Devices}}},
  shorttitle = {{{MobileFaceNets}}},
  author = {Chen, Sheng and Liu, Yang and Gao, Xiang and Han, Zhen},
  year = {2018},
  month = jun,
  number = {arXiv:1804.07573},
  eprint = {1804.07573},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-02-27},
  abstract = {We present a class of extremely efficient CNN models, MobileFaceNets, which use less than 1 million parameters and are specifically tailored for high-accuracy real-time face verification on mobile and embedded devices. We first make a simple analysis on the weakness of common mobile networks for face verification. The weakness has been well overcome by our specifically designed MobileFaceNets. Under the same experimental conditions, our MobileFaceNets achieve significantly superior accuracy as well as more than 2 times actual speedup over MobileNetV2. After trained by ArcFace loss on the refined MS-Celeb-1M, our single MobileFaceNet of 4.0MB size achieves 99.55\% accuracy on LFW and 92.59\% TAR@FAR1e-6 on MegaFace, which is even comparable to state-of-the-art big CNN models of hundreds MB size. The fastest one of MobileFaceNets has an actual inference time of 18 milliseconds on a mobile phone. For face verification, MobileFaceNets achieve significantly improved efficiency over previous state-of-the-art mobile CNNs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  annotation = {325 citations (Semantic Scholar/arXiv) [2023-02-28]},
  note = {Comment: Accepted as a conference paper at CCBR 2018. Camera-ready version},
  file = {/home/david/Zotero/storage/E4QRQ4WZ/Chen et al. - 2018 - MobileFaceNets Efficient CNNs for Accurate Real-T.pdf;/home/david/Zotero/storage/UVXFIIUC/1804.html}
}

@misc{chenUnconstrainedFaceVerification2016,
  title = {Unconstrained {{Face Verification}} Using {{Deep CNN Features}}},
  author = {Chen, Jun-Cheng and Patel, Vishal M. and Chellappa, Rama},
  year = {2016},
  month = mar,
  number = {arXiv:1508.01722},
  eprint = {1508.01722},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-02-27},
  abstract = {In this paper, we present an algorithm for unconstrained face verification based on deep convolutional features and evaluate it on the newly released IARPA Janus Benchmark A (IJB-A) dataset. The IJB-A dataset includes real-world unconstrained faces from 500 subjects with full pose and illumination variations which are much harder than the traditional Labeled Face in the Wild (LFW) and Youtube Face (YTF) datasets. The deep convolutional neural network (DCNN) is trained using the CASIA-WebFace dataset. Extensive experiments on the IJB-A dataset are provided.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {280 citations (Semantic Scholar/arXiv) [2023-02-28]},
  file = {/home/david/Zotero/storage/BQCJS9MY/Chen et al. - 2016 - Unconstrained Face Verification using Deep CNN Fea.pdf;/home/david/Zotero/storage/U9K9HNPP/1508.html}
}

@article{chrysosDeepPolynomialNeural2021,
  title = {Deep {{Polynomial Neural Networks}}},
  author = {Chrysos, Grigorios and Moschoglou, Stylianos and Bouritsas, Giorgos and Deng, Jiankang and Panagakis, Yannis and Zafeiriou, Stefanos},
  year = {2021},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  eprint = {2006.13026},
  primaryclass = {cs, stat},
  pages = {1--1},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2021.3058891},
  urldate = {2023-02-28},
  abstract = {Deep Convolutional Neural Networks (DCNNs) are currently the method of choice both for generative, as well as for discriminative learning in computer vision and machine learning. The success of DCNNs can be attributed to the careful selection of their building blocks (e.g., residual blocks, rectifiers, sophisticated normalization schemes, to mention but a few). In this paper, we propose \$\textbackslash Pi\$-Nets, a new class of function approximators based on polynomial expansions. \$\textbackslash Pi\$-Nets are polynomial neural networks, i.e., the output is a high-order polynomial of the input. The unknown parameters, which are naturally represented by high-order tensors, are estimated through a collective tensor factorization with factors sharing. We introduce three tensor decompositions that significantly reduce the number of parameters and show how they can be efficiently implemented by hierarchical neural networks. We empirically demonstrate that \$\textbackslash Pi\$-Nets are very expressive and they even produce good results without the use of non-linear activation functions in a large battery of tasks and signals, i.e., images, graphs, and audio. When used in conjunction with activation functions, \$\textbackslash Pi\$-Nets produce state-of-the-art results in three challenging tasks, i.e. image generation, face verification and 3D mesh representation learning. The source code is available at \textbackslash url\{https://github.com/grigorisg9gr/polynomial\_nets\}.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {24 citations (Semantic Scholar/arXiv) [2023-02-28] 24 citations (Semantic Scholar/DOI) [2023-02-28] 10 citations (Crossref) [2023-02-28]},
  note = {Comment: Published in IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI). Code: https://github.com/grigorisg9gr/polynomial\_nets. arXiv admin note: substantial text overlap with arXiv:2003.03828},
  file = {/home/david/Zotero/storage/PDY8T3SA/Chrysos et al. - 2021 - Deep Polynomial Neural Networks.pdf;/home/david/Zotero/storage/6H28XNEZ/2006.html}
}

@inproceedings{ciresanCommitteeNeuralNetworks2011,
  title = {A Committee of Neural Networks for Traffic Sign Classification},
  booktitle = {The 2011 {{International Joint Conference}} on {{Neural Networks}}},
  author = {Cire{\c s}an, Dan and Meier, Ueli and Masci, Jonathan and Schmidhuber, J{\"u}rgen},
  year = {2011},
  month = jul,
  pages = {1918--1921},
  issn = {2161-4407},
  doi = {10.1109/IJCNN.2011.6033458},
  abstract = {We describe the approach that won the preliminary phase of the German traffic sign recognition benchmark with a better-than-human recognition rate of 98.98\%.We obtain an even better recognition rate of 99.15\% by further training the nets. Our fast, fully parameterizable GPU implementation of a Convolutional Neural Network does not require careful design of pre-wired feature extractors, which are rather learned in a supervised way. A CNN/MLP committee further boosts recognition performance.},
  keywords = {Biological neural networks,Convolutional codes,Error analysis,Image color analysis,Kernel,Neurons,Training},
  file = {/home/david/Zotero/storage/9VGNNCCB/Cireşan et al. - 2011 - A committee of neural networks for traffic sign cl.pdf;/home/david/Zotero/storage/8QJLFKCB/stamp.html}
}

@article{ciresanDeepBigSimple2010,
  title = {Deep, {{Big}}, {{Simple Neural Nets}} for {{Handwritten Digit Recognition}}},
  author = {Cire{\c s}an, Dan Claudiu and Meier, Ueli and Gambardella, Luca Maria and Schmidhuber, J{\"u}rgen},
  year = {2010},
  month = dec,
  journal = {Neural Computation},
  volume = {22},
  number = {12},
  pages = {3207--3220},
  issn = {0899-7667},
  doi = {10.1162/NECO_a_00052},
  urldate = {2023-01-25},
  abstract = {Good old online backpropagation for plain multilayer perceptrons yields a very low 0.35\% error rate on the MNIST handwritten digits benchmark. All we need to achieve this best result so far are many hidden layers, many neurons per layer, numerous deformed training images to avoid overfitting, and graphics cards to greatly speed up learning.}
}

@article{ciresanDeepNeuralNetworks2012,
  title = {Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images},
  author = {Ciresan, D.C. and Giusti, A. and Gambardella, L.M. and Schmidhuber, J.},
  year = {2012},
  journal = {NIPS},
  volume = {25},
  pages = {2852--2860},
  note = {Export Date: 26 January 2023; Cited By: 92}
}

@inproceedings{ciresanMitosisDetectionBreast2013,
  title = {Mitosis {{Detection}} in {{Breast Cancer Histology Images}} with {{Deep Neural Networks}}},
  booktitle = {Medical {{Image Computing}} and {{Computer-Assisted Intervention}} \textendash{} {{MICCAI}} 2013},
  author = {Cire{\c s}an, Dan C. and Giusti, Alessandro and Gambardella, Luca M. and Schmidhuber, J{\"u}rgen},
  editor = {Mori, Kensaku and Sakuma, Ichiro and Sato, Yoshinobu and Barillot, Christian and Navab, Nassir},
  year = {2013},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {411--418},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-40763-5_51},
  abstract = {We use deep max-pooling convolutional neural networks to detect mitosis in breast histology images. The networks are trained to classify each pixel in the images, using as context a patch centered on the pixel. Simple postprocessing is then applied to the network output. Our approach won the ICPR 2012 mitosis detection competition, outperforming other contestants by a significant margin.},
  isbn = {978-3-642-40763-5},
  langid = {english},
  keywords = {Convolutional Neural Network,Deep Neural Network,Ground Truth,Input Image,Mitotic Nucleus},
  file = {/home/david/Zotero/storage/2XVR4SD2/Cireşan et al. - 2013 - Mitosis Detection in Breast Cancer Histology Image.pdf}
}

@article{cootesViewbasedActiveAppearance2002,
  title = {View-Based Active Appearance Models},
  author = {Cootes, T.F and Wheeler, G.V and Walker, K.N and Taylor, C.J},
  year = {2002},
  month = aug,
  journal = {Image and Vision Computing},
  volume = {20},
  number = {9},
  pages = {657--664},
  issn = {0262-8856},
  doi = {10.1016/S0262-8856(02)00055-0},
  abstract = {We demonstrate that a small number of 2D linear statistical models are sufficient to capture the shape and appearance of a face from a wide range of viewpoints. Such models can be used to estimate head orientation and track faces through large angles. Given multiple images of the same face we can learn a coupled model describing the relationship between the frontal appearance and the profile of a face. This relationship can be used to predict new views of a face seen from one view and to constrain search algorithms which seek to locate a face in multiple views simultaneously.},
  keywords = {active appearance models,Appearance models,view-based models}
}

@inproceedings{daboueiBoostingDeepFace2020,
  title = {Boosting {{Deep Face Recognition}} via {{Disentangling Appearance}} and {{Geometry}}},
  booktitle = {2020 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Dabouei, Ali and Taherkhani, Fariborz and Soleymani, Sobhan and Dawson, Jeremy and Nasrabadi, Nasser M.},
  year = {2020},
  month = mar,
  pages = {309--318},
  publisher = {{IEEE}},
  address = {{Snowmass Village, CO, USA}},
  doi = {10.1109/WACV45572.2020.9093326},
  urldate = {2023-02-28},
  abstract = {In this paper, we propose a framework for disentangling the appearance and geometry representations in the face recognition task. To provide supervision for this aim, we generate geometrically identical faces by incorporating spatial transformations. We demonstrate that the proposed approach enhances the performance of deep face recognition models by assisting the training process in two ways. First, it enforces the early and intermediate convolutional layers to learn more representative features that satisfy the properties of disentangled embeddings. Second, it augments the training set by altering faces geometrically. Through extensive experiments, we demonstrate that integrating the proposed approach into state-of-the-art face recognition methods effectively improves their performance on challenging datasets, such as LFW, YTF, and MegaFace. Both theoretical and practical aspects of the method are analyzed rigorously by concerning ablation studies and knowledge transfer tasks. Furthermore, we show that the knowledge leaned by the proposed method can favor other face-related tasks, such as attribute prediction.},
  isbn = {978-1-72816-553-0},
  langid = {english},
  annotation = {8 citations (Semantic Scholar/DOI) [2023-02-28] 5 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/YWA8BGXC/Dabouei et al. - 2020 - Boosting Deep Face Recognition via Disentangling A.pdf}
}

@article{dengArcFaceAdditiveAngular,
  title = {{{ArcFace}}: {{Additive Angular Margin Loss}} for {{Deep Face Recognition}}},
  author = {Deng, Jiankang and Guo, Jia and Xue, Niannan and Zafeiriou, Stefanos},
  langid = {english},
  file = {/home/david/Zotero/storage/7ISYG3JZ/Deng et al. - ArcFace Additive Angular Margin Loss for Deep Fac.pdf}
}

@article{dengArcFaceAdditiveAngular2021,
  title = {{{ArcFace}}: {{Additive Angular Margin Loss}} for {{Deep Face Recognition}}},
  shorttitle = {{{ArcFace}}},
  author = {Deng, Jiankang and Guo, Jia and Yang, Jing and Xue, Niannan and Kotsia, Irene and Zafeiriou, Stefanos},
  year = {2021},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  eprint = {1801.07698},
  primaryclass = {cs},
  pages = {1--1},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2021.3087709},
  urldate = {2023-02-28},
  abstract = {Recently, a popular line of research in face recognition is adopting margins in the well-established softmax loss function to maximize class separability. In this paper, we first introduce an Additive Angular Margin Loss (ArcFace), which not only has a clear geometric interpretation but also significantly enhances the discriminative power. Since ArcFace is susceptible to the massive label noise, we further propose sub-center ArcFace, in which each class contains \$K\$ sub-centers and training samples only need to be close to any of the \$K\$ positive sub-centers. Sub-center ArcFace encourages one dominant sub-class that contains the majority of clean faces and non-dominant sub-classes that include hard or noisy faces. Based on this self-propelled isolation, we boost the performance through automatically purifying raw web faces under massive real-world noise. Besides discriminative feature embedding, we also explore the inverse problem, mapping feature vectors to face images. Without training any additional generator or discriminator, the pre-trained ArcFace model can generate identity-preserved face images for both subjects inside and outside the training data only by using the network gradient and Batch Normalization (BN) priors. Extensive experiments demonstrate that ArcFace can enhance the discriminative feature embedding as well as strengthen the generative face synthesis.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {3302 citations (Semantic Scholar/arXiv) [2023-02-28] 6 citations (Semantic Scholar/DOI) [2023-02-28] 19 citations (Crossref) [2023-02-28]},
  note = {Comment: ArcFace TPAMI version},
  file = {/home/david/Zotero/storage/ITYULXXS/Deng et al. - 2021 - ArcFace Additive Angular Margin Loss for Deep Fac.pdf;/home/david/Zotero/storage/CD5RAMKT/1801.html}
}

@inproceedings{dengHarnessingUnrecognizableFaces2023,
  title = {Harnessing {{Unrecognizable Faces}} for {{Improving Face Recognition}}},
  booktitle = {2023 {{IEEE}}/{{CVF Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Deng, Siqi and Xiong, Yuanjun and Wang, Meng and Xia, Wei and Soatto, Stefano},
  year = {2023},
  month = jan,
  pages = {3413--3422},
  publisher = {{IEEE}},
  address = {{Waikoloa, HI, USA}},
  doi = {10.1109/WACV56688.2023.00342},
  urldate = {2023-02-28},
  abstract = {The common implementation of face recognition systems as a cascade of a detection stage and a recognition or verification stage can cause problems beyond failures of the detector. When the detector succeeds, it can detect faces that cannot be recognized, no matter how capable the recognition system is. Recognizability, a latent variable, should therefore be factored into the design and implementation of face recognition systems. We propose a measure of recognizability of a face image that leverages a key empirical observation: An embedding of face images, implemented by a deep neural network trained using mostly recognizable identities, induces a partition of the hypersphere whereby unrecognizable identities cluster together. This occurs regardless of the phenomenon that causes a face to be unrecognizable, be it optical or motion blur, partial occlusion, spatial quantization, or poor illumination. Therefore, we use the distance from such an ``unrecognizable identity'' as a measure of recognizability, and incorporate it into the design of the overall system. We show that accounting for recognizability reduces the error rate of single-image face recognition by 58\% at FAR=1e-5 on the IJB-C Covariate Verification benchmark, and reduces the verification error rate by 24\% at FAR=1e-5 in set-based recognition on the IJB-C benchmark.},
  isbn = {978-1-66549-346-8},
  langid = {english},
  annotation = {3 citations (Semantic Scholar/DOI) [2023-04-18] 0 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/ZFCI7RP7/Deng et al. - 2023 - Harnessing Unrecognizable Faces for Improving Face.pdf}
}

@misc{dengJointMultiviewFace2017,
  title = {Joint {{Multi-view Face Alignment}} in the {{Wild}}},
  author = {Deng, Jiankang and Trigeorgis, George and Zhou, Yuxiang and Zafeiriou, Stefanos},
  year = {2017},
  month = aug,
  number = {arXiv:1708.06023},
  eprint = {1708.06023},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-15},
  abstract = {The de facto algorithm for facial landmark estimation involves running a face detector with a subsequent deformable model fitting on the bounding box. This encompasses two basic problems: i) the detection and deformable fitting steps are performed independently, while the detector might not provide best-suited initialisation for the fitting step, ii) the face appearance varies hugely across different poses, which makes the deformable face fitting very challenging and thus distinct models have to be used (\textbackslash eg, one for profile and one for frontal faces). In this work, we propose the first, to the best of our knowledge, joint multi-view convolutional network to handle large pose variations across faces in-the-wild, and elegantly bridge face detection and facial landmark localisation tasks. Existing joint face detection and landmark localisation methods focus only on a very small set of landmarks. By contrast, our method can detect and align a large number of landmarks for semi-frontal (68 landmarks) and profile (39 landmarks) faces. We evaluate our model on a plethora of datasets including standard static image datasets such as IBUG, 300W, COFW, and the latest Menpo Benchmark for both semi-frontal and profile faces. Significant improvement over state-of-the-art methods on deformable face tracking is witnessed on 300VW benchmark. We also demonstrate state-of-the-art results for face detection on FDDB and MALF datasets.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: submit to IEEE Transactions on Image Processing},
  file = {/home/david/Zotero/storage/J74Y3BA9/Deng et al. - 2017 - Joint Multi-view Face Alignment in the Wild.pdf;/home/david/Zotero/storage/2DQD32GZ/1708.html}
}

@inproceedings{dengMarginalLossDeep2017,
  title = {Marginal {{Loss}} for {{Deep Face Recognition}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Deng, Jiankang and Zhou, Yuxiang and Zafeiriou, Stefanos},
  year = {2017},
  month = jul,
  pages = {2006--2014},
  publisher = {{IEEE}},
  address = {{Honolulu, HI, USA}},
  doi = {10.1109/CVPRW.2017.251},
  urldate = {2023-02-28},
  abstract = {Convolutional neural networks have significantly boosted the performance of face recognition in recent years due to its high capacity in learning discriminative features. In order to enhance the discriminative power of the deeply learned features, we propose a new supervision signal named marginal loss for deep face recognition. Specifically, the marginal loss simultaneously minimises the intra-class variances as well as maximises the inter-class distances by focusing on the marginal samples. With the joint supervision of softmax loss and marginal loss, we can easily train a robust CNNs to obtain more discriminative deep features. Extensive experiments on several relevant face recognition benchmarks, Labelled Faces in the Wild (LFW), YouTube Faces (YTF), Cross-Age Celebrity Dataset (CACD), Age Database (AgeDB) and MegaFace Challenge, prove the effectiveness of the proposed marginal loss.},
  isbn = {978-1-5386-0733-6},
  langid = {english},
  annotation = {168 citations (Semantic Scholar/DOI) [2023-02-28] 120 citations (Crossref) [2023-02-28]},
  note = {Loss functions},
  file = {/home/david/Zotero/storage/7VUFR9HB/Deng et al. - 2017 - Marginal Loss for Deep Face Recognition.pdf}
}

@misc{dengRetinaFaceSinglestageDense2019,
  title = {{{RetinaFace}}: {{Single-stage Dense Face Localisation}} in the {{Wild}}},
  shorttitle = {{{RetinaFace}}},
  author = {Deng, Jiankang and Guo, Jia and Zhou, Yuxiang and Yu, Jinke and Kotsia, Irene and Zafeiriou, Stefanos},
  year = {2019},
  month = may,
  number = {arXiv:1905.00641},
  eprint = {1905.00641},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-13},
  abstract = {Though tremendous strides have been made in uncontrolled face detection, accurate and efficient face localisation in the wild remains an open challenge. This paper presents a robust single-stage face detector, named RetinaFace, which performs pixel-wise face localisation on various scales of faces by taking advantages of joint extra-supervised and self-supervised multi-task learning. Specifically, We make contributions in the following five aspects: (1) We manually annotate five facial landmarks on the WIDER FACE dataset and observe significant improvement in hard face detection with the assistance of this extra supervision signal. (2) We further add a self-supervised mesh decoder branch for predicting a pixel-wise 3D shape face information in parallel with the existing supervised branches. (3) On the WIDER FACE hard test set, RetinaFace outperforms the state of the art average precision (AP) by 1.1\% (achieving AP equal to 91.4\%). (4) On the IJB-C test set, RetinaFace enables state of the art methods (ArcFace) to improve their results in face verification (TAR=89.59\% for FAR=1e-6). (5) By employing light-weight backbone networks, RetinaFace can run real-time on a single CPU core for a VGA-resolution image. Extra annotations and code have been made available at: https://github.com/deepinsight/insightface/tree/master/RetinaFace.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/david/Zotero/storage/97C8PXHB/Deng et al. - 2019 - RetinaFace Single-stage Dense Face Localisation i.pdf;/home/david/Zotero/storage/I79XJZC6/1905.html}
}

@inproceedings{dengUVGANAdversarialFacial2018,
  title = {{{UV-GAN}}: {{Adversarial Facial UV Map Completion}} for {{Pose-Invariant Face Recognition}}},
  shorttitle = {{{UV-GAN}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Deng, Jiankang and Cheng, Shiyang and Xue, Niannan and Zhou, Yuxiang and Zafeiriou, Stefanos},
  year = {2018},
  month = jun,
  pages = {7093--7102},
  publisher = {{IEEE}},
  address = {{Salt Lake City, UT}},
  doi = {10.1109/CVPR.2018.00741},
  urldate = {2023-02-28},
  abstract = {Recently proposed robust 3D face alignment methods establish either dense or sparse correspondence between a 3D face model and a 2D facial image. The use of these methods presents new challenges as well as opportunities for facial texture analysis. In particular, by sampling the image using the fitted model, a facial UV can be created. Unfortunately, due to self-occlusion, such a UV map is always incomplete. In this paper, we propose a framework for training Deep Convolutional Neural Network (DCNN) to complete the facial UV map extracted from in-the-wild images. To this end, we first gather complete UV maps by fitting a 3D Morphable Model (3DMM) to various multiview image and video datasets, as well as leveraging on a new 3D dataset with over 3,000 identities. Second, we devise a meticulously designed architecture that combines local and global adversarial DCNNs to learn an identity-preserving facial UV completion model. We demonstrate that by attaching the completed UV to the fitted mesh and generating instances of arbitrary poses, we can increase pose variations for training deep face recognition/verification models, and minimise pose discrepancy during testing, which lead to better performance. Experiments on both controlled and in-the-wild UV datasets prove the effectiveness of our adversarial UV completion model. We achieve state-of-theart verification accuracy, 94.05\%, under the CFP frontalprofile protocol only by combining pose augmentation during training and pose discrepancy reduction during testing. We will release the first in-the-wild UV dataset (we refer as WildUV) that comprises of complete facial UV maps from 1,892 identities for research purposes.},
  isbn = {978-1-5386-6420-9},
  langid = {english},
  annotation = {147 citations (Semantic Scholar/DOI) [2023-02-28] 100 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/JMYYIQG7/Deng et al. - 2018 - UV-GAN Adversarial Facial UV Map Completion for P.pdf}
}

@inproceedings{dengVariationalPrototypeLearning2021,
  title = {Variational {{Prototype Learning}} for {{Deep Face Recognition}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Deng, Jiankang and Guo, Jia and Yang, Jing and Lattas, Alexandros and Zafeiriou, Stefanos},
  year = {2021},
  month = jun,
  pages = {11901--11910},
  publisher = {{IEEE}},
  address = {{Nashville, TN, USA}},
  doi = {10.1109/CVPR46437.2021.01173},
  urldate = {2023-02-28},
  abstract = {Deep face recognition has achieved remarkable improvements due to the introduction of margin-based softmax loss, in which the prototype stored in the last linear layer represents the center of each class. In these methods, training samples are enforced to be close to positive prototypes and far apart from negative prototypes by a clear margin. However, we argue that prototype learning only employs sample-to-prototype comparisons without considering sample-to-sample comparisons during training and the low loss value gives us an illusion of perfect feature embedding, impeding the further exploration of SGD. To this end, we propose Variational Prototype Learning (VPL), which represents every class as a distribution instead of a point in the latent space. By identifying the slow feature drift phenomenon, we directly inject memorized features into prototypes to approximate variational prototype sampling. The proposed VPL can simulate sample-to-sample comparisons within the classification framework, encouraging the SGD solver to be more exploratory, while boosting performance. Moreover, VPL is conceptually simple, easy to implement, computationally efficient and memory saving. We present extensive experimental results on popular benchmarks, which demonstrate the superiority of the proposed VPL method over the state-of-the-art competitors.},
  isbn = {978-1-66544-509-2},
  langid = {english},
  annotation = {21 citations (Semantic Scholar/DOI) [2023-02-28] 16 citations (Crossref) [2023-02-28]},
  note = {Loss functions},
  file = {/home/david/Zotero/storage/PKUVMC9B/Deng et al. - 2021 - Variational Prototype Learning for Deep Face Recog.pdf}
}

@inproceedings{dharPASSProtectedAttribute2021,
  title = {{{PASS}}: {{Protected Attribute Suppression System}} for {{Mitigating Bias}} in {{Face Recognition}}},
  shorttitle = {{{PASS}}},
  booktitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Dhar, Prithviraj and Gleason, Joshua and Roy, Aniket and Castillo, Carlos D. and Chellappa, Rama},
  year = {2021},
  month = oct,
  pages = {15067--15076},
  publisher = {{IEEE}},
  address = {{Montreal, QC, Canada}},
  doi = {10.1109/ICCV48922.2021.01481},
  urldate = {2023-02-28},
  abstract = {Face recognition networks encode information about sensitive attributes while being trained for identity classification. Such encoding has two major issues: (a) it makes the face representations susceptible to privacy leakage (b) it appears to contribute to bias in face recognition. However, existing bias mitigation approaches generally require end-to-end training and are unable to achieve high verification accuracy. Therefore, we present a descriptor-based adversarial de-biasing approach called `Protected Attribute Suppression System (PASS)'. PASS can be trained on top of descriptors obtained from any previously trained high-performing network to classify identities and simultaneously reduce encoding of sensitive attributes. This eliminates the need for end-toend training. As a component of PASS, we present a novel discriminator training strategy that discourages a network from encoding protected attribute information. We show the efficacy of PASS to reduce gender and skintone information in descriptors from SOTA face recognition networks like Arcface. As a result, PASS descriptors outperform existing baselines in reducing gender and skintone bias on the IJB-C dataset, while maintaining a high verification accuracy.},
  isbn = {978-1-66542-812-5},
  langid = {english},
  annotation = {15 citations (Semantic Scholar/DOI) [2023-02-28] 4 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/7IZI2HP8/Dhar et al. - 2021 - PASS Protected Attribute Suppression System for M.pdf}
}

@inproceedings{dongEfficientDecisionBasedBlackBox2019,
  title = {Efficient {{Decision-Based Black-Box Adversarial Attacks}} on {{Face Recognition}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Dong, Yinpeng and Su, Hang and Wu, Baoyuan and Li, Zhifeng and Liu, Wei and Zhang, Tong and Zhu, Jun},
  year = {2019},
  month = jun,
  pages = {7706--7714},
  publisher = {{IEEE}},
  address = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.00790},
  urldate = {2023-02-28},
  abstract = {Face recognition has obtained remarkable progress in recent years due to the great improvement of deep convolutional neural networks (CNNs). However, deep CNNs are vulnerable to adversarial examples, which can cause fateful consequences in real-world face recognition applications with security-sensitive purposes. Adversarial attacks are widely studied as they can identify the vulnerability of the models before they are deployed. In this paper, we evaluate the robustness of state-of-the-art face recognition models in the decision-based black-box attack setting, where the attackers have no access to the model parameters and gradients, but can only acquire hard-label predictions by sending queries to the target model. This attack setting is more practical in real-world face recognition systems. To improve the efficiency of previous methods, we propose an evolutionary attack algorithm, which can model the local geometry of the search directions and reduce the dimension of the search space. Extensive experiments demonstrate the effectiveness of the proposed method that induces a minimum perturbation to an input face image with fewer queries. We also apply the proposed method to attack a real-world face recognition system successfully.},
  isbn = {978-1-72813-293-8},
  langid = {english},
  annotation = {232 citations (Semantic Scholar/DOI) [2023-02-28] 132 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/VDZY4ZWA/Dong et al. - 2019 - Efficient Decision-Based Black-Box Adversarial Att.pdf}
}

@article{Dreyfus1973383,
  type = {Article},
  title = {The Computational Solution of Optimal Control Problems with Time Lag},
  author = {Dreyfus, Stuart E.},
  year = {1973},
  journal = {IEEE Transactions on Automatic Control},
  volume = {18},
  number = {4},
  pages = {383--385},
  doi = {10.1109/TAC.1973.1100330},
  publication_stage = {Final},
  source = {Scopus},
  note = {Cited by: 32}
}

@inproceedings{duanUniformFaceLearningDeep2019,
  title = {{{UniformFace}}: {{Learning Deep Equidistributed Representation}} for {{Face Recognition}}},
  shorttitle = {{{UniformFace}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Duan, Yueqi and Lu, Jiwen and Zhou, Jie},
  year = {2019},
  month = jun,
  pages = {3410--3419},
  publisher = {{IEEE}},
  address = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.00353},
  urldate = {2023-02-28},
  abstract = {In this paper, we propose a new supervision objective named uniform loss to learn deep equidistributed representations for face recognition. Most existing methods aim to learn discriminative face features, encouraging large interclass distances and small intra-class variations. However, they ignore the distribution of faces in the holistic feature space, which may lead to severe locality and unbalance. With the prior that faces lie on a hypersphere manifold, we impose an equidistributed constraint by uniformly spreading the class centers on the manifold, so that the minimum distance between class centers can be maximized through complete exploitation of the feature space. To this end, we consider the class centers as like charges on the surface of hypersphere with inter-class repulsion, and minimize the total electric potential energy as the uniform loss. Extensive experimental results on the MegaFace Challenge I, IARPA Janus Benchmark A (IJB-A), Youtube Faces (YTF) and Labeled Faces in the Wild (LFW) datasets show the effectiveness of the proposed uniform loss.},
  isbn = {978-1-72813-293-8},
  langid = {english},
  annotation = {77 citations (Semantic Scholar/DOI) [2023-02-28] 55 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/HP6JZMSX/Duan et al. - 2019 - UniformFace Learning Deep Equidistributed Represe.pdf}
}

@article{duElementsEndtoendDeep2022,
  title = {The {{Elements}} of {{End-to-end Deep Face Recognition}}: {{A Survey}} of {{Recent Advances}}},
  shorttitle = {The {{Elements}} of {{End-to-end Deep Face Recognition}}},
  author = {Du, Hang and Shi, Hailin and Zeng, Dan and Zhang, Xiao-Ping and Mei, Tao},
  year = {2022},
  month = jan,
  journal = {ACM Computing Surveys},
  volume = {54},
  number = {10s},
  pages = {1--42},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3507902},
  urldate = {2023-03-07},
  abstract = {Face recognition (FR) is one of the most popular and long-standing topics in computer vision. With the recent development of deep learning techniques and large-scale datasets, deep face recognition has made remarkable progress and has been widely used in many real-world applications. Given a natural image or video frame as input, an end-to-end deep face recognition system outputs the face feature for recognition. To achieve this, a typical end-to-end system is built with three key elements: face detection, face alignment, and face representation. Face detection locates faces in the image or frame. Then, the face alignment is proceeded to calibrate the faces to the canonical view and crop them with a normalized pixel size. Finally, in the stage of face representation, the discriminative features are extracted from the aligned face for recognition. Nowadays, all of the three elements are fulfilled by the technique of deep convolutional neural network. In this survey article, we present a comprehensive review about the recent advance of each element of the end-to-end deep face recognition, since the thriving deep learning techniques have greatly improved their capability of them. To start with, we present an overview of the end-to-end deep face recognition. Then, we review the advance of each element, respectively, covering many aspects such as the to-date algorithm designs, evaluation metrics, datasets, performance comparison, existing challenges, and promising directions for future research. Also, we provide a detailed discussion about the effect of each element on its subsequent elements and the holistic system. Through this survey, we wish to bring contributions in two aspects: first, readers can conveniently identify the methods which are quite strong-baseline style in the subcategory for further exploration; second, one can also employ suitable methods for establishing a state-of-the-art end-to-end face recognition system from scratch.},
  langid = {english},
  annotation = {30 citations (Semantic Scholar/DOI) [2023-04-18]},
  file = {/home/david/Zotero/storage/9IQG8GB7/Du et al. - 2022 - The Elements of End-to-end Deep Face Recognition .pdf}
}

@inproceedings{duongVec2FaceUnveilHuman2020,
  title = {{{Vec2Face}}: {{Unveil Human Faces From Their Blackbox Features}} in {{Face Recognition}}},
  shorttitle = {{{Vec2Face}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Duong, Chi Nhan and Truong, Thanh-Dat and Luu, Khoa and Quach, Kha Gia and Bui, Hung and Roy, Kaushik},
  year = {2020},
  month = jun,
  pages = {6131--6140},
  publisher = {{IEEE}},
  address = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.00617},
  urldate = {2023-02-28},
  abstract = {Unveiling face images of a subject given his/her highlevel representations extracted from a blackbox Face Recognition engine is extremely challenging. It is because the limitations of accessible information from that engine including its structure and uninterpretable extracted features. This paper presents a novel generative structure with Bijective Metric Learning, namely Bijective Generative Adversarial Networks in a Distillation framework (DiBiGAN), for synthesizing faces of an identity given that person's features. In order to effectively address this problem, this work firstly introduces a bijective metric so that the distance measurement and metric learning process can be directly adopted in image domain for an image reconstruction task. Secondly, a distillation process is introduced to maximize the information exploited from the blackbox face recognition engine. Then a Feature-Conditional Generator Structure with Exponential Weighting Strategy is presented for a more robust generator that can synthesize realistic faces with ID preservation. Results on several benchmarking datasets including CelebA, LFW, AgeDB, CFP-FP against matching engines have demonstrated the effectiveness of DiBiGAN on both image realism and ID preservation properties.},
  isbn = {978-1-72817-168-5},
  langid = {english},
  annotation = {23 citations (Semantic Scholar/DOI) [2023-02-28] 11 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/GD3CJFQG/Duong et al. - 2020 - Vec2Face Unveil Human Faces From Their Blackbox F.pdf}
}

@inproceedings{ebrahimisaadabadiQualityAwareSampletoSample2023,
  title = {A {{Quality Aware Sample-to-Sample Comparison}} for {{Face Recognition}}},
  booktitle = {2023 {{IEEE}}/{{CVF Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Ebrahimi Saadabadi, Mohammad Saeed and Rahimi Malakshan, Sahar and Zafari, Ali and Mostofa, Moktari and Nasrabadi, Nasser M.},
  year = {2023},
  month = jan,
  pages = {6118--6127},
  publisher = {{IEEE}},
  address = {{Waikoloa, HI, USA}},
  doi = {10.1109/WACV56688.2023.00607},
  urldate = {2023-02-28},
  abstract = {Currently available face datasets mainly consist of a large number of high-quality and a small number of lowquality samples. As a result, a Face Recognition (FR) network fails to learn the distribution of low-quality samples since they are less frequent during training (underrepresented). Moreover, current state-of-the-art FR training paradigms are based on the sample-to-center comparison (i.e., Softmax-based classifier), which results in a lack of uniformity between train and test metrics. This work integrates a quality-aware learning process at the sample level into the classification training paradigm (QAFace). In this regard, Softmax centers are adaptively guided to pay more attention to low-quality samples by using a quality-aware function. Accordingly, QAFace adds a quality-based adjustment to the updating procedure of the Softmax-based classifier to improve the performance on the underrepresented low-quality samples. Our method adaptively finds and assigns more attention to the recognizable low-quality samples in the training datasets. In addition, QAFace ignores the unrecognizable low-quality samples using the feature magnitude as a proxy for quality. As a result, QAFace prevents class centers from getting distracted from the optimal direction. The proposed method is superior to the state-ofthe-art algorithms in extensive experimental results on the CFP-FP, LFW, CPLFW, CALFW, AgeDB, IJB-B, and IJB-C datasets.},
  isbn = {978-1-66549-346-8},
  langid = {english},
  annotation = {1 citations (Semantic Scholar/DOI) [2023-04-18] 0 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/6VKKAXY3/Ebrahimi Saadabadi et al. - 2023 - A Quality Aware Sample-to-Sample Comparison for Fa.pdf}
}

@article{elizondoLinearSeparabilityProblem2006,
  title = {The {{Linear Separability Problem}}: {{Some Testing Methods}}},
  shorttitle = {The {{Linear Separability Problem}}},
  author = {Elizondo, D.},
  year = {2006},
  month = mar,
  journal = {IEEE Transactions on Neural Networks},
  volume = {17},
  number = {2},
  pages = {330--344},
  issn = {1045-9227},
  doi = {10.1109/TNN.2005.860871},
  urldate = {2023-01-24},
  abstract = {The notion of linear separability is used widely in machine learning research. Learning algorithms that use this concept to learn include neural networks (single layer perceptron and recursive deterministic perceptron), and kernel machines (support vector machines). This paper presents an overview of several of the methods for testing linear separability between two classes. The methods are divided into four groups: Those based on linear programming, those based on computational geometry, one based on neural networks, and one based on quadratic programming. The Fisher linear discriminant method is also presented. A section on the quantification of the complexity of classification problems is included.},
  langid = {english},
  file = {/home/david/Zotero/storage/JX227D5E/Elizondo - 2006 - The Linear Separability Problem Some Testing Meth.pdf}
}

@inproceedings{farakiCrossDomainSimilarityLearning2021,
  title = {Cross-{{Domain Similarity Learning}} for {{Face Recognition}} in {{Unseen Domains}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Faraki, Masoud and Yu, Xiang and Tsai, Yi-Hsuan and Suh, Yumin and Chandraker, Manmohan},
  year = {2021},
  month = jun,
  pages = {15287--15296},
  publisher = {{IEEE}},
  address = {{Nashville, TN, USA}},
  doi = {10.1109/CVPR46437.2021.01504},
  urldate = {2023-02-28},
  abstract = {Face recognition models trained under the assumption of identical training and test distributions often suffer from poor generalization when faced with unknown variations, such as a novel ethnicity or unpredictable individual make-ups during test time. In this paper, we introduce a novel crossdomain metric learning loss, which we dub Cross-Domain Triplet (CDT) loss, to improve face recognition in unseen domains. The CDT loss encourages learning semantically meaningful features by enforcing compact feature clusters of identities from one domain, where the compactness is measured by underlying similarity metrics that belong to another training domain with different statistics. Intuitively, it discriminatively correlates explicit metrics derived from one domain, with triplet samples from another domain in a unified loss function to be minimized within a network, which leads to better alignment of the training domains. The network parameters are further enforced to learn generalized features under domain shift, in a model-agnostic learning pipeline. Unlike the recent work of Meta Face Recognition [18], our method does not require careful hard-pair sample mining and filtering strategy during training. Extensive experiments on various face recognition benchmarks show the superiority of our method in handling variations, compared to baseline and the state-of-the-art methods.},
  isbn = {978-1-66544-509-2},
  langid = {english},
  annotation = {13 citations (Semantic Scholar/DOI) [2023-02-28] 7 citations (Crossref) [2023-02-28]},
  note = {Loss functions 
\par
Very important},
  file = {/home/david/Zotero/storage/LU5NFJSL/Faraki_Cross-Domain_Similarity_Learning_for_Face_Recognition_in_Unseen_Domains_CVPR_2021_paper.pdf}
}

@article{farleySimulationSelforganizingSystems1954,
  title = {Simulation of Self-Organizing Systems by Digital Computer},
  author = {Farley, B. and Clark, W.},
  year = {1954},
  journal = {Transactions of the IRE Professional Group on Information Theory},
  volume = {4},
  number = {4},
  pages = {76--84},
  doi = {10.1109/TIT.1954.1057468}
}

@misc{fengWingLossRobust2018,
  title = {Wing {{Loss}} for {{Robust Facial Landmark Localisation}} with {{Convolutional Neural Networks}}},
  author = {Feng, Zhen-Hua and Kittler, Josef and Awais, Muhammad and Huber, Patrik and Wu, Xiao-Jun},
  year = {2018},
  month = oct,
  number = {arXiv:1711.06753},
  eprint = {1711.06753},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-14},
  abstract = {We present a new loss function, namely Wing loss, for robust facial landmark localisation with Convolutional Neural Networks (CNNs). We first compare and analyse different loss functions including L2, L1 and smooth L1. The analysis of these loss functions suggests that, for the training of a CNN-based localisation model, more attention should be paid to small and medium range errors. To this end, we design a piece-wise loss function. The new loss amplifies the impact of errors from the interval (-w, w) by switching from L1 loss to a modified logarithm function. To address the problem of under-representation of samples with large out-of-plane head rotations in the training set, we propose a simple but effective boosting strategy, referred to as pose-based data balancing. In particular, we deal with the data imbalance problem by duplicating the minority training samples and perturbing them by injecting random image rotation, bounding box translation and other data augmentation approaches. Last, the proposed approach is extended to create a two-stage framework for robust facial landmark localisation. The experimental results obtained on AFLW and 300W demonstrate the merits of the Wing loss function, and prove the superiority of the proposed method over the state-of-the-art approaches.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: 11 pages, 6 figures, 6 tables},
  file = {/home/david/Zotero/storage/Y5T82Z7Z/Feng et al. - 2018 - Wing Loss for Robust Facial Landmark Localisation .pdf;/home/david/Zotero/storage/QXA97BBG/1711.html}
}

@inproceedings{gongMitigatingFaceRecognition2021,
  title = {Mitigating {{Face Recognition Bias}} via {{Group Adaptive Classifier}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Gong, Sixue and Liu, Xiaoming and Jain, Anil K.},
  year = {2021},
  month = jun,
  pages = {3413--3423},
  publisher = {{IEEE}},
  address = {{Nashville, TN, USA}},
  doi = {10.1109/CVPR46437.2021.00342},
  urldate = {2023-02-28},
  abstract = {Face recognition is known to exhibit bias - subjects in a certain demographic group can be better recognized than other groups. This work aims to learn a fair face representation, where faces of every group could be more equally represented. Our proposed group adaptive classifier mitigates bias by using adaptive convolution kernels and attention mechanisms on faces based on their demographic attributes. The adaptive module comprises kernel masks and channelwise attention maps for each demographic group so as to activate different facial regions for identification, leading to more discriminative features pertinent to their demographics. Our introduced automated adaptation strategy determines whether to apply adaptation to a certain layer by iteratively computing the dissimilarity among demographic-adaptive parameters. A new de-biasing loss function is proposed to mitigate the gap of average intra-class distance between demographic groups. Experiments on face benchmarks (RFW, LFW, IJB-A, and IJB-C) show that our work is able to mitigate face recognition bias across demographic groups while maintaining the competitive accuracy.},
  isbn = {978-1-66544-509-2},
  langid = {english},
  annotation = {46 citations (Semantic Scholar/DOI) [2023-02-28] 23 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/VRR7YUTY/Gong et al. - 2021 - Mitigating Face Recognition Bias via Group Adaptiv.pdf}
}

@misc{guoLearningMetaFace2020,
  title = {Learning {{Meta Face Recognition}} in {{Unseen Domains}}},
  author = {Guo, Jianzhu and Zhu, Xiangyu and Zhao, Chenxu and Cao, Dong and Lei, Zhen and Li, Stan Z.},
  year = {2020},
  month = mar,
  number = {arXiv:2003.07733},
  eprint = {2003.07733},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-02-28},
  abstract = {Face recognition systems are usually faced with unseen domains in real-world applications and show unsatisfactory performance due to their poor generalization. For example, a well-trained model on webface data cannot deal with the ID vs. Spot task in surveillance scenario. In this paper, we aim to learn a generalized model that can directly handle new unseen domains without any model updating. To this end, we propose a novel face recognition method via meta-learning named Meta Face Recognition (MFR). MFR synthesizes the source/target domain shift with a meta-optimization objective, which requires the model to learn effective representations not only on synthesized source domains but also on synthesized target domains. Specifically, we build domain-shift batches through a domain-level sampling strategy and get back-propagated gradients/meta-gradients on synthesized source/target domains by optimizing multi-domain distributions. The gradients and meta-gradients are further combined to update the model to improve generalization. Besides, we propose two benchmarks for generalized face recognition evaluation. Experiments on our benchmarks validate the generalization of our method compared to several baselines and other state-of-the-arts. The proposed benchmarks will be available at https://github.com/cleardusk/MFR.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {91 citations (Semantic Scholar/arXiv) [2023-02-28]},
  note = {Proposes 2 benchmarks},
  file = {/home/david/Zotero/storage/CVGCNQUE/Guo et al. - 2020 - Learning Meta Face Recognition in Unseen Domains.pdf;/home/david/Zotero/storage/PIPIWXWR/2003.html}
}

@misc{guoMSCeleb1MDatasetBenchmark2016,
  title = {{{MS-Celeb-1M}}: {{A Dataset}} and {{Benchmark}} for {{Large-Scale Face Recognition}}},
  shorttitle = {{{MS-Celeb-1M}}},
  author = {Guo, Yandong and Zhang, Lei and Hu, Yuxiao and He, Xiaodong and Gao, Jianfeng},
  year = {2016},
  month = jul,
  number = {arXiv:1607.08221},
  eprint = {1607.08221},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-25},
  abstract = {In this paper, we design a benchmark task and provide the associated datasets for recognizing face images and link them to corresponding entity keys in a knowledge base. More specifically, we propose a benchmark task to recognize one million celebrities from their face images, by using all the possibly collected face images of this individual on the web as training data. The rich information provided by the knowledge base helps to conduct disambiguation and improve the recognition accuracy, and contributes to various real-world applications, such as image captioning and news video analysis. Associated with this task, we design and provide concrete measurement set, evaluation protocol, as well as training data. We also present in details our experiment setup and report promising baseline results. Our benchmark task could lead to one of the largest classification problems in computer vision. To the best of our knowledge, our training dataset, which contains 10M images in version 1, is the largest publicly available one in the world.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {1440 citations (Semantic Scholar/arXiv) [2023-04-26]},
  file = {/home/david/Zotero/storage/8YBB6TVS/Guo et al. - 2016 - MS-Celeb-1M A Dataset and Benchmark for Large-Sca.pdf;/home/david/Zotero/storage/BIIDF86E/1607.html}
}

@article{guRecentAdvancesConvolutional2018,
  title = {Recent Advances in Convolutional Neural Networks},
  author = {Gu, Jiuxiang and Wang, Zhenhua and Kuen, Jason and Ma, Lianyang and Shahroudy, Amir and Shuai, Bing and Liu, Ting and Wang, Xingxing and Wang, Gang and Cai, Jianfei and Chen, Tsuhan},
  year = {2018},
  month = may,
  journal = {Pattern Recognition},
  volume = {77},
  pages = {354--377},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2017.10.013},
  urldate = {2023-02-09},
  abstract = {In the last few years, deep learning has led to very good performance on a variety of problems, such as visual recognition, speech recognition and natural language processing. Among different types of deep neural networks, convolutional neural networks have been most extensively studied. Leveraging on the rapid growth in the amount of the annotated data and the great improvements in the strengths of graphics processor units, the research on convolutional neural networks has been emerged swiftly and achieved state-of-the-art results on various tasks. In this paper, we provide a broad survey of the recent advances in convolutional neural networks. We detailize the improvements of CNN on different aspects, including layer design, activation function, loss function, regularization, optimization and fast computation. Besides, we also introduce various applications of convolutional neural networks in computer vision, speech and natural language processing.},
  langid = {english},
  keywords = {Convolutional neural network,Deep learning},
  file = {/home/david/Zotero/storage/DCGZR9BV/Gu et al. - 2018 - Recent advances in convolutional neural networks.pdf;/home/david/Zotero/storage/HNN7RX6S/S0031320317304120.html}
}

@article{haenleinBriefHistoryArtificial2019,
  title = {A {{Brief History}} of {{Artificial Intelligence}}: {{On}} the {{Past}}, {{Present}}, and {{Future}} of {{Artificial Intelligence}}},
  shorttitle = {A {{Brief History}} of {{Artificial Intelligence}}},
  author = {Haenlein, Michael and Kaplan, Andreas},
  year = {2019},
  month = jul,
  journal = {California Management Review},
  volume = {61},
  pages = {000812561986492},
  doi = {10.1177/0008125619864925},
  abstract = {This introduction to this special issue discusses artificial intelligence (AI), commonly defined as ``a system's ability to interpret external data correctly, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation.'' It summarizes seven articles published in this special issue that present a wide variety of perspectives on AI, authored by several of the world's leading experts and specialists in AI. It concludes by offering a comprehensive outlook on the future of AI, drawing on micro-, meso-, and macro-perspectives.},
  file = {/home/david/Zotero/storage/GFIB753Q/Haenlein and Kaplan - 2019 - A Brief History of Artificial Intelligence On the.pdf}
}

@misc{hasnatMisesFisherMixtureModelbased2017,
  title = {Von {{Mises-Fisher Mixture Model-based Deep}} Learning: {{Application}} to {{Face Verification}}},
  shorttitle = {Von {{Mises-Fisher Mixture Model-based Deep}} Learning},
  author = {Hasnat, Md Abul and Bohn{\'e}, Julien and Milgram, Jonathan and Gentric, St{\'e}phane and Chen, Liming},
  year = {2017},
  month = dec,
  number = {arXiv:1706.04264},
  eprint = {1706.04264},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-02-28},
  abstract = {A number of pattern recognition tasks, \textbackslash textit\{e.g.\}, face verification, can be boiled down to classification or clustering of unit length directional feature vectors whose distance can be simply computed by their angle. In this paper, we propose the von Mises-Fisher (vMF) mixture model as the theoretical foundation for an effective deep-learning of such directional features and derive a novel vMF Mixture Loss and its corresponding vMF deep features. The proposed vMF feature learning achieves the characteristics of discriminative learning, \textbackslash textit\{i.e.\}, compacting the instances of the same class while increasing the distance of instances from different classes. Moreover, it subsumes a number of popular loss functions as well as an effective method in deep learning, namely normalization. We conduct extensive experiments on face verification using 4 different challenging face datasets, \textbackslash textit\{i.e.\}, LFW, YouTube faces, CACD and IJB-A. Results show the effectiveness and excellent generalization ability of the proposed approach as it achieves state-of-the-art results on the LFW, YouTube faces and CACD datasets and competitive results on the IJB-A dataset.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {66 citations (Semantic Scholar/arXiv) [2023-02-28]},
  note = {vMF Loss},
  file = {/home/david/Zotero/storage/NPYMVG8D/Hasnat et al. - 2017 - von Mises-Fisher Mixture Model-based Deep learning.pdf;/home/david/Zotero/storage/9RIEIGYX/1706.html}
}

@inproceedings{hayatJointRegistrationRepresentation2017,
  title = {Joint {{Registration}} and {{Representation Learning}} for {{Unconstrained Face Identification}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Hayat, Munawar and Khan, Salman H. and Werghi, Naoufel and Goecke, Roland},
  year = {2017},
  month = jul,
  pages = {1551--1560},
  publisher = {{IEEE}},
  address = {{Honolulu, HI}},
  doi = {10.1109/CVPR.2017.169},
  urldate = {2023-04-15},
  abstract = {Recent advances in deep learning have resulted in human-level performances on popular unconstrained face datasets including Labeled Faces in the Wild and YouTube Faces. To further advance research, IJB-A benchmark was recently introduced with more challenges especially in the form of extreme head poses. Registration of such faces is quite demanding and often requires laborious procedures like facial landmark localization. In this paper, we propose a Convolutional Neural Networks based data-driven approach which learns to simultaneously register and represent faces. We validate the proposed scheme on template based unconstrained face identification. Here, a template contains multiple media in the form of images and video frames. Unlike existing methods which synthesize all template media information at feature level, we propose to keep the template media intact. Instead, we represent gallery templates by their trained one-vs-rest discriminative models and then employ a Bayesian strategy which optimally fuses decisions of all medias in a query template. We demonstrate the efficacy of the proposed scheme on IJB-A, YouTube Celebrities and COX datasets where our approach achieves significant relative performance boosts of 3.6\%, 21.6\% and 12.8\% respectively.},
  isbn = {978-1-5386-0457-1},
  langid = {english},
  file = {/home/david/Zotero/storage/4VIMSAC9/Hayat et al. - 2017 - Joint Registration and Representation Learning for.pdf}
}

@book{hebbOrganizationBehaviorNeuropsychological1949,
  title = {The {{Organization}} of {{Behavior}}: {{A Neuropsychological Theory}}},
  shorttitle = {The {{Organization}} of {{Behavior}}},
  author = {Hebb, Donald Olding},
  year = {1949},
  publisher = {{Wiley}},
  abstract = {Description du ph\'enom\`ene neuropsychologique r\'egissant le laps de temps entre le stimulus et la r\'eponse, le neurophysiologique et le psychologique. Que ce passe-t-il durant cet interval, dans le cervau humain?},
  googlebooks = {dZ0eDiLTwuEC},
  isbn = {978-0-471-36727-7},
  langid = {english}
}

@inproceedings{heDynamicFeatureLearning2018,
  title = {Dynamic {{Feature Learning}} for {{Partial Face Recognition}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {He, Lingxiao and Li, Haiqing and Zhang, Qi and Sun, Zhenan},
  year = {2018},
  month = jun,
  pages = {7054--7063},
  issn = {2575-7075},
  doi = {10.1109/CVPR.2018.00737},
  abstract = {Partial face recognition (PFR) in unconstrained environment is a very important task, especially in video surveillance, mobile devices, etc. However, a few studies have tackled how to recognize an arbitrary patch of a face image. This study combines Fully Convolutional Network (FCN) with Sparse Representation Classification (SRC) to propose a novel partial face recognition approach, called Dynamic Feature Matching (DFM), to address partial face images regardless of size. Based on DFM, we propose a sliding loss to optimize FCN by reducing the intra-variation between a face patch and face images of a subject, which further improves the performance of DFM. The proposed DFM is evaluated on several partial face databases, including LFW, YTF and CASIA-NIR-Distance databases. Experimental results demonstrate the effectiveness and advantages of DFM in comparison with state-of-the-art PFR methods.},
  keywords = {Convolution,Databases,Face,Face recognition,Feature extraction,Microsoft Windows,Probes},
  annotation = {42 citations (Semantic Scholar/DOI) [2023-02-28]},
  file = {/home/david/Zotero/storage/QYVNDLPS/He et al. - 2018 - Dynamic Feature Learning for Partial Face Recognit.pdf;/home/david/Zotero/storage/UNTIP35W/stamp.html}
}

@inproceedings{heEnhancingFaceRecognition2022,
  title = {Enhancing {{Face Recognition}} with {{Self-Supervised 3D Reconstruction}}},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, Mingjie and Zhang, Jie and Shan, Shiguang and Chen, Xilin},
  year = {2022},
  month = jun,
  pages = {4052--4061},
  publisher = {{IEEE}},
  address = {{New Orleans, LA, USA}},
  doi = {10.1109/CVPR52688.2022.00403},
  urldate = {2023-02-28},
  abstract = {Attributed to both the development of deep networks and abundant data, automatic face recognition (FR) has quickly reached human-level capacity in the past few years. However, the FR problem is not perfectly solved in case of uncontrolled illumination and pose. In this paper, we propose to enhance face recognition with a bypass of self-supervised 3D reconstruction, which enforces the neural backbone to focus on the identity-related depth and albedo information while neglects the identity-irrelevant pose and illumination information. Specifically, inspired by the physical model of image formation, we improve the backbone FR network by introducing a 3D face reconstruction loss with two auxiliary networks. The first one estimates the pose and illumination from the input face image while the second one decodes the canonical depth and albedo from the intermediate feature of the FR backbone network. The whole network is trained in end-to-end manner with both classic face identification loss and the loss of 3D face reconstruction with the physical parameters. In this way, the self-supervised reconstruction acts as a regularization that enables the recognition network to understand faces in 3D view, and the learnt features are forced to encode more information of canonical facial depth and albedo, which is more intrinsic and beneficial to face recognition. Extensive experimental results on various face recognition benchmarks show that, without any cost of extra annotations and computations, our method outperforms state-of-the-art ones. Moreover, the learnt representations can also well generalize to other face-related downstream tasks such as the facial attribute recognition with limited labeled data.},
  isbn = {978-1-66546-946-3},
  langid = {english},
  annotation = {5 citations (Semantic Scholar/DOI) [2023-04-18] 3 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/VUAWMXEB/He et al. - 2022 - Enhancing Face Recognition with Self-Supervised 3D.pdf}
}

@article{hochreiterLongShortTermMemory1997,
  title = {Long {{Short-Term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  month = nov,
  journal = {Neural Computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  issn = {0899-7667},
  doi = {10.1162/neco.1997.9.8.1735},
  urldate = {2023-01-25},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}
}

@inproceedings{houDisentangledRepresentationAgeInvariant2021,
  title = {Disentangled {{Representation}} for {{Age-Invariant Face Recognition}}: {{A Mutual Information Minimization Perspective}}},
  shorttitle = {Disentangled {{Representation}} for {{Age-Invariant Face Recognition}}},
  booktitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Hou, Xuege and Li, Yali and Wang, Shengjin},
  year = {2021},
  month = oct,
  pages = {3672--3681},
  publisher = {{IEEE}},
  address = {{Montreal, QC, Canada}},
  doi = {10.1109/ICCV48922.2021.00367},
  urldate = {2023-02-28},
  abstract = {General face recognition has seen remarkable progress in recent years. However, large age gap still remains a big challenge due to significant alterations in facial appearance and bone structure. Disentanglement plays a key role in partitioning face representations into identity-dependent and age-dependent components for age-invariant face recognition (AIFR). In this paper we propose a multi-task learning framework based on mutual information minimization (MT-MIM), which casts the disentangled representation learning as an objective of information constraints. The method trains a disentanglement network to minimize mutual information between the identity component and age component of the face image from the same person, and reduce the effect of age variations during the identification process. For quantitative measure of the degree of disentanglement, we verify that mutual information can represent as metric. The resulting identity-dependent representations are used for age-invariant face recognition. We evaluate MT-MIM on popular public-domain face aging datasets (FG-NET, MORPH Album 2, CACD and AgeDB) and obtained significant improvements over previous state-of-theart methods. Specifically, our method exceeds the baseline models by over 0.4\% on MORPH Album 2, and over 0.7\% on CACD subsets, which are impressive improvements at the high accuracy levels of above 99\% and an average of 94\%.},
  isbn = {978-1-66542-812-5},
  langid = {english},
  annotation = {8 citations (Semantic Scholar/DOI) [2023-02-28] 3 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/85SXASV2/Hou et al. - 2021 - Disentangled Representation for Age-Invariant Face.pdf}
}

@misc{howardMobileNetsEfficientConvolutional2017,
  title = {{{MobileNets}}: {{Efficient Convolutional Neural Networks}} for {{Mobile Vision Applications}}},
  shorttitle = {{{MobileNets}}},
  author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  year = {2017},
  month = apr,
  number = {arXiv:1704.04861},
  eprint = {1704.04861},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-02-27},
  abstract = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {9991 citations (Semantic Scholar/arXiv) [2023-02-28]},
  file = {/home/david/Zotero/storage/4NFPDF2F/Howard et al. - 2017 - MobileNets Efficient Convolutional Neural Network.pdf;/home/david/Zotero/storage/22I9D4P4/1704.html}
}

@inproceedings{huangCurricularFaceAdaptiveCurriculum2020,
  title = {{{CurricularFace}}: {{Adaptive Curriculum Learning Loss}} for {{Deep Face Recognition}}},
  shorttitle = {{{CurricularFace}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Huang, Yuge and Wang, Yuhan and Tai, Ying and Liu, Xiaoming and Shen, Pengcheng and Li, Shaoxin and Li, Jilin and Huang, Feiyue},
  year = {2020},
  month = jun,
  pages = {5900--5909},
  publisher = {{IEEE}},
  address = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.00594},
  urldate = {2023-02-28},
  abstract = {As an emerging topic in face recognition, designing margin-based loss functions can increase the feature margin between different classes for enhanced discriminability. More recently, the idea of mining-based strategies is adopted to emphasize the misclassified samples, achieving promising results. However, during the entire training process, the prior methods either do not explicitly emphasize the sample based on its importance that renders the hard samples not fully exploited; or explicitly emphasize the effects of semi-hard/hard samples even at the early training stage that may lead to convergence issue. In this work, we propose a novel Adaptive Curriculum Learning loss (CurricularFace) that embeds the idea of curriculum learning into the loss function to achieve a novel training strategy for deep face recognition, which mainly addresses easy samples in the early training stage and hard ones in the later stage. Specifically, our CurricularFace adaptively adjusts the relative importance of easy and hard samples during different training stages. In each stage, different samples are assigned with different importance according to their corresponding difficultness. Extensive experimental results on popular benchmarks demonstrate the superiority of our CurricularFace over the state-of-the-art competitors.},
  isbn = {978-1-72817-168-5},
  langid = {english},
  annotation = {221 citations (Semantic Scholar/DOI) [2023-02-28] 148 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/IM7XG9HU/Huang et al. - 2020 - CurricularFace Adaptive Curriculum Learning Loss .pdf}
}

@misc{huangDenseBoxUnifyingLandmark2015,
  title = {{{DenseBox}}: {{Unifying Landmark Localization}} with {{End}} to {{End Object Detection}}},
  shorttitle = {{{DenseBox}}},
  author = {Huang, Lichao and Yang, Yi and Deng, Yafeng and Yu, Yinan},
  year = {2015},
  month = sep,
  number = {arXiv:1509.04874},
  eprint = {1509.04874},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-13},
  abstract = {How can a single fully convolutional neural network (FCN) perform on object detection? We introduce DenseBox, a unified end-to-end FCN framework that directly predicts bounding boxes and object class confidences through all locations and scales of an image. Our contribution is two-fold. First, we show that a single FCN, if designed and optimized carefully, can detect multiple different objects extremely accurately and efficiently. Second, we show that when incorporating with landmark localization during multi-task learning, DenseBox further improves object detection accuray. We present experimental results on public benchmark datasets including MALF face detection and KITTI car detection, that indicate our DenseBox is the state-of-the-art system for detecting challenging objects such as faces and cars.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/david/Zotero/storage/9IPKZK3I/Huang et al. - 2015 - DenseBox Unifying Landmark Localization with End .pdf;/home/david/Zotero/storage/NCUGEH8H/1509.html}
}

@inproceedings{huangEvaluationorientedKnowledgeDistillation2022,
  title = {Evaluation-Oriented {{Knowledge Distillation}} for {{Deep Face Recognition}}},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Huang, Yuge and Wu, Jiaxiang and Xu, Xingkun and Ding, Shouhong},
  year = {2022},
  month = jun,
  pages = {18719--18728},
  publisher = {{IEEE}},
  address = {{New Orleans, LA, USA}},
  doi = {10.1109/CVPR52688.2022.01818},
  urldate = {2023-02-28},
  abstract = {Knowledge distillation (KD) is a widely-used technique that utilizes large networks to improve the performance of compact models. Previous KD approaches usually aim to guide the student to mimic the teacher's behavior completely in the representation space. However, such one-toone corresponding constraints may lead to inflexible knowledge transfer from the teacher to the student, especially those with low model capacities. Inspired by the ultimate goal of KD methods, we propose a novel Evaluationoriented KD method (EKD) for deep face recognition to directly reduce the performance gap between the teacher and student models during training. Specifically, we adopt the commonly used evaluation metrics in face recognition, i.e., False Positive Rate (FPR) and True Positive Rate (TPR) as the performance indicator. According to the evaluation protocol, the critical pair relations that cause the TPR and FPR difference between the teacher and student models are selected. Then, the critical relations in the student are constrained to approximate the corresponding ones in the teacher by a novel rank-based loss function, giving more flexibility to the student with low capacity. Extensive experimental results on popular benchmarks demonstrate the superiority of our EKD over state-of-the-art competitors.},
  isbn = {978-1-66546-946-3},
  langid = {english},
  annotation = {4 citations (Semantic Scholar/DOI) [2023-04-07] 1 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/RYVJU62L/Huang et al. - 2022 - Evaluation-oriented Knowledge Distillation for Dee.pdf}
}

@article{huangPLFaceProgressiveLearning2023,
  title = {{{PLFace}}: {{Progressive Learning}} for {{Face Recognition}} with {{Mask Bias}}},
  shorttitle = {{{PLFace}}},
  author = {Huang, Baojin and Wang, Zhongyuan and Wang, Guangcheng and Jiang, Kui and Han, Zhen and Lu, Tao and Liang, Chao},
  year = {2023},
  month = mar,
  journal = {Pattern Recognition},
  volume = {135},
  pages = {109142},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2022.109142},
  urldate = {2023-02-27},
  abstract = {The outbreak of the COVID-19 coronavirus epidemic has promoted the development of masked face recognition (MFR). Nevertheless, the performance of regular face recognition is severely compromised when the MFR accuracy is blindly pursued. More facts indicate that MFR should be regarded as a mask bias of face recognition rather than an independent task. To mitigate mask bias, we propose a novel Progressive Learning Loss (PLFace) that achieves a progressive training strategy for deep face recognition to learn balanced performance for masked/mask-free faces recognition based on margin losses. Particularly, our PLFace adaptively adjusts the relative importance of masked and mask-free samples during different training stages. In the early stage of training, PLFace mainly learns the feature representations of mask-free samples. At this time, the regular sample embeddings shrink to the corresponding prototype, which represents the center of each class while being stored in the last linear layer. In the later stage of training, PLFace converges on mask-free samples and further focuses on masked samples until the masked sample embeddings are also gathered in the center of the class. The entire training process emphasizes the paradigm that normal samples shrink first and masked samples gather afterward. Extensive experimental results on popular regular and masked face benchmarks demonstrate that our proposed PLFace can effectively eliminate mask bias in face recognition. Compared to state-of-the-art competitors, PLFace significantly improves the accuracy of MFR while maintaining the performance of normal face recognition.},
  langid = {english},
  keywords = {Face recognition,Mask bias,Progressive learning},
  annotation = {1 citations (Semantic Scholar/DOI) [2023-04-07] 0 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/EBN2FEFB/Huang et al. - 2023 - PLFace Progressive Learning for Face Recognition .pdf}
}

@misc{huangPropagationNetPropagatePoints2020,
  title = {{{PropagationNet}}: {{Propagate Points}} to {{Curve}} to {{Learn Structure Information}}},
  shorttitle = {{{PropagationNet}}},
  author = {Huang, Xiehe and Deng, Weihong and Shen, Haifeng and Zhang, Xiubao and Ye, Jieping},
  year = {2020},
  month = jun,
  number = {arXiv:2006.14308},
  eprint = {2006.14308},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-08},
  abstract = {Deep learning technique has dramatically boosted the performance of face alignment algorithms. However, due to large variability and lack of samples, the alignment problem in unconstrained situations, \textbackslash emph\{e.g\}\textbackslash onedot large head poses, exaggerated expression, and uneven illumination, is still largely unsolved. In this paper, we explore the instincts and reasons behind our two proposals, \textbackslash emph\{i.e\}\textbackslash onedot Propagation Module and Focal Wing Loss, to tackle the problem. Concretely, we present a novel structure-infused face alignment algorithm based on heatmap regression via propagating landmark heatmaps to boundary heatmaps, which provide structure information for further attention map generation. Moreover, we propose a Focal Wing Loss for mining and emphasizing the difficult samples under in-the-wild condition. In addition, we adopt methods like CoordConv and Anti-aliased CNN from other fields that address the shift-variance problem of CNN for face alignment. When implementing extensive experiments on different benchmarks, \textbackslash emph\{i.e\}\textbackslash onedot WFLW, 300W, and COFW, our method outperforms state-of-the-arts by a significant margin. Our proposed approach achieves 4.05\textbackslash\% mean error on WFLW, 2.93\textbackslash\% mean error on 300W full-set, and 3.71\textbackslash\% mean error on COFW.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: 10 pages, 8 figures, 8 tables, CVPR2020},
  file = {/home/david/Zotero/storage/P8Y3UUBV/Huang et al. - 2020 - PropagationNet Propagate Points to Curve to Learn.pdf;/home/david/Zotero/storage/2VX4G3T9/2006.html}
}

@inproceedings{huangWhenAgeInvariantFace2021,
  title = {When {{Age-Invariant Face Recognition Meets Face Age Synthesis}}: {{A Multi-Task Learning Framework}}},
  shorttitle = {When {{Age-Invariant Face Recognition Meets Face Age Synthesis}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Huang, Zhizhong and Zhang, Junping and Shan, Hongming},
  year = {2021},
  month = jun,
  pages = {7278--7287},
  publisher = {{IEEE}},
  address = {{Nashville, TN, USA}},
  doi = {10.1109/CVPR46437.2021.00720},
  urldate = {2023-02-28},
  abstract = {To minimize the effects of age variation in face recognition, previous work either extracts identity-related discriminative features by minimizing the correlation between identity- and age-related features, called age-invariant face recognition (AIFR), or removes age variation by transforming the faces of different age groups into the same age group, called face age synthesis (FAS); however, the former lacks visual results for model interpretation while the latter suffers from artifacts compromising downstream recognition. Therefore, this paper proposes a unified, multi-task framework to jointly handle these two tasks, termed MTLFace, which can learn age-invariant identity-related representation while achieving pleasing face synthesis. Specifically, we first decompose the mixed face features into two uncorrelated components\textemdash identity- and age-related features\textemdash through an attention mechanism, and then decorrelate these two components using multi-task training and continuous domain adaption. In contrast to the conventional one-hot encoding that achieves group-level FAS, we propose a novel identity conditional module to achieve identity-level FAS, with a weight-sharing strategy to improve the age smoothness of synthesized faces. In addition, we collect and release a large cross-age face dataset with age and gender annotations to advance AIFR and FAS. Extensive experiments on five benchmark cross-age datasets demonstrate the superior performance of our proposed MTLFace over state-of-the-art methods for AIFR and FAS. We further validate MTLFace on two popular general face recognition datasets, showing competitive performance for face recognition in the wild. The source code and dataset are available at https://github.com/ Hzzone/MTLFace.},
  isbn = {978-1-66544-509-2},
  langid = {english},
  annotation = {31 citations (Semantic Scholar/DOI) [2023-02-28] 22 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/PSLZ3T79/Huang et al. - 2021 - When Age-Invariant Face Recognition Meets Face Age.pdf}
}

@inproceedings{huAttributeEnhancedFaceRecognition2017,
  title = {Attribute-{{Enhanced Face Recognition}} with {{Neural Tensor Fusion Networks}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Hu, Guosheng and Hua, Yang and Yuan, Yang and Zhang, Zhihong and Lu, Zheng and Mukherjee, Sankha S. and Hospedales, Timothy M. and Robertson, Neil M. and Yang, Yongxin},
  year = {2017},
  month = oct,
  pages = {3764--3773},
  publisher = {{IEEE}},
  address = {{Venice}},
  doi = {10.1109/ICCV.2017.404},
  urldate = {2023-02-28},
  abstract = {Deep learning has achieved great success in face recognition, however deep-learned features still have limited invariance to strong intra-personal variations such as large pose changes. It is observed that some facial attributes (e.g. eyebrow thickness, gender) are robust to such variations. We present the first work to systematically explore how the fusion of face recognition features (FRF) and facial attribute features (FAF) can enhance face recognition performance in various challenging scenarios. Despite the promise of FAF, we find that in practice existing fusion methods fail to leverage FAF to boost face recognition performance in some challenging scenarios. Thus, we develop a powerful tensor-based framework which formulates feature fusion as a tensor optimisation problem. It is nontrivial to directly optimise this tensor due to the large number of parameters to optimise. To solve this problem, we establish a theoretical equivalence between low-rank tensor optimisation and a two-stream gated neural network. This equivalence allows tractable learning using standard neural network optimisation tools, leading to accurate and stable optimisation. Experimental results show the fused feature works better than individual features, thus proving for the first time that facial attributes aid face recognition. We achieve state-of-the-art performance on three popular databases: MultiPIE (cross pose, lighting and expression), CASIA NIR-VIS2.0 (cross-modality environment) and LFW (uncontrolled environment).},
  isbn = {978-1-5386-1032-9},
  langid = {english},
  annotation = {59 citations (Semantic Scholar/DOI) [2023-02-28] 36 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/C7LNAMH9/Hu et al. - 2017 - Attribute-Enhanced Face Recognition with Neural Te.pdf}
}

@inproceedings{huFindingTinyFaces2017,
  title = {Finding {{Tiny Faces}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Hu, Peiyun and Ramanan, Deva},
  year = {2017},
  month = jul,
  pages = {1522--1530},
  publisher = {{IEEE}},
  address = {{Honolulu, HI}},
  doi = {10.1109/CVPR.2017.166},
  urldate = {2023-04-14},
  isbn = {978-1-5386-0457-1},
  langid = {english},
  file = {/home/david/Zotero/storage/MAJFLB4X/Hu and Ramanan - 2017 - Finding Tiny Faces.pdf}
}

@inproceedings{huNoiseTolerantParadigmTraining2019,
  title = {Noise-{{Tolerant Paradigm}} for {{Training Face Recognition CNNs}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Hu, Wei and Huang, Yangyu and Zhang, Fan and Li, Ruirui},
  year = {2019},
  month = jun,
  pages = {11879--11888},
  publisher = {{IEEE}},
  address = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.01216},
  urldate = {2023-02-28},
  abstract = {Benefit from large-scale training datasets, deep Convolutional Neural Networks(CNNs) have achieved impressive results in face recognition(FR). However, tremendous scale of datasets inevitably lead to noisy data, which obviously reduce the performance of the trained CNN models. Kicking out wrong labels from large-scale FR datasets is still very expensive, although some cleaning approaches are proposed. According to the analysis of the whole process of training CNN models supervised by angular margin based loss(AM-Loss) functions, we find that the \texttheta{} distribution of training samples implicitly reflects their probability of being clean. Thus, we propose a novel training paradigm that employs the idea of weighting samples based on the above probability. Without any prior knowledge of noise, we can train high performance CNN models with large-scale FR datasets. Experiments demonstrate the effectiveness of our training paradigm. The codes are available at https: //github.com/huangyangyu/NoiseFace.},
  isbn = {978-1-72813-293-8},
  langid = {english},
  annotation = {46 citations (Semantic Scholar/DOI) [2023-02-28] 44 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/HY4Q52MY/Hu et al. - 2019 - Noise-Tolerant Paradigm for Training Face Recognit.pdf}
}

@article{ivakhnenkoCyberneticPredictingDevices,
  title = {Cybernetic {{Predicting Devices}}},
  author = {Ivakhnenko, A G and Lapa, V G},
  langid = {english},
  file = {/home/david/Zotero/storage/5DSPZAUF/Ivakhnenko and Lapa - Cybernetic Predicting Devices.pdf}
}

@book{josephContributionsPerceptronTheory1960,
  title = {Contributions to {{Perceptron Theory}}},
  author = {Joseph, Roger David},
  year = {1960},
  publisher = {{Cornell Aeronautical Laboratory}},
  googlebooks = {O9JUAAAAYAAJ},
  langid = {english}
}

@inproceedings{kangAttentionalFeaturePairRelation2019,
  title = {Attentional {{Feature-Pair Relation Networks}} for {{Accurate Face Recognition}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Kang, Bong-Nam and Kim, Yonghyun and Jun, Bongjin and Kim, Daijin},
  year = {2019},
  month = oct,
  pages = {5471--5480},
  publisher = {{IEEE}},
  address = {{Seoul, Korea (South)}},
  doi = {10.1109/ICCV.2019.00557},
  urldate = {2023-02-28},
  abstract = {Human face recognition is one of the most important research areas in biometrics. However, the robust face recognition under a drastic change of the facial pose, expression, and illumination is a big challenging problem for its practical application. Such variations make face recognition more difficult. In this paper, we propose a novel face recognition method, called Attentional Feature-pair Relation Network (AFRN), which represents the face by the relevant pairs of local appearance block features with their attention scores. The AFRN represents the face by all possible pairs of the 9\texttimes 9 local appearance block features, the importance of each pair is considered by the attention map that is obtained from the low-rank bilinear pooling, and each pair is weighted by its corresponding attention score. To increase the accuracy, we select top-K pairs of local appearance block features as relevant facial information and drop the remaining irrelevant. The weighted top-K pairs are propagated to extract the joint feature-pair relation by using bilinear attention network. In experiments, we show the effectiveness of the proposed AFRN and achieve the outstanding performance in the 1:1 face verification and 1:N face identification tasks compared to existing stateof-the-art methods on the challenging LFW, YTF, CALFW, CPLFW, CFP, AgeDB, IJB-A, IJB-B, and IJB-C datasets.},
  isbn = {978-1-72814-803-8},
  langid = {english},
  annotation = {28 citations (Semantic Scholar/DOI) [2023-02-28] 25 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/NSMC6WSA/Kang et al. - 2019 - Attentional Feature-Pair Relation Networks for Acc.pdf}
}

@article{kangDeepSimilarityMetric2019,
  title = {A {{Deep Similarity Metric Method Based}} on {{Incomplete Data}} for {{Traffic Anomaly Detection}} in {{IoT}}},
  author = {Kang, Xu and Song, Bin and Sun, Fengyao},
  year = {2019},
  month = jan,
  journal = {Applied Sciences},
  volume = {9},
  pages = {135},
  doi = {10.3390/app9010135},
  abstract = {In recent years, with the development of the Internet of Things (IoT) technology, a large amount of data can be captured from sensors for real-time analysis. By monitoring the traffic video data from the IoT, we can detect the anomalies that may occur and evaluate the security. However, the number of traffic anomalies is extremely limited, so there is a severe over-fitting problem when using traditional deep learning methods. In order to solve the problem above, we propose a similarity metric Convolutional Neural Network (CNN) based on a channel attention model for traffic anomaly detection task. The method mainly includes (1) A Siamese network with a hierarchical attention model by word embedding so that it can selectively measure similarities between anomalies and the templates. (2) A deep transfer learning method can automatically annotate an unlabeled set while fine-tuning the network. (3) A background modeling method combining spatial and temporal information for anomaly extraction. Experiments show that the proposed method is three percentage points higher than deep convolutional generative adversarial network (DCGAN) and five percentage points higher than AutoEncoder on the accuracy. No more time consumption is needed for the annotation process. The extracted candidates can be classified correctly through the proposed method.},
  file = {/home/david/Zotero/storage/235373ZA/Kang et al. - 2019 - A Deep Similarity Metric Method Based on Incomplet.pdf}
}

@article{khanSurveyRecentArchitectures2020,
  title = {A Survey of the Recent Architectures of Deep Convolutional Neural Networks},
  author = {Khan, Asifullah and Sohail, Anabia and Zahoora, Umme and Qureshi, Aqsa Saeed},
  year = {2020},
  month = dec,
  journal = {Artificial Intelligence Review},
  volume = {53},
  number = {8},
  pages = {5455--5516},
  issn = {1573-7462},
  doi = {10.1007/s10462-020-09825-6},
  urldate = {2023-02-09},
  abstract = {Deep Convolutional Neural Network (CNN) is a special type of Neural Networks, which has shown exemplary performance on several competitions related to Computer Vision and Image Processing. Some of the exciting application areas of CNN include Image Classification and Segmentation, Object Detection, Video Processing, Natural Language Processing, and Speech Recognition. The powerful learning ability of deep CNN is primarily due to the use of multiple feature extraction stages that can automatically learn representations from the data. The availability of a large amount of data and improvement in the hardware technology has accelerated the research in CNNs, and recently interesting deep CNN architectures have been reported. Several inspiring ideas to bring advancements in CNNs have been explored, such as the use of different activation and loss functions, parameter optimization, regularization, and architectural innovations. However, the significant improvement in the representational capacity of the deep CNN is achieved through architectural innovations. Notably, the ideas of exploiting spatial and channel information, depth and width of architecture, and multi-path information processing have gained substantial attention. Similarly, the idea of using a block of layers as a structural unit is also gaining popularity. This survey thus focuses on the intrinsic taxonomy present in the recently reported deep CNN architectures and, consequently, classifies the recent innovations in CNN architectures into seven different categories. These seven categories are based on spatial exploitation, depth, multi-path, width, feature-map exploitation, channel boosting, and attention. Additionally, the elementary understanding of CNN components, current challenges, and applications of CNN are also provided.},
  langid = {english},
  file = {/home/david/Zotero/storage/TIG3U4K3/Khan et al. - 2020 - A survey of the recent architectures of deep convo.pdf}
}

@inproceedings{kimAdaFaceQualityAdaptive2022,
  title = {{{AdaFace}}: {{Quality Adaptive Margin}} for {{Face Recognition}}},
  shorttitle = {{{AdaFace}}},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Kim, Minchul and Jain, Anil K. and Liu, Xiaoming},
  year = {2022},
  month = jun,
  pages = {18729--18738},
  publisher = {{IEEE}},
  address = {{New Orleans, LA, USA}},
  doi = {10.1109/CVPR52688.2022.01819},
  urldate = {2023-02-27},
  abstract = {Recognition in low quality face datasets is challenging because facial attributes are obscured and degraded. Advances in margin-based loss functions have resulted in enhanced discriminability of faces in the embedding space. Further, previous studies have studied the effect of adaptive losses to assign more importance to misclassified (hard) examples. In this work, we introduce another aspect of adaptiveness in the loss function, namely the image quality. We argue that the strategy to emphasize misclassified samples should be adjusted according to their image quality. Specifically, the relative importance of easy or hard samples should be based on the sample's image quality. We propose a new loss function that emphasizes samples of different difficulties based on their image quality. Our method achieves this in the form of an adaptive margin function by approximating the image quality with feature norms. Extensive experiments show that our method, AdaFace, improves the face recognition performance over the state-ofthe-art (SoTA) on four datasets (IJB-B, IJB-C, IJB-S and TinyFace). Code and models are released in Supp.},
  isbn = {978-1-66546-946-3},
  langid = {english},
  annotation = {41 citations (Semantic Scholar/DOI) [2023-04-18] 7 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/6CFTZCMP/Kim et al. - 2022 - AdaFace Quality Adaptive Margin for Face Recognit.pdf}
}

@inproceedings{kimGroupFaceLearningLatent2020,
  title = {{{GroupFace}}: {{Learning Latent Groups}} and {{Constructing Group-Based Representations}} for {{Face Recognition}}},
  shorttitle = {{{GroupFace}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Kim, Yonghyun and Park, Wonpyo and Roh, Myung-Cheol and Shin, Jongju},
  year = {2020},
  month = jun,
  pages = {5620--5629},
  publisher = {{IEEE}},
  address = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.00566},
  urldate = {2023-02-28},
  abstract = {In the field of face recognition, a model learns to distinguish millions of face images with fewer dimensional embedding features, and such vast information may not be properly encoded in the conventional model with a single branch. We propose a novel face-recognition-specialized architecture called GroupFace that utilizes multiple groupaware representations, simultaneously, to improve the quality of the embedding feature. The proposed method provides self-distributed labels that balance the number of samples belonging to each group without additional human annotations, and learns the group-aware representations that can narrow down the search space of the target identity. We prove the effectiveness of the proposed method by showing extensive ablation studies and visualizations. All the components of the proposed method can be trained in an end-to-end manner with a marginal increase of computational complexity. Finally, the proposed method achieves the state-of-the-art results with significant improvements in 1:1 face verification and 1:N face identification tasks on the following public datasets: LFW, YTF, CALFW, CPLFW, CFP, AgeDB-30, MegaFace, IJB-B and IJB-C.},
  isbn = {978-1-72817-168-5},
  langid = {english},
  annotation = {53 citations (Semantic Scholar/DOI) [2023-02-28] 39 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/LIHLTFP6/Kim et al. - 2020 - GroupFace Learning Latent Groups and Constructing.pdf}
}

@inproceedings{klarePushingFrontiersUnconstrained2015,
  title = {Pushing the Frontiers of Unconstrained Face Detection and Recognition: {{IARPA Janus Benchmark A}}},
  shorttitle = {Pushing the Frontiers of Unconstrained Face Detection and Recognition},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Klare, Brendan F. and Klein, Ben and Taborsky, Emma and Blanton, Austin and Cheney, Jordan and Allen, Kristen and Grother, Patrick and Mah, Alan and Burge, Mark and Jain, Anil K.},
  year = {2015},
  month = jun,
  pages = {1931--1939},
  publisher = {{IEEE}},
  address = {{Boston, MA, USA}},
  doi = {10.1109/CVPR.2015.7298803},
  urldate = {2023-02-28},
  abstract = {Rapid progress in unconstrained face recognition has resulted in a saturation in recognition accuracy for current benchmark datasets. While important for early progress, a chief limitation in most benchmark datasets is the use of a commodity face detector to select face imagery. The implication of this strategy is restricted variations in face pose and other confounding factors. This paper introduces the IARPA Janus Benchmark A (IJB-A), a publicly available media in the wild dataset containing 500 subjects with manually localized face images. Key features of the IJB-A dataset are: (i) full pose variation, (ii) joint use for face recognition and face detection benchmarking, (iii) a mix of images and videos, (iv) wider geographic variation of subjects, (v) protocols supporting both open-set identification (1:N search) and verification (1:1 comparison), (vi) an optional protocol that allows modeling of gallery subjects, and (vii) ground truth eye and nose locations. The dataset has been developed using 1,501,267 million crowd sourced annotations. Baseline accuracies for both face detection and face recognition from commercial and open source algorithms demonstrate the challenge offered by this new unconstrained benchmark.},
  isbn = {978-1-4673-6964-0},
  langid = {english},
  annotation = {640 citations (Semantic Scholar/DOI) [2023-02-28] 383 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/RIXC6FP7/Klare et al. - 2015 - Pushing the frontiers of unconstrained face detect.pdf}
}

@inproceedings{krizhevskyImageNetClassificationDeep2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  year = {2012},
  volume = {25},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-01-26},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7\textbackslash\% and 18.9\textbackslash\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
  file = {/home/david/Zotero/storage/PI6EC7PZ/Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf}
}

@incollection{learned-millerLabeledFacesWild2016,
  title = {Labeled {{Faces}} in the {{Wild}}: {{A Survey}}},
  shorttitle = {Labeled {{Faces}} in the {{Wild}}},
  booktitle = {Advances in {{Face Detection}} and {{Facial Image Analysis}}},
  author = {{Learned-Miller}, Erik and Huang, Gary B. and RoyChowdhury, Aruni and Li, Haoxiang and Hua, Gang},
  editor = {Kawulok, Michal and Celebi, M. Emre and Smolka, Bogdan},
  year = {2016},
  pages = {189--248},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-25958-1_8},
  urldate = {2023-03-09},
  abstract = {In 2007, Labeled Faces in the Wild was released in an effort to spur research in face recognition, specifically for the problem of face verification with unconstrained images. Since that time, more than 50 papers have been published that improve upon this benchmark in some respect. A remarkably wide variety of innovative methods have been developed to overcome the challenges presented in this database. As performance on some aspects of the benchmark approaches 100\% accuracy, it seems appropriate to review this progress, derive what general principles we can from these works, and identify key future challenges in face recognition. In this survey, we review the contributions to LFW for which the authors have provided results to the curators (results found on the LFW results web page). We also review the cross cutting topic of alignment and how it is used in various methods. We end with a brief discussion of recent databases designed to challenge the next generation of face recognition algorithms.},
  isbn = {978-3-319-25956-7 978-3-319-25958-1},
  langid = {english},
  file = {/home/david/Zotero/storage/Y8ESHSR2/Learned-Miller et al. - 2016 - Labeled Faces in the Wild A Survey.pdf}
}

@article{lecunDeepLearning2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year = {2015},
  month = may,
  journal = {Nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature14539},
  langid = {english},
  file = {/home/david/Zotero/storage/FUTB2UE9/nature14539.pdf}
}

@article{lecunGradientBasedLearningApplied1998,
  title = {Gradient-{{Based Learning Applied}} to {{Document Recognition}}},
  author = {LeCun, Yann and Bottou, Leon and Bengio, Yoshua and Ha, Patrick},
  year = {1998},
  langid = {english},
  file = {/home/david/Zotero/storage/VQIWJ4R7/LeCun et al. - 1998 - Gradient-Based Learning Applied to Document Recogn.pdf}
}

@article{leiLearningDiscriminantFace2014,
  title = {Learning Discriminant Face Descriptor},
  author = {Lei, Z. and Pietikainen, M. and Li, S.Z.},
  year = {2014},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {36},
  number = {2},
  pages = {289--302},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2013.112},
  abstract = {Local feature descriptor is an important module for face recognition and those like Gabor and local binary patterns (LBP) have proven effective face descriptors. Traditionally, the form of such local descriptors is predefined in a handcrafted way. In this paper, we propose a method to learn a discriminant face descriptor (DFD) in a data-driven way. The idea is to learn the most discriminant local features that minimize the difference of the features between images of the same person and maximize that between images from different people. In particular, we propose to enhance the discriminative ability of face representation in three aspects. First, the discriminant image filters are learned. Second, the optimal neighborhood sampling strategy is soft determined. Third, the dominant patterns are statistically constructed. Discriminative learning is incorporated to extract effective and robust features. We further apply the proposed method to the heterogeneous (cross-modality) face recognition problem and learn DFD in a coupled way (coupled DFD or C-DFD) to reduce the gap between features of heterogeneous face images to improve the performance of this challenging problem. Extensive experiments on FERET, CAS-PEAL-R1, LFW, and HFB face databases validate the effectiveness of the proposed DFD learning on both homogeneous and heterogeneous face recognition problems. The DFD improves POEM and LQP by about 4.5 percent on LFW database and the C-DFD enhances the heterogeneous face recognition performance of LBP by over 25 percent. \textcopyright{} 1979-2012 IEEE.},
  langid = {english},
  keywords = {discriminant face descriptor,discriminant learning,Face recognition,heterogeneous face recognition,image filter learning},
  note = {Cited By :287},
  file = {/home/david/Zotero/storage/JEPRCXLN/display.html}
}

@inproceedings{liDynamicClassQueue2021,
  title = {Dynamic {{Class Queue}} for {{Large Scale Face Recognition In}} the {{Wild}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Li, Bi and Xi, Teng and Zhang, Gang and Feng, Haocheng and Han, Junyu and Liu, Jingtuo and Ding, Errui and Liu, Wenyu},
  year = {2021},
  month = jun,
  pages = {3762--3771},
  publisher = {{IEEE}},
  address = {{Nashville, TN, USA}},
  doi = {10.1109/CVPR46437.2021.00376},
  urldate = {2023-02-28},
  isbn = {978-1-66544-509-2},
  langid = {english},
  annotation = {11 citations (Semantic Scholar/DOI) [2023-02-28] 4 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/QJ6WN8BC/Li et al. - 2021 - Dynamic Class Queue for Large Scale Face Recogniti.pdf}
}

@book{liHandbookFaceRecognition2011,
  title = {Handbook of {{Face Recognition}}},
  editor = {Li, Stan Z. and Jain, Anil K.},
  year = {2011},
  publisher = {{Springer}},
  address = {{London}},
  doi = {10.1007/978-0-85729-932-1},
  urldate = {2023-02-14},
  isbn = {978-0-85729-931-4 978-0-85729-932-1},
  langid = {english},
  file = {/home/david/Zotero/storage/KAIXADVJ/Li and Jain - 2011 - Handbook of Face Recognition.pdf}
}

@phdthesis{linnainmaa1970representation,
  title = {The Representation of the Cumulative Rounding Error of an Algorithm as a {{Taylor}} Expansion of the Local Rounding Errors},
  author = {Linnainmaa, Seppo},
  year = {1970},
  school = {Master's Thesis (in Finnish), Univ. Helsinki}
}

@inproceedings{liSphericalConfidenceLearning2021,
  title = {Spherical {{Confidence Learning}} for {{Face Recognition}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Li, Shen and Xu, Jianqing and Xu, Xiaqing and Shen, Pengcheng and Li, Shaoxin and Hooi, Bryan},
  year = {2021},
  month = jun,
  pages = {15624--15632},
  issn = {2575-7075},
  doi = {10.1109/CVPR46437.2021.01537},
  abstract = {An emerging line of research has found that spherical spaces better match the underlying geometry of facial im-ages, as evidenced by the state-of-the-art facial recognition methods which benefit empirically from spherical representations. Yet, these approaches rely on deterministic embeddings and hence suffer from the feature ambiguity dilemma, whereby ambiguous or noisy images are mapped into poorly learned regions of representation space, leading to inaccuracies. Probabilistic Face Embeddings (PFE) [17] is the first attempt to address this dilemma. However, we theoretically and empirically identify two main failures of PFE when it is applied to spherical deterministic embeddings aforementioned. To address these issues, in this paper, we propose a novel framework for face confidence learning in spherical space. Mathematically, we extend the von Mises Fisher density to its r-radius counterpart and derive a new optimization objective in closed form. Theoretically, the proposed probabilistic framework provably allows for better interpretability, leading to principled feature comparison and pooling. Extensive experimental results on multiple challenging benchmarks confirm our hypothesis and theory, and showcase the advantages of our framework over prior probabilistic methods and spherical deterministic embed-dings in various face recognition tasks.},
  keywords = {Benchmark testing,Computer vision,Face recognition,Geometry,Noise measurement,Probabilistic logic,Task analysis},
  annotation = {24 citations (Semantic Scholar/DOI) [2023-02-28]},
  file = {/home/david/Zotero/storage/NIHNCRQ7/Li et al. - 2021 - Spherical Confidence Learning for Face Recognition.pdf;/home/david/Zotero/storage/GUK4CMET/stamp.html}
}

@article{liSurveyConvolutionalNeural2022,
  title = {A {{Survey}} of {{Convolutional Neural Networks}}: {{Analysis}}, {{Applications}}, and {{Prospects}}},
  shorttitle = {A {{Survey}} of {{Convolutional Neural Networks}}},
  author = {Li, Zewen and Liu, Fan and Yang, Wenjie and Peng, Shouheng and Zhou, Jun},
  year = {2022},
  month = dec,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {33},
  number = {12},
  pages = {6999--7019},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2021.3084827},
  abstract = {A convolutional neural network (CNN) is one of the most significant networks in the deep learning field. Since CNN made impressive achievements in many areas, including but not limited to computer vision and natural language processing, it attracted much attention from both industry and academia in the past few years. The existing reviews mainly focus on CNN's applications in different scenarios without considering CNN from a general perspective, and some novel ideas proposed recently are not covered. In this review, we aim to provide some novel ideas and prospects in this fast-growing field. Besides, not only 2-D convolution but also 1-D and multidimensional ones are involved. First, this review introduces the history of CNN. Second, we provide an overview of various convolutions. Third, some classic and advanced CNN models are introduced; especially those key points making them reach state-of-the-art results. Fourth, through experimental analysis, we draw some conclusions and provide several rules of thumb for functions and hyperparameter selection. Fifth, the applications of 1-D, 2-D, and multidimensional convolution are covered. Finally, some open issues and promising directions for CNN are discussed as guidelines for future work.},
  keywords = {Computer vision,Convolutional neural networks,convolutional neural networks (CNNs),deep learning,Deep learning,deep neural networks,Feature extraction,Neurons},
  annotation = {357 citations (Semantic Scholar/DOI) [2023-04-07]},
  file = {/home/david/Zotero/storage/3JWJ27JB/Li et al. - 2022 - A Survey of Convolutional Neural Networks Analysi.pdf;/home/david/Zotero/storage/47ITFL48/stamp.html}
}

@inproceedings{liuAdaptiveFaceAdaptiveMargin2019,
  title = {{{AdaptiveFace}}: {{Adaptive Margin}} and {{Sampling}} for {{Face Recognition}}},
  shorttitle = {{{AdaptiveFace}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Liu, Hao and Zhu, Xiangyu and Lei, Zhen and Li, Stan Z.},
  year = {2019},
  month = jun,
  pages = {11939--11948},
  publisher = {{IEEE}},
  address = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.01222},
  urldate = {2023-02-28},
  abstract = {Training large-scale unbalanced data is the central topic in face recognition. In the past two years, face recognition has achieved remarkable improvements due to the introduction of margin based Softmax loss. However, these methods have an implicit assumption that all the classes possess sufficient samples to describe its distribution, so that a manually set margin is enough to equally squeeze each intraclass variations. However, real face datasets are highly unbalanced, which means the classes have tremendously different numbers of samples. In this paper, we argue that the margin should be adapted to different classes. We propose the Adaptive Margin Softmax to adjust the margins for different classes adaptively. In addition to the unbalance challenge, face data always consists of large-scale classes and samples. Smartly selecting valuable classes and samples to participate in the training makes the training more effective and efficient. To this end, we also make the sampling process adaptive in two folds: Firstly, we propose the Hard Prototype Mining to adaptively select a small number of hard classes to participate in classification. Secondly, for data sampling, we introduce the Adaptive Data Sampling to find valuable samples for training adaptively. We combine these three parts together as AdaptiveFace. Extensive analysis and experiments on LFW, LFW BLUFR and MegaFace show that our method performs better than state-of-the-art methods using the same network architecture and training dataset. Code is available at https: //github.com/haoliu1994/AdaptiveFace.},
  isbn = {978-1-72813-293-8},
  langid = {english},
  annotation = {105 citations (Semantic Scholar/DOI) [2023-02-28] 67 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/IDM76QXL/Liu et al. - 2019 - AdaptiveFace Adaptive Margin and Sampling for Fac.pdf}
}

@inproceedings{liuDAMDiscrepancyAlignment2021a,
  title = {{{DAM}}: {{Discrepancy Alignment Metric}} for {{Face Recognition}}},
  shorttitle = {{{DAM}}},
  booktitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Liu, Jiaheng and Wu, Yudong and Wu, Yichao and Li, Chuming and Hu, Xiaolin and Liang, Ding and Wang, Mengyu},
  year = {2021},
  month = oct,
  pages = {3794--3803},
  issn = {2380-7504},
  doi = {10.1109/ICCV48922.2021.00379},
  abstract = {The field of face recognition (FR) has witnessed remarkable progress with the surge of deep learning. The effective loss functions play an important role for FR. In this paper, we observe that a majority of loss functions, including the widespread triplet loss and softmax-based cross-entropy loss, embed inter-class (negative) similarity sn and intra-class (positive) similarity sp into similarity pairs and optimize to reduce (sn - sp) in the training process. However, in the verification process, existing metrics directly take the absolute similarity between two features as the confidence of belonging to the same identity, which inevitably causes a gap between the training and verification process. To bridge the gap, we propose a new metric called Discrepancy Alignment Metric (DAM) for verification, which introduces the Local Inter-class Discrepancy (LID) for each face image to normalize the absolute similarity score. To estimate the LID of each face image in the verification process, we propose two types of LID Estimation (LIDE) methods, which are reference-based and learning-based estimation methods, respectively. The proposed DAM is plug-and-play and can be easily applied to the most existing methods. Extensive experiments on multiple popular face recognition benchmark datasets demonstrate the effectiveness of our proposed method.},
  keywords = {Computer vision,Dams,Deep learning,Estimation,Face recognition,Faces,Measurement,Recognition and classification,Training},
  annotation = {7 citations (Semantic Scholar/DOI) [2023-02-28]},
  note = {Face verification},
  file = {/home/david/Zotero/storage/RPWAI4Z7/Liu et al. - 2021 - DAM Discrepancy Alignment Metric for Face Recogni.pdf;/home/david/Zotero/storage/DBKEUK89/stamp.html}
}

@misc{liuDeepLearningFace2015,
  title = {Deep {{Learning Face Attributes}} in the {{Wild}}},
  author = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
  year = {2015},
  month = sep,
  number = {arXiv:1411.7766},
  eprint = {1411.7766},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-25},
  abstract = {Predicting face attributes in the wild is challenging due to complex face variations. We propose a novel deep learning framework for attribute prediction in the wild. It cascades two CNNs, LNet and ANet, which are fine-tuned jointly with attribute tags, but pre-trained differently. LNet is pre-trained by massive general object categories for face localization, while ANet is pre-trained by massive face identities for attribute prediction. This framework not only outperforms the state-of-the-art with a large margin, but also reveals valuable facts on learning face representation. (1) It shows how the performances of face localization (LNet) and attribute prediction (ANet) can be improved by different pre-training strategies. (2) It reveals that although the filters of LNet are fine-tuned only with image-level attribute tags, their response maps over entire images have strong indication of face locations. This fact enables training LNet for face localization with only image-level annotations, but without face bounding boxes or landmarks, which are required by all attribute recognition works. (3) It also demonstrates that the high-level hidden neurons of ANet automatically discover semantic concepts after pre-training with massive face identities, and such concepts are significantly enriched after fine-tuning with attribute tags. Each attribute can be well explained with a sparse linear combination of these concepts.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {5753 citations (Semantic Scholar/arXiv) [2023-04-26]},
  note = {Comment: To appear in International Conference on Computer Vision (ICCV) 2015},
  file = {/home/david/Zotero/storage/YJVH6N72/Liu et al. - 2015 - Deep Learning Face Attributes in the Wild.pdf;/home/david/Zotero/storage/5BFJB64K/1411.html}
}

@inproceedings{liuFairLossMarginAware2019,
  title = {Fair {{Loss}}: {{Margin-Aware Reinforcement Learning}} for {{Deep Face Recognition}}},
  shorttitle = {Fair {{Loss}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Liu, Bingyu and Deng, Weihong and Zhong, Yaoyao and Wang, Mei and Hu, Jiani and Tao, Xunqiang and Huang, Yaohai},
  year = {2019},
  month = oct,
  pages = {10051--10060},
  publisher = {{IEEE}},
  address = {{Seoul, Korea (South)}},
  doi = {10.1109/ICCV.2019.01015},
  urldate = {2023-02-28},
  abstract = {Recently, large-margin softmax loss methods, such as angular softmax loss (SphereFace), large margin cosine loss (CosFace), and additive angular margin loss (ArcFace), have demonstrated impressive performance on deep face recognition. These methods incorporate a fixed additive margin to all the classes, ignoring the class imbalance problem. However, imbalanced problem widely exists in various real-world face datasets, in which samples from some classes are in a higher number than others. We argue that the number of a class would influence its demand for the additive margin. In this paper, we introduce a new margin-aware reinforcement learning based loss function, namely fair loss, in which each class will learn an appropriate adaptive margin by Deep Q-learning. Specifically, we train an agent to learn a margin adaptive strategy for each class, and make the additive margins for different classes more reasonable. Our method has better performance than present large-margin loss functions on three benchmarks, Labeled Face in the Wild (LFW), Youtube Faces (YTF) and MegaFace, which demonstrates that our method could learn better face representation on imbalanced face datasets.},
  isbn = {978-1-72814-803-8},
  langid = {english},
  annotation = {46 citations (Semantic Scholar/DOI) [2023-02-28] 29 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/XVA49MFG/Liu et al. - 2019 - Fair Loss Margin-Aware Reinforcement Learning for.pdf}
}

@misc{liuHAMBoxDelvingOnline2019,
  title = {{{HAMBox}}: {{Delving}} into {{Online High-quality Anchors Mining}} for {{Detecting Outer Faces}}},
  shorttitle = {{{HAMBox}}},
  author = {Liu, Yang and Tang, Xu and Wu, Xiang and Han, Junyu and Liu, Jingtuo and Ding, Errui},
  year = {2019},
  month = dec,
  number = {arXiv:1912.09231},
  eprint = {1912.09231},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-13},
  abstract = {Current face detectors utilize anchors to frame a multi-task learning problem which combines classification and bounding box regression. Effective anchor design and anchor matching strategy enable face detectors to localize faces under large pose and scale variations. However, we observe that more than 80\% correctly predicted bounding boxes are regressed from the unmatched anchors (the IoUs between anchors and target faces are lower than a threshold) in the inference phase. It indicates that these unmatched anchors perform excellent regression ability, but the existing methods neglect to learn from them. In this paper, we propose an Online High-quality Anchor Mining Strategy (HAMBox), which explicitly helps outer faces compensate with high-quality anchors. Our proposed HAMBox method could be a general strategy for anchor-based single-stage face detection. Experiments on various datasets, including WIDER FACE, FDDB, AFW and PASCAL Face, demonstrate the superiority of the proposed method. Furthermore, our team win the championship on the Face Detection test track of WIDER Face and Pedestrian Challenge 2019. We will release the codes with PaddlePaddle.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: 9 pages, 6 figures. arXiv admin note: text overlap with 1802.09058 by other authors},
  file = {/home/david/Zotero/storage/FX9EXWGQ/Liu et al. - 2019 - HAMBox Delving into Online High-quality Anchors M.pdf;/home/david/Zotero/storage/SSP9TIUW/1912.html}
}

@misc{liuLargeMarginSoftmaxLoss2017,
  title = {Large-{{Margin Softmax Loss}} for {{Convolutional Neural Networks}}},
  author = {Liu, Weiyang and Wen, Yandong and Yu, Zhiding and Yang, Meng},
  year = {2017},
  month = nov,
  number = {arXiv:1612.02295},
  eprint = {1612.02295},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-02-28},
  abstract = {Cross-entropy loss together with softmax is arguably one of the most common used supervision components in convolutional neural networks (CNNs). Despite its simplicity, popularity and excellent performance, the component does not explicitly encourage discriminative learning of features. In this paper, we propose a generalized large-margin softmax (L-Softmax) loss which explicitly encourages intra-class compactness and inter-class separability between learned features. Moreover, L-Softmax not only can adjust the desired margin but also can avoid overfitting. We also show that the L-Softmax loss can be optimized by typical stochastic gradient descent. Extensive experiments on four benchmark datasets demonstrate that the deeply-learned features with L-softmax loss become more discriminative, hence significantly boosting the performance on a variety of visual classification and verification tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {1079 citations (Semantic Scholar/arXiv) [2023-02-28]},
  note = {L-softmax},
  file = {/home/david/Zotero/storage/MXR57UIN/Liu et al. - 2017 - Large-Margin Softmax Loss for Convolutional Neural.pdf;/home/david/Zotero/storage/SCFBFSND/1612.html}
}

@inproceedings{liuLearningLearnDiverse2022,
  title = {Learning to {{Learn}} across {{Diverse Data Biases}} in {{Deep Face Recognition}}},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Liu, Chang and Yu, Xiang and Tsai, Yi-Hsuan and Faraki, Masoud and Moslemi, Ramin and Chandraker, Manmohan and Fu, Yun},
  year = {2022},
  month = jun,
  pages = {4062--4072},
  publisher = {{IEEE}},
  address = {{New Orleans, LA, USA}},
  doi = {10.1109/CVPR52688.2022.00404},
  urldate = {2023-02-28},
  isbn = {978-1-66546-946-3},
  langid = {english},
  annotation = {6 citations (Semantic Scholar/DOI) [2023-04-07] 2 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/7TJMRI35/Liu et al. - 2022 - Learning to Learn across Diverse Data Biases in De.pdf}
}

@misc{liuRethinkingFeatureDiscrimination2017,
  title = {Rethinking {{Feature Discrimination}} and {{Polymerization}} for {{Large-scale Recognition}}},
  author = {Liu, Yu and Li, Hongyang and Wang, Xiaogang},
  year = {2017},
  month = oct,
  number = {arXiv:1710.00870},
  eprint = {1710.00870},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1710.00870},
  urldate = {2023-02-28},
  abstract = {Feature matters. How to train a deep network to acquire discriminative features across categories and polymerized features within classes has always been at the core of many computer vision tasks, specially for large-scale recognition systems where test identities are unseen during training and the number of classes could be at million scale. In this paper, we address this problem based on the simple intuition that the cosine distance of features in high-dimensional space should be close enough within one class and far away across categories. To this end, we proposed the congenerous cosine (COCO) algorithm to simultaneously optimize the cosine similarity among data. It inherits the softmax property to make inter-class features discriminative as well as shares the idea of class centroid in metric learning. Unlike previous work where the center is a temporal, statistical variable within one mini-batch during training, the formulated centroid is responsible for clustering inner-class features to enforce them polymerized around the network truncus. COCO is bundled with discriminative training and learned end-to-end with stable convergence. Experiments on five benchmarks have been extensively conducted to verify the effectiveness of our approach on both small-scale classification task and large-scale human recognition problem.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {109 citations (Semantic Scholar/arXiv) [2023-02-28]},
  note = {CoCo Loss},
  file = {/home/david/Zotero/storage/8CCS4T7V/Liu et al. - 2017 - Rethinking Feature Discrimination and Polymerizati.pdf;/home/david/Zotero/storage/2N7WG8WG/1710.html}
}

@misc{liuSphereFaceDeepHypersphere2018,
  title = {{{SphereFace}}: {{Deep Hypersphere Embedding}} for {{Face Recognition}}},
  shorttitle = {{{SphereFace}}},
  author = {Liu, Weiyang and Wen, Yandong and Yu, Zhiding and Li, Ming and Raj, Bhiksha and Song, Le},
  year = {2018},
  month = jan,
  number = {arXiv:1704.08063},
  eprint = {1704.08063},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-02-28},
  abstract = {This paper addresses deep face recognition (FR) problem under open-set protocol, where ideal face features are expected to have smaller maximal intra-class distance than minimal inter-class distance under a suitably chosen metric space. However, few existing algorithms can effectively achieve this criterion. To this end, we propose the angular softmax (A-Softmax) loss that enables convolutional neural networks (CNNs) to learn angularly discriminative features. Geometrically, A-Softmax loss can be viewed as imposing discriminative constraints on a hypersphere manifold, which intrinsically matches the prior that faces also lie on a manifold. Moreover, the size of angular margin can be quantitatively adjusted by a parameter \$m\$. We further derive specific \$m\$ to approximate the ideal feature criterion. Extensive analysis and experiments on Labeled Face in the Wild (LFW), Youtube Faces (YTF) and MegaFace Challenge show the superiority of A-Softmax loss in FR tasks. The code has also been made publicly available.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {2116 citations (Semantic Scholar/arXiv) [2023-02-28]},
  note = {Comment: CVPR 2017 (v4: updated the Appendix)},
  file = {/home/david/Zotero/storage/3M8SEJJF/Liu et al. - 2018 - SphereFace Deep Hypersphere Embedding for Face Re.pdf;/home/david/Zotero/storage/TA73LZTU/1704.html}
}

@incollection{liuSSDSingleShot2016,
  title = {{{SSD}}: {{Single Shot MultiBox Detector}}},
  shorttitle = {{{SSD}}},
  author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
  year = {2016},
  volume = {9905},
  eprint = {1512.02325},
  primaryclass = {cs},
  pages = {21--37},
  doi = {10.1007/978-3-319-46448-0_2},
  urldate = {2023-04-13},
  abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For \$300\textbackslash times 300\$ input, SSD achieves 72.1\% mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for \$500\textbackslash times 500\$ input, SSD achieves 75.1\% mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at https://github.com/weiliu89/caffe/tree/ssd .},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: ECCV 2016},
  file = {/home/david/Zotero/storage/RE2MU8FP/Liu et al. - 2016 - SSD Single Shot MultiBox Detector.pdf;/home/david/Zotero/storage/SXCPRCBW/1512.html}
}

@misc{liuTargetingUltimateAccuracy2015,
  title = {Targeting {{Ultimate Accuracy}}: {{Face Recognition}} via {{Deep Embedding}}},
  shorttitle = {Targeting {{Ultimate Accuracy}}},
  author = {Liu, Jingtuo and Deng, Yafeng and Bai, Tao and Wei, Zhengping and Huang, Chang},
  year = {2015},
  month = jul,
  number = {arXiv:1506.07310},
  eprint = {1506.07310},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-02-28},
  abstract = {Face Recognition has been studied for many decades. As opposed to traditional hand-crafted features such as LBP and HOG, much more sophisticated features can be learned automatically by deep learning methods in a data-driven way. In this paper, we propose a two-stage approach that combines a multi-patch deep CNN and deep metric learning, which extracts low dimensional but very discriminative features for face verification and recognition. Experiments show that this method outperforms other state-of-the-art methods on LFW dataset, achieving 99.77\% pair-wise verification accuracy and significantly better accuracy under other two more practical protocols. This paper also discusses the importance of data size and the number of patches, showing a clear path to practical high-performance face recognition systems in real world.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {220 citations (Semantic Scholar/arXiv) [2023-02-28]},
  note = {Baidu},
  file = {/home/david/Zotero/storage/IB2WB2GE/Liu et al. - 2015 - Targeting Ultimate Accuracy Face Recognition via .pdf;/home/david/Zotero/storage/GZDFKD5J/1506.html}
}

@article{liuTwoStreamTransformerNetworks2018,
  title = {Two-{{Stream Transformer Networks}} for {{Video-Based Face Alignment}}},
  author = {Liu, Hao and Lu, Jiwen and Feng, Jianjiang and Zhou, Jie},
  year = {2018},
  month = nov,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {40},
  number = {11},
  pages = {2546--2554},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2017.2734779},
  abstract = {In this paper, we propose a two-stream transformer networks (TSTN) approach for video-based face alignment. Unlike conventional image-based face alignment approaches which cannot explicitly model the temporal dependency in videos and motivated by the fact that consistent movements of facial landmarks usually occur across consecutive frames, our TSTN aims to capture the complementary information of both the spatial appearance on still frames and the temporal consistency information across frames. To achieve this, we develop a two-stream architecture, which decomposes the video-based face alignment into spatial and temporal streams accordingly. Specifically, the spatial stream aims to transform the facial image to the landmark positions by preserving the holistic facial shape structure. Accordingly, the temporal stream encodes the video input as active appearance codes, where the temporal consistency information across frames is captured to help shape refinements. Experimental results on the benchmarking video-based face alignment datasets show very competitive performance of our method in comparisons to the state-of-the-arts.},
  keywords = {biometrics,convolutional neural networks,Face,Face alignment,face tracking,Indexes,Machine learning,recurrent neural networks,Shape,Streaming media,Transforms,Videos},
  file = {/home/david/Zotero/storage/27HLI3RD/Liu et al. - 2018 - Two-Stream Transformer Networks for Video-Based Fa.pdf;/home/david/Zotero/storage/CS3RVLF3/stamp.html}
}

@inproceedings{liVirFaceEnhancingFace2021,
  title = {{{VirFace}}: {{Enhancing Face Recognition}} via {{Unlabeled Shallow Data}}},
  shorttitle = {{{VirFace}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Li, Wenyu and Guo, Tianchu and Li, Pengyu and Chen, Binghui and Wang, Biao and Zuo, Wangmeng and Zhang, Lei},
  year = {2021},
  month = jun,
  pages = {14724--14733},
  publisher = {{IEEE}},
  address = {{Nashville, TN, USA}},
  doi = {10.1109/CVPR46437.2021.01449},
  urldate = {2023-02-28},
  abstract = {Recently, how to exploit unlabeled data for training face recognition models has been attracting increasing attention. However, few works consider the unlabeled shallow data1 in real-world scenarios. The existing semi-supervised face recognition methods that focus on generating pseudo labels or minimizing softmax classification probabilities of the unlabeled data do not work very well on the unlabeled shallow data. It is still a challenge on how to effectively utilize the unlabeled shallow face data to improve the performance of face recognition. In this paper, we propose a novel face recognition method, named VirFace, to effectively exploit the unlabeled shallow data for face recognition. VirFace consists of VirClass and VirInstance. Specifically, VirClass enlarges the inter-class distance by injecting the unlabeled data as new identities, while VirInstance produces virtual instances sampled from the learned distribution of each identity to further enlarge the inter-class distance. To the best of our knowledge, we are the first to tackle the problem of unlabeled shallow face data. Extensive experiments have been conducted on both the small- and large-scale datasets, e.g. LFW and IJB-C, etc, demonstrating the superiority of the proposed method.},
  isbn = {978-1-66544-509-2},
  langid = {english},
  annotation = {9 citations (Semantic Scholar/DOI) [2023-02-28] 6 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/HYB4A3SQ/Li et al. - 2021 - VirFace Enhancing Face Recognition via Unlabeled .pdf}
}

@inproceedings{liVirtualFullyConnectedLayer2021,
  title = {Virtual {{Fully-Connected Layer}}: {{Training}} a {{Large-Scale Face Recognition Dataset}} with {{Limited Computational Resources}}},
  shorttitle = {Virtual {{Fully-Connected Layer}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Li, Pengyu and Wang, Biao and Zhang, Lei},
  year = {2021},
  month = jun,
  pages = {13310--13319},
  publisher = {{IEEE}},
  address = {{Nashville, TN, USA}},
  doi = {10.1109/CVPR46437.2021.01311},
  urldate = {2023-02-28},
  abstract = {Recently, deep face recognition has achieved significant progress because of Convolutional Neural Networks (CNNs) and large-scale datasets. However, training CNNs on a large-scale face recognition dataset with limited computational resources is still a challenge. This is because the classification paradigm needs to train a fully-connected layer as the category classifier, and its parameters will be in the hundreds of millions if the training dataset contains millions of identities. This requires many computational resources, such as GPU memory. The metric learning paradigm is an economical computation method, but its performance is greatly inferior to that of the classification paradigm. To address this challenge, we propose a simple but effective CNN layer called the Virtual fully-connected (Virtual FC) layer to reduce the computational consumption of the classification paradigm. Without bells and whistles, the proposed Virtual FC reduces the parameters by more than 100 times with respect to the fully-connected layer and achieves competitive performance on mainstream face recognition evaluation datasets. Moreover, the performance of our Virtual FC layer on the evaluation datasets is superior to that of the metric learning paradigm by a significant margin. Our code will be released in hopes of disseminating our idea to other domains1.},
  isbn = {978-1-66544-509-2},
  langid = {english},
  annotation = {10 citations (Semantic Scholar/DOI) [2023-02-28] 4 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/5TJ6A3ZK/Li et al. - 2021 - Virtual Fully-Connected Layer Training a Large-Sc.pdf}
}

@article{martinezLocalEvidenceAggregation2013,
  title = {Local {{Evidence Aggregation}} for {{Regression-Based Facial Point Detection}}},
  author = {Martinez, Brais and Valstar, Michel F. and Binefa, Xavier and Pantic, Maja},
  year = {2013},
  month = may,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {35},
  number = {5},
  pages = {1149--1163},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2012.205},
  abstract = {We propose a new algorithm to detect facial points in frontal and near-frontal face images. It combines a regression-based approach with a probabilistic graphical model-based face shape model that restricts the search to anthropomorphically consistent regions. While most regression-based approaches perform a sequential approximation of the target location, our algorithm detects the target location by aggregating the estimates obtained from stochastically selected local appearance information into a single robust prediction. The underlying assumption is that by aggregating the different estimates, their errors will cancel out as long as the regressor inputs are uncorrelated. Once this new perspective is adopted, the problem is reformulated as how to optimally select the test locations over which the regressors are evaluated. We propose to extend the regression-based model to provide a quality measure of each prediction, and use the shape model to restrict and correct the sampling region. Our approach combines the low computational cost typical of regression-based approaches with the robustness of exhaustive-search approaches. The proposed algorithm was tested on over 7,500 images from five databases. Results showed significant improvement over the current state of the art.},
  keywords = {Face,Facial point detection,Feature extraction,object detection,Prediction algorithms,probabilistic graphical networks,Shape,Support vector machines,support vector regression,Training,Vectors},
  file = {/home/david/Zotero/storage/ACNQ6URN/Martinez et al. - 2013 - Local Evidence Aggregation for Regression-Based Fa.pdf;/home/david/Zotero/storage/WJP2VTZ8/stamp.html}
}

@inproceedings{masiPoseAwareFaceRecognition2016,
  title = {Pose-{{Aware Face Recognition}} in the {{Wild}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Masi, Iacopo and Rawls, Stephen and Medioni, Gerard and Natarajan, Prem},
  year = {2016},
  month = jun,
  pages = {4838--4846},
  publisher = {{IEEE}},
  address = {{Las Vegas, NV, USA}},
  doi = {10.1109/CVPR.2016.523},
  urldate = {2023-02-28},
  abstract = {We propose a method to push the frontiers of unconstrained face recognition in the wild, focusing on the problem of extreme pose variations. As opposed to current techniques which either expect a single model to learn pose invariance through massive amounts of training data, or which normalize images to a single frontal pose, our method explicitly tackles pose variation by using multiple posespecific models and rendered face images. We leverage deep Convolutional Neural Networks (CNNs) to learn discriminative representations we call Pose-Aware Models (PAMs) using 500K images from the CASIA WebFace dataset. We present a comparative evaluation on the new IARPA Janus Benchmark A (IJB-A) and PIPA datasets. On these datasets PAMs achieve remarkably better performance than commercial products and surprisingly also outperform methods that are specifically fine-tuned on the target dataset.},
  isbn = {978-1-4673-8851-1},
  langid = {english},
  annotation = {247 citations (Semantic Scholar/DOI) [2023-02-28] 162 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/YA3JMPUM/Masi et al. - 2016 - Pose-Aware Face Recognition in the Wild.pdf}
}

@article{mccarthyPROPOSALDARTMOUTHSUMMER,
  title = {A {{PROPOSAL FOR THE DARTMOUTH SUMMER RESEARCH PROJECT ON ARTIFICIAL INTELLIGENCE}}},
  author = {McCarthy, J and Minsky, M L and Rochester, N and Corporation, I B M and Shannon, C E},
  langid = {english},
  file = {/home/david/Zotero/storage/3BPHNKVQ/McCarthy et al. - A PROPOSAL FOR THE DARTMOUTH SUMMER RESEARCH PROJE.pdf}
}

@article{mccullochLOGICALCALCULUSIDEAS,
  title = {A {{LOGICAL CALCULUS OF THE IDEAS IMMANENT IN NERVOUS ACTIVITY}}},
  author = {Mcculloch, Warren S and Pitts, Walter},
  langid = {english},
  file = {/home/david/Zotero/storage/7MBILWB3/Mcculloch and Pitts - A LOGICAL CALCULUS OF THE IDEAS IMMANENT IN NERVOU.pdf}
}

@inproceedings{mengMagFaceUniversalRepresentation2021,
  title = {{{MagFace}}: {{A Universal Representation}} for {{Face Recognition}} and {{Quality Assessment}}},
  shorttitle = {{{MagFace}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Meng, Qiang and Zhao, Shichao and Huang, Zhida and Zhou, Feng},
  year = {2021},
  month = jun,
  pages = {14220--14229},
  publisher = {{IEEE}},
  address = {{Nashville, TN, USA}},
  doi = {10.1109/CVPR46437.2021.01400},
  urldate = {2023-02-28},
  abstract = {The performance of face recognition system degrades when the variability of the acquired faces increases. Prior work alleviates this issue by either monitoring the face quality in pre-processing or predicting the data uncertainty along with the face feature. This paper proposes MagFace, a category of losses that learn a universal feature embedding whose magnitude can measure the quality of the given face. Under the new loss, it can be proven that the magnitude of the feature embedding monotonically increases if the subject is more likely to be recognized. In addition, MagFace introduces an adaptive mechanism to learn a wellstructured within-class feature distributions by pulling easy samples to class centers while pushing hard samples away. This prevents models from overfitting on noisy low-quality samples and improves face recognition in the wild. Extensive experiments conducted on face recognition, quality assessments as well as clustering demonstrate its superiority over state-of-the-arts. The code is available at https://github.com/IrvingMeng/MagFace.},
  isbn = {978-1-66544-509-2},
  langid = {english},
  annotation = {159 citations (Semantic Scholar/DOI) [2023-02-28] 92 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/Q5Q25NL9/Meng et al. - 2021 - MagFace A Universal Representation for Face Recog.pdf}
}

@book{minaeeGoingDeeperFace2021,
  title = {Going {{Deeper Into Face Detection}}: {{A Survey}}},
  author = {Minaee, Shervin and Luo, Ping and Lin, Zhe and Bowyer, Kevin},
  year = {2021},
  month = mar
}

@book{minsky69perceptrons,
  title = {Perceptrons: {{An}} Introduction to Computational Geometry},
  author = {Minsky, Marvin and Papert, Seymour},
  year = {1969},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA, USA}},
  added-at = {2008-05-16T13:57:01.000+0200},
  description = {: mf : blob : \guillemotright{} bibtex},
  interhash = {d80d4948a422623047f1b800272c0389},
  intrahash = {06a5a6751b3e61408455fca2ed8d87fc},
  keywords = {linear-classification neural-networks seminal},
  timestamp = {2008-05-16T13:57:02.000+0200}
}

@article{moravecEvaluationFinalExamination2022,
  title = {Evaluation of Final Examination Performance at {{Czech University}} of {{Life Sciences}} during the {{COVID-19}} Outbreak},
  author = {Moravec, Luk{\'a}{\v s} and Je{\v c}m{\'i}nek, Jakub and Kukalov{\'a}, Gabriela},
  year = {2022},
  month = mar,
  journal = {Journal on Efficiency and Responsibility in Education and Science},
  volume = {15},
  number = {1},
  pages = {47--52},
  issn = {1803-1617},
  doi = {10.7160/eriesj.2022.150105},
  urldate = {2023-03-09},
  abstract = {The COVID-19 pandemic outbreak has upended the educational system worldwide, possibly with severe long-term consequences as most training institutions were forced to move to an online environment. Given the sudden transition to remote education, the main objective of this contribution is to evaluate the impact of distance education on examination results. We investigated the examination results of tax related subjects collected at the Czech University of Life Sciences in Prague during the period from 2014 to 2020. The sample consists of examination results of 120 different classes within 6 years with a total amount of 7268 observations. ~Firstly, we pivoted the data into the long format and performed binary logistic regression. Our findings suggest that the odds that student successfully passes the exam increases if the student was examined online compared to in-person. Additionally, we used KNN regression which enables us to predict success rate for an upcoming semester. According to our analysis, it is expected that on average 82 students out of 100 will successfully pass the exam. The model was calibrated using cross-validation to choose optimal K.},
  copyright = {Copyright (c) 2022 Luk\'a\v{s} Moravec, Jakub Je\v{c}m\'inek, Gabriela Kukalov\'a},
  langid = {english},
  keywords = {distance education},
  annotation = {0 citations (Semantic Scholar/DOI) [2023-04-07]},
  file = {/home/david/Zotero/storage/5PY58HAL/Moravec et al. - 2022 - Evaluation of final examination performance at Cze.pdf}
}

@misc{mugaluFaceRecognitionMethod2021,
  title = {Face {{Recognition}} as a {{Method}} of {{Authentication}} in a {{Web-Based System}}},
  author = {Mugalu, Ben Wycliff and Wamala, Rodrick Calvin and Serugunda, Jonathan and Katumba, Andrew},
  year = {2021},
  month = mar,
  number = {arXiv:2103.15144},
  eprint = {2103.15144},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-02-27},
  abstract = {Online information systems currently heavily rely on the username and password traditional method for protecting information and controlling access. With the advancement in biometric technology and popularity of fields like AI and Machine Learning, biometric security is becoming increasingly popular because of the usability advantage. This paper reports how machine learning based face recognition can be integrated into a web-based system as a method of authentication to reap the benefits of improved usability. This paper includes a comparison of combinations of detection and classification algorithms with FaceNet for face recognition. The results show that a combination of MTCNN for detection, Facenet for generating embeddings, and LinearSVC for classification outperforms other combinations with a 95\% accuracy. The resulting classifier is integrated into the web-based system and used for authenticating users.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  annotation = {0 citations (Semantic Scholar/arXiv) [2023-02-28]},
  note = {Comment: 7 pages, 9 figures, National Conference on Communications},
  file = {/home/david/Zotero/storage/UI3XRB6P/Mugalu et al. - 2021 - Face Recognition as a Method of Authentication in .pdf;/home/david/Zotero/storage/BRL8JLC6/2103.html}
}

@inproceedings{nechLevelPlayingField2017,
  title = {Level {{Playing Field}} for {{Million Scale Face Recognition}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Nech, Aaron and {Kemelmacher-Shlizerman}, Ira},
  year = {2017},
  month = jul,
  pages = {3406--3415},
  publisher = {{IEEE}},
  address = {{Honolulu, HI}},
  doi = {10.1109/CVPR.2017.363},
  urldate = {2023-02-28},
  isbn = {978-1-5386-0457-1},
  langid = {english},
  annotation = {150 citations (Semantic Scholar/DOI) [2023-02-28] 88 citations (Crossref) [2023-02-28]},
  note = {Datasets},
  file = {/home/david/Zotero/storage/N3SYF5AE/Nech and Kemelmacher-Shlizerman - 2017 - Level Playing Field for Million Scale Face Recogni.pdf}
}

@inproceedings{newell1959report,
  title = {Report on a General Problem Solving Program},
  booktitle = {{{IFIP}} Congress},
  author = {Newell, Allen and Shaw, John C and Simon, Herbert A},
  year = {1959},
  volume = {256},
  pages = {64},
  organization = {{Pittsburgh, PA}}
}

@article{niuResearchFaceRecognition2023,
  title = {Research on a Face Recognition Algorithm Based on {{3D}} Face Data and {{2D}} Face Image Matching},
  author = {Niu, Wenjie and Zhao, Yuankun and Yu, Zhiyan and Liu, Yu and Gong, Yu},
  year = {2023},
  month = mar,
  journal = {Journal of Visual Communication and Image Representation},
  volume = {91},
  pages = {103757},
  issn = {1047-3203},
  doi = {10.1016/j.jvcir.2023.103757},
  urldate = {2023-02-23},
  abstract = {Under the condition of weak light or no light, the recognition accuracy of the mature 2D face recognition technology decreases sharply. In this paper, a face recognition algorithm based on the matching of 3D face data and 2D face images is proposed. Firstly, 3D face data is reconstructed from the 2D face in the database based on the 3DMM algorithm, and the face depth image is obtained through orthogonal projection. Then, the average curvature map of the face depth image is used to enhance the data of the depth image. Finally, an improved residual neural network based on the depth image and curvature is designed to compare the scanned face with the face in the database. The method proposed in this paper is tested on the 3D face data in three public face datasets (Texas 3DFRD, FRGC v2.0, and Lock3DFace), and the recognition accuracy is 84.25\%, 83.39\%, and 78.24\%, respectively.},
  langid = {english},
  keywords = {3D face recognition,Data enhancement,Deep learning,Depth image},
  annotation = {0 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/BVE4S72J/Niu et al. - 2023 - Research on a face recognition algorithm based on .pdf;/home/david/Zotero/storage/WNCR6MDF/S104732032300007X.html}
}

@article{ohGPUImplementationNeural2004,
  title = {{{GPU}} Implementation of Neural Networks},
  author = {Oh, Kyoung-Su and Jung, Keechul},
  year = {2004},
  month = jun,
  journal = {Pattern Recognition},
  volume = {37},
  number = {6},
  pages = {1311--1314},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2004.01.013},
  abstract = {Graphics processing unit (GPU) is used for a faster artificial neural network. It is used to implement the matrix multiplication of a neural network to enhance the time performance of a text detection system. Preliminary results produced a 20-fold performance enhancement using an ATI RADEON 9700 PRO board. The parallelism of a GPU is fully utilized by accumulating a lot of input feature vectors and weight vectors, then converting the many inner-product operations into one matrix operation. Further research areas include benchmarking the performance with various hardware and GPU-aware learning algorithms.},
  keywords = {Graphics processing unit(GPU),Multi-layer perceptron,Neural network(NN),Text detection}
}

@article{p.n.belhumeurEigenfacesVsFisherfaces1997,
  title = {Eigenfaces vs. {{Fisherfaces}}: Recognition Using Class Specific Linear Projection},
  author = {{P. N. Belhumeur} and {J. P. Hespanha} and {D. J. Kriegman}},
  year = {1997},
  month = jul,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {19},
  number = {7},
  pages = {711--720},
  issn = {1939-3539},
  doi = {10.1109/34.598228}
}

@inproceedings{palDiscriminativeInvariantKernel2016,
  title = {Discriminative {{Invariant Kernel Features}}: {{A Bells-and-Whistles-Free Approach}} to {{Unsupervised Face Recognition}} and {{Pose Estimation}}},
  shorttitle = {Discriminative {{Invariant Kernel Features}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Pal, Dipan K. and {Juefei-Xu}, Felix and Savvides, Marios},
  year = {2016},
  month = jun,
  pages = {5590--5599},
  publisher = {{IEEE}},
  address = {{Las Vegas, NV, USA}},
  doi = {10.1109/CVPR.2016.603},
  urldate = {2023-02-28},
  abstract = {We propose an explicitly discriminative and `simple' approach to generate invariance to nuisance transformations modeled as unitary. In practice, the approach works well to handle non-unitary transformations as well. Our theoretical results extend the reach of a recent theory of invariance to discriminative and kernelized features based on unitary kernels. As a special case, a single common framework can be used to generate subject-specific pose-invariant features for face recognition and vice-versa for pose estimation. We show that our main proposed method (DIKF) can perform well under very challenging large-scale semisynthetic face matching and pose estimation protocols with unaligned faces using no landmarking whatsoever. We additionally benchmark on CMU MPIE and outperform previous work in almost all cases on off-angle face matching while we are on par with the previous state-of-the-art on the LFW unsupervised and image-restricted protocols, without any low-level image descriptors other than raw-pixels.},
  isbn = {978-1-4673-8851-1},
  langid = {english},
  annotation = {25 citations (Semantic Scholar/DOI) [2023-02-28] 10 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/VM9CXG34/Pal et al. - 2016 - Discriminative Invariant Kernel Features A Bells-.pdf}
}

@inproceedings{parkhiDeepFaceRecognition2015,
  title = {Deep {{Face Recognition}}},
  booktitle = {Procedings of the {{British Machine Vision Conference}} 2015},
  author = {Parkhi, Omkar M. and Vedaldi, Andrea and Zisserman, Andrew},
  year = {2015},
  pages = {41.1-41.12},
  publisher = {{British Machine Vision Association}},
  address = {{Swansea}},
  doi = {10.5244/C.29.41},
  urldate = {2023-02-27},
  abstract = {The goal of this paper is face recognition \textendash{} from either a single photograph or from a set of faces tracked in a video. Recent progress in this area has been due to two factors: (i) end to end learning for the task using a convolutional neural network (CNN), and (ii) the availability of very large scale training datasets.},
  isbn = {978-1-901725-53-7},
  langid = {english},
  annotation = {4491 citations (Semantic Scholar/DOI) [2023-02-28] 1868 citations (Crossref) [2023-02-28]},
  note = {VGG-Face},
  file = {/home/david/Zotero/storage/RGBI7S74/Parkhi et al. - 2015 - Deep Face Recognition.pdf}
}

@inproceedings{pengReconstructionBasedDisentanglementPoseInvariant2017,
  title = {Reconstruction-{{Based Disentanglement}} for {{Pose-Invariant Face Recognition}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Peng, Xi and Yu, Xiang and Sohn, Kihyuk and Metaxas, Dimitris N. and Chandraker, Manmohan},
  year = {2017},
  month = oct,
  pages = {1632--1641},
  publisher = {{IEEE}},
  address = {{Venice}},
  doi = {10.1109/ICCV.2017.180},
  urldate = {2023-02-28},
  isbn = {978-1-5386-1032-9},
  langid = {english},
  annotation = {119 citations (Semantic Scholar/DOI) [2023-02-28] 88 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/SU7GX6AM/Peng et al. - 2017 - Reconstruction-Based Disentanglement for Pose-Inva.pdf}
}

@inproceedings{popescuFaceVerificationChallenging2022,
  title = {Face {{Verification}} with {{Challenging Imposters}} and {{Diversified Demographics}}},
  booktitle = {2022 {{IEEE}}/{{CVF Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Popescu, Adrian and Stefan, Liviu-Daniel and {Deshayes-Chossart}, Jerome and Ionescu, Bogdan},
  year = {2022},
  month = jan,
  pages = {1151--1160},
  publisher = {{IEEE}},
  address = {{Waikoloa, HI, USA}},
  doi = {10.1109/WACV51458.2022.00122},
  urldate = {2023-02-28},
  abstract = {Face verification aims to distinguish between genuine and imposter pairs of faces, which include the same or different identities, respectively. The performance reported in recent years gives the impression that the task is practically solved. Here, we revisit the problem and argue that existing evaluation datasets were built using two oversimplifying design choices. First, the usual identity selection to form imposter pairs is not challenging enough because, in practice, verification is needed to detect challenging imposters. Second, the underlying demographics of existing datasets are often insufficient to account for the wide diversity of facial characteristics of people from across the world. To mitigate these limitations, we introduce the F aV CI2D dataset. Imposter pairs are challenging because they include visually similar faces selected from a large pool of demographically diversified identities. The dataset also includes metadata related to gender, country and age to facilitate fine-grained analysis of results. F aV CI2D is generated from freely distributable resources. Experiments with state-of-the-art deep models that provide nearly 100\% performance on existing datasets show a significant performance drop for F aV CI2D, confirming our starting hypothesis. Equally important, we analyze legal and ethical challenges which appeared in recent years and hindered the development of face analysis research. We introduce a series of design choices which address these challenges and make the dataset constitution and usage more sustainable and fairer. F aV CI2D is available at https: //github.com/AIM ultimediaLab/FaV CI2D-F ace- Verification- w ith- Challeng ing- Impo sters-and-Diversified-Demographics.},
  isbn = {978-1-66540-915-5},
  langid = {english},
  annotation = {1 citations (Semantic Scholar/DOI) [2023-04-18] 1 citations (Crossref) [2023-02-28]},
  note = {Datasets},
  file = {/home/david/Zotero/storage/R64LNC6V/Popescu et al. - 2022 - Face Verification with Challenging Imposters and D.pdf}
}

@misc{qiFaceRecognitionCentralized2018,
  title = {Face {{Recognition}} via {{Centralized Coordinate Learning}}},
  author = {Qi, Xianbiao and Zhang, Lei},
  year = {2018},
  month = jan,
  number = {arXiv:1801.05678},
  eprint = {1801.05678},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-02-28},
  abstract = {Owe to the rapid development of deep neural network (DNN) techniques and the emergence of large scale face databases, face recognition has achieved a great success in recent years. During the training process of DNN, the face features and classification vectors to be learned will interact with each other, while the distribution of face features will largely affect the convergence status of network and the face similarity computing in test stage. In this work, we formulate jointly the learning of face features and classification vectors, and propose a simple yet effective centralized coordinate learning (CCL) method, which enforces the features to be dispersedly spanned in the coordinate space while ensuring the classification vectors to lie on a hypersphere. An adaptive angular margin is further proposed to enhance the discrimination capability of face features. Extensive experiments are conducted on six face benchmarks, including those have large age gap and hard negative samples. Trained only on the small-scale CASIA Webface dataset with 460K face images from about 10K subjects, our CCL model demonstrates high effectiveness and generality, showing consistently competitive performance across all the six benchmark databases.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {26 citations (Semantic Scholar/arXiv) [2023-02-28]},
  note = {CCL},
  file = {/home/david/Zotero/storage/PQJIC4LL/Qi and Zhang - 2018 - Face Recognition via Centralized Coordinate Learni.pdf;/home/david/Zotero/storage/26PYW8TG/1801.html}
}

@misc{qiuSynFaceFaceRecognition2021,
  title = {{{SynFace}}: {{Face Recognition}} with {{Synthetic Data}}},
  shorttitle = {{{SynFace}}},
  author = {Qiu, Haibo and Yu, Baosheng and Gong, Dihong and Li, Zhifeng and Liu, Wei and Tao, Dacheng},
  year = {2021},
  month = dec,
  number = {arXiv:2108.07960},
  eprint = {2108.07960},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-02-28},
  abstract = {With the recent success of deep neural networks, remarkable progress has been achieved on face recognition. However, collecting large-scale real-world training data for face recognition has turned out to be challenging, especially due to the label noise and privacy issues. Meanwhile, existing face recognition datasets are usually collected from web images, lacking detailed annotations on attributes (e.g., pose and expression), so the influences of different attributes on face recognition have been poorly investigated. In this paper, we address the above-mentioned issues in face recognition using synthetic face images, i.e., SynFace. Specifically, we first explore the performance gap between recent state-of-the-art face recognition models trained with synthetic and real face images. We then analyze the underlying causes behind the performance gap, e.g., the poor intra-class variations and the domain gap between synthetic and real face images. Inspired by this, we devise the SynFace with identity mixup (IM) and domain mixup (DM) to mitigate the above performance gap, demonstrating the great potentials of synthetic data for face recognition. Furthermore, with the controllable face synthesis model, we can easily manage different factors of synthetic face generation, including pose, expression, illumination, the number of identities, and samples per identity. Therefore, we also perform a systematically empirical analysis on synthetic face images to provide some insights on how to effectively utilize synthetic data for face recognition.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {22 citations (Semantic Scholar/arXiv) [2023-02-28]},
  note = {Comment: Accepted by ICCV 2021. Code is available at https://github.com/haibo-qiu/SynFace},
  file = {/home/david/Zotero/storage/WTA6ZGYG/Qiu et al. - 2021 - SynFace Face Recognition with Synthetic Data.pdf;/home/david/Zotero/storage/BX8ZNYPX/2108.html}
}

@inproceedings{rainaLargescaleDeepUnsupervised2009b,
  title = {Large-Scale Deep Unsupervised Learning Using Graphics Processors},
  booktitle = {Proceedings of the 26th {{Annual International Conference}} on {{Machine Learning}}},
  author = {Raina, Rajat and Madhavan, Anand and Ng, Andrew Y.},
  year = {2009},
  month = jun,
  series = {{{ICML}} '09},
  pages = {873--880},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1553374.1553486},
  urldate = {2023-01-25},
  abstract = {The promise of unsupervised learning methods lies in their potential to use vast amounts of unlabeled data to learn complex, highly nonlinear models with millions of free parameters. We consider two well-known unsupervised learning models, deep belief networks (DBNs) and sparse coding, that have recently been applied to a flurry of machine learning applications (Hinton \& Salakhutdinov, 2006; Raina et al., 2007). Unfortunately, current learning algorithms for both models are too slow for large-scale applications, forcing researchers to focus on smaller-scale models, or to use fewer training examples. In this paper, we suggest massively parallel methods to help resolve these problems. We argue that modern graphics processors far surpass the computational capabilities of multicore CPUs, and have the potential to revolutionize the applicability of deep unsupervised learning methods. We develop general principles for massively parallelizing unsupervised learning tasks using graphics processors. We show that these principles can be applied to successfully scaling up learning algorithms for both DBNs and sparse coding. Our implementation of DBN learning is up to 70 times faster than a dual-core CPU implementation for large models. For example, we are able to reduce the time required to learn a four-layer DBN with 100 million free parameters from several weeks to around a single day. For sparse coding, we develop a simple, inherently parallel algorithm, that leads to a 5 to 15-fold speedup over previous methods.},
  isbn = {978-1-60558-516-1},
  file = {/home/david/Zotero/storage/QDRZR76Z/Raina et al. - 2009 - Large-scale deep unsupervised learning using graph.pdf}
}

@article{ranjanDeepLearningUnderstanding2018,
  title = {Deep {{Learning}} for {{Understanding Faces}}: {{Machines May Be Just}} as {{Good}}, or {{Better}}, than {{Humans}}},
  shorttitle = {Deep {{Learning}} for {{Understanding Faces}}},
  author = {Ranjan, Rajeev and Sankaranarayanan, Swami and Bansal, Ankan and Bodla, Navaneeth and Chen, Jun-Cheng and Patel, Vishal M. and Castillo, Carlos D. and Chellappa, Rama},
  year = {2018},
  month = jan,
  journal = {IEEE Signal Processing Magazine},
  volume = {35},
  number = {1},
  pages = {66--83},
  issn = {1558-0792},
  doi = {10.1109/MSP.2017.2764116},
  abstract = {Recent developments in deep convolutional neural networks (DCNNs) have shown impressive performance improvements on various object detection/recognition problems. This has been made possible due to the availability of large annotated data and a better understanding of the nonlinear mapping between images and class labels, as well as the affordability of powerful graphics processing units (GPUs). These developments in deep learning have also improved the capabilities of machines in understanding faces and automatically executing the tasks of face detection, pose estimation, landmark localization, and face recognition from unconstrained images and videos. In this article, we provide an overview of deep-learning methods used for face recognition. We discuss different modules involved in designing an automatic face recognition system and the role of deep learning for each of them. Some open issues regarding DCNNs for face recognition problems are then discussed. This article should prove valuable to scientists, engineers, and end users working in the fields of face recognition, security, visual surveillance, and biometrics.},
  keywords = {Biometrics,Detectors,Face recognition,Feature extraction,Graphics processing,Machine learning,Neural networks,Pose estimation,Videos},
  file = {/home/david/Zotero/storage/X93F6B7D/Ranjan et al. - 2018 - Deep Learning for Understanding Faces Machines Ma.pdf;/home/david/Zotero/storage/3CJIWQ64/stamp.html}
}

@misc{ranjanL2constrainedSoftmaxLoss2017,
  title = {L2-Constrained {{Softmax Loss}} for {{Discriminative Face Verification}}},
  author = {Ranjan, Rajeev and Castillo, Carlos D. and Chellappa, Rama},
  year = {2017},
  month = jun,
  number = {arXiv:1703.09507},
  eprint = {1703.09507},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-02-28},
  abstract = {In recent years, the performance of face verification systems has significantly improved using deep convolutional neural networks (DCNNs). A typical pipeline for face verification includes training a deep network for subject classification with softmax loss, using the penultimate layer output as the feature descriptor, and generating a cosine similarity score given a pair of face images. The softmax loss function does not optimize the features to have higher similarity score for positive pairs and lower similarity score for negative pairs, which leads to a performance gap. In this paper, we add an L2-constraint to the feature descriptors which restricts them to lie on a hypersphere of a fixed radius. This module can be easily implemented using existing deep learning frameworks. We show that integrating this simple step in the training pipeline significantly boosts the performance of face verification. Specifically, we achieve state-of-the-art results on the challenging IJB-A dataset, achieving True Accept Rate of 0.909 at False Accept Rate 0.0001 on the face verification protocol. Additionally, we achieve state-of-the-art performance on LFW dataset with an accuracy of 99.78\%, and competing performance on YTF dataset with accuracy of 96.08\%.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {383 citations (Semantic Scholar/arXiv) [2023-02-28]},
  note = {L2-softmax
\par
Loss functions},
  file = {/home/david/Zotero/storage/FSLTZPBA/Ranjan et al. - 2017 - L2-constrained Softmax Loss for Discriminative Fac.pdf;/home/david/Zotero/storage/2FSGKAYP/1703.html}
}

@inproceedings{ranzatoEfficientLearningSparse2006,
  title = {Efficient {{Learning}} of {{Sparse Representations}} with an {{Energy-Based Model}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {aurelio Ranzato, Marc' and Poultney, Christopher and Chopra, Sumit and Cun, Yann},
  year = {2006},
  volume = {19},
  publisher = {{MIT Press}},
  urldate = {2023-01-25},
  abstract = {We describe a novel unsupervised method for learning sparse, overcomplete features. The model uses a linear encoder, and a linear decoder preceded by a sparsifying non-linearity that turns a code vector into a quasi-binary sparse code vector. Given an input, the optimal code minimizes the distance between the output of the decoder and the input patch while being as similar as possible to the encoder output. Learning proceeds in a two-phase EM-like fashion: (1) compute the minimum-energy code vector, (2) adjust the parameters of the encoder and decoder so as to decrease the energy. The model produces "stroke detectors" when trained on handwritten numerals, and Gabor-like filters when trained on natural image patches. Inference and learning are very fast, requiring no preprocessing, and no expensive sampling. Using the proposed unsupervised method to initialize the first layer of a convolutional network, we achieved an error rate slightly lower than the best reported result on the MNIST dataset. Finally, an extension of the method is described to learn topographical filter maps.},
  file = {/home/david/Zotero/storage/ZAKI3IQG/Ranzato et al. - 2006 - Efficient Learning of Sparse Representations with .pdf}
}

@inproceedings{raoAttentionAwareDeepReinforcement2017,
  title = {Attention-{{Aware Deep Reinforcement Learning}} for {{Video Face Recognition}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Rao, Yongming and Lu, Jiwen and Zhou, Jie},
  year = {2017},
  month = oct,
  pages = {3951--3960},
  publisher = {{IEEE}},
  address = {{Venice}},
  doi = {10.1109/ICCV.2017.424},
  urldate = {2023-02-28},
  abstract = {In this paper, we propose an attention-aware deep reinforcement learning (ADRL) method for video face recognition, which aims to discard the misleading and confounding frames and find the focuses of attentions in face videos for person recognition. We formulate the process of finding the attentions of videos as a Markov decision process and train the attention model through a deep reinforcement learning framework without using extra labels. Unlike existing attention models, our method takes information from both the image space and the feature space as the input to make better use of face information that is discarded in the feature learning process. Besides, our approach is attention-aware, which seeks different attentions of videos for the recognition of different pairs of videos. Our approach achieves very competitive video face recognition performance on three widely used video face datasets.},
  isbn = {978-1-5386-1032-9},
  langid = {english},
  annotation = {138 citations (Semantic Scholar/DOI) [2023-02-28] 96 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/BDRZ9A59/Rao et al. - 2017 - Attention-Aware Deep Reinforcement Learning for Vi.pdf}
}

@inproceedings{raoLearningDiscriminativeAggregation2017,
  title = {Learning {{Discriminative Aggregation Network}} for {{Video-Based Face Recognition}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Rao, Yongming and Lin, Ji and Lu, Jiwen and Zhou, Jie},
  year = {2017},
  month = oct,
  pages = {3801--3810},
  publisher = {{IEEE}},
  address = {{Venice}},
  doi = {10.1109/ICCV.2017.408},
  urldate = {2023-02-28},
  abstract = {In this paper, we propose a discriminative aggregation network (DAN) method for video face recognition, which aims to integrate information from video frames effectively and efficiently. Unlike existing aggregation methods, our method aggregates raw video frames directly instead of the features obtained by complex processing. By combining the idea of metric learning and adversarial learning, we learn an aggregation network that produces more discriminative synthesized images compared to raw input frames. Our framework reduces the number of frames to be processed and significantly speed up the recognition procedure. Furthermore, low-quality frames containing misleading information are filtered and denoised during the aggregation process, which makes our system more robust and discriminative. Experimental results show that our method can generate discriminative images from video clips and improve the overall recognition performance in both the speed and accuracy on three widely used datasets.},
  isbn = {978-1-5386-1032-9},
  langid = {english},
  annotation = {34 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/VNJT2I6P/Rao et al. - 2017 - Learning Discriminative Aggregation Network for Vi.pdf}
}

@misc{renFasterRCNNRealTime2016,
  title = {Faster {{R-CNN}}: {{Towards Real-Time Object Detection}} with {{Region Proposal Networks}}},
  shorttitle = {Faster {{R-CNN}}},
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  year = {2016},
  month = jan,
  number = {arXiv:1506.01497},
  eprint = {1506.01497},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-13},
  abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Extended tech report},
  file = {/home/david/Zotero/storage/YNBMUM8A/Ren et al. - 2016 - Faster R-CNN Towards Real-Time Object Detection w.pdf;/home/david/Zotero/storage/EWPELMLX/1506.html}
}

@article{rochesterTestsCellAssembly1956,
  title = {Tests on a Cell Assembly Theory of the Action of the Brain, Using a Large Digital Computer},
  author = {Rochester, N. and Holland, J. and Haibt, L. and Duda, W.},
  year = {1956},
  journal = {IRE Transactions on Information Theory},
  volume = {2},
  number = {3},
  pages = {80--93},
  doi = {10.1109/TIT.1956.1056810}
}

@article{rosenblattPerceptronProbabilisticModel1958,
  title = {The Perceptron: {{A}} Probabilistic Model for Information Storage and Organization in the Brain.},
  author = {Rosenblatt, F.},
  year = {1958},
  journal = {Psychological Review},
  volume = {65},
  pages = {386--408},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1471(Electronic),0033-295X(Print)},
  doi = {10.1037/h0042519},
  abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {*Brain,*Cognition,*Memory,Nervous System}
}

@book{rosenblattPrinciplesNeurodynamicsPerceptrons1962,
  title = {Principles of {{Neurodynamics}}: {{Perceptrons}} and the {{Theory}} of {{Brain Mechanisms}}},
  shorttitle = {Principles of {{Neurodynamics}}},
  author = {Rosenblatt, Frank},
  year = {1962},
  publisher = {{Spartan Books}},
  googlebooks = {7FhRAAAAMAAJ},
  langid = {english}
}

@inproceedings{ruizSimulatedAdversarialTesting2022,
  title = {Simulated {{Adversarial Testing}} of {{Face Recognition Models}}},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Ruiz, Nataniel and Kortylewski, Adam and Qiu, Weichao and Xie, Cihang and Bargal, Sarah Adel and Yuille, Alan and Sclaroff, Stan},
  year = {2022},
  month = jun,
  pages = {4135--4145},
  publisher = {{IEEE}},
  address = {{New Orleans, LA, USA}},
  doi = {10.1109/CVPR52688.2022.00411},
  urldate = {2023-02-28},
  abstract = {Most machine learning models are validated and tested on fixed datasets. This can give an incomplete picture of the capabilities and weaknesses of the model. Such weaknesses can be revealed at test time in the real world. The risks involved in such failures can be loss of profits, loss of time or even loss of life in certain critical applications. In order to alleviate this issue, simulators can be controlled in a finegrained manner using interpretable parameters to explore the semantic image manifold. In this work, we propose a framework for learning how to test machine learning algorithms using simulators in an adversarial manner in order to find weaknesses in the model before deploying it in critical scenarios. We apply this method in a face recognition setup. We show that certain weaknesses of models trained on real data can be discovered using simulated samples. Using our proposed method, we can find adversarial synthetic faces that fool contemporary face recognition models. This demonstrates the fact that these models have weaknesses that are not measured by commonly used validation datasets. We hypothesize that this type of adversarial examples are not isolated, but usually lie in connected spaces in the latent space of the simulator. We present a method to find these adversarial regions as opposed to the typical adversarial points found in the adversarial example literature.},
  isbn = {978-1-66546-946-3},
  langid = {english},
  annotation = {3 citations (Semantic Scholar/DOI) [2023-04-07] 0 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/I36WJK6K/Ruiz et al. - 2022 - Simulated Adversarial Testing of Face Recognition .pdf}
}

@misc{rumelhart1986learning,
  title = {Learning Internal Representations by Error Propagation", in\textbackslash{{Parallel Distributed Processing}}", {{DE}} Rumelhart, {{JL McClelland}} Eds},
  author = {Rumelhart, {\relax DE} and Hinton, {\relax GE} and Williams, {\relax RJ}},
  year = {1986},
  publisher = {{MIT Press, Cambridge}}
}

@article{schmidhuberDeepLearningNeural2015,
  title = {Deep Learning in Neural Networks: {{An}} Overview},
  shorttitle = {Deep Learning in Neural Networks},
  author = {Schmidhuber, J{\"u}rgen},
  year = {2015},
  month = jan,
  journal = {Neural Networks},
  volume = {61},
  pages = {85--117},
  issn = {08936080},
  doi = {10.1016/j.neunet.2014.09.003},
  urldate = {2023-01-24},
  abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
  langid = {english},
  file = {/home/david/Zotero/storage/PT65D7X6/Schmidhuber - 2015 - Deep learning in neural networks An overview.pdf}
}

@inproceedings{schroffFaceNetUnifiedEmbedding2015,
  title = {{{FaceNet}}: {{A Unified Embedding}} for {{Face Recognition}} and {{Clustering}}},
  shorttitle = {{{FaceNet}}},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
  year = {2015},
  month = jun,
  eprint = {1503.03832},
  primaryclass = {cs},
  pages = {815--823},
  doi = {10.1109/CVPR.2015.7298682},
  urldate = {2023-02-16},
  abstract = {Despite significant recent advances in the field of face recognition, implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings as feature vectors. Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches. To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method. The benefit of our approach is much greater representational efficiency: we achieve state-of-the-art face recognition performance using only 128-bytes per face. On the widely used Labeled Faces in the Wild (LFW) dataset, our system achieves a new record accuracy of 99.63\%. On YouTube Faces DB it achieves 95.12\%. Our system cuts the error rate in comparison to the best published result by 30\% on both datasets. We also introduce the concept of harmonic embeddings, and a harmonic triplet loss, which describe different versions of face embeddings (produced by different networks) that are compatible to each other and allow for direct comparison between each other.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {9739 citations (Semantic Scholar/arXiv) [2023-02-28] 9739 citations (Semantic Scholar/DOI) [2023-02-28] 5901 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/RG8NJWND/Schroff et al. - 2015 - FaceNet A Unified Embedding for Face Recognition .pdf;/home/david/Zotero/storage/FCDK6MYZ/1503.html}
}

@inproceedings{senguptaFrontalProfileFace2016,
  title = {Frontal to Profile Face Verification in the Wild},
  booktitle = {2016 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Sengupta, Soumyadip and Chen, Jun-Cheng and Castillo, Carlos and Patel, Vishal M. and Chellappa, Rama and Jacobs, David W.},
  year = {2016},
  month = mar,
  pages = {1--9},
  doi = {10.1109/WACV.2016.7477558},
  abstract = {We have collected a new face data set that will facilitate research in the problem of frontal to profile face verification `in the wild'. The aim of this data set is to isolate the factor of pose variation in terms of extreme poses like profile, where many features are occluded, along with other `in the wild' variations. We call this data set the Celebrities in Frontal-Profile (CFP) data set. We find that human performance on Frontal-Profile verification in this data set is only slightly worse (94.57\% accuracy) than that on Frontal-Frontal verification (96.24\% accuracy). However we evaluated many state-of-the-art algorithms, including Fisher Vector, Sub-SML and a Deep learning algorithm. We observe that all of them degrade more than 10\% from Frontal-Frontal to Frontal-Profile verification. The Deep learning implementation, which performs comparable to humans on Frontal-Frontal, performs significantly worse (84.91\% accuracy) on Frontal-Profile. This suggests that there is a gap between human performance and automatic face recognition methods for large pose variation in unconstrained images.},
  keywords = {Face,Face recognition,Feature extraction,Machine learning,Measurement,Protocols,Training data},
  annotation = {462 citations (Semantic Scholar/DOI) [2023-02-28] 303 citations (Crossref) [2023-02-28]},
  note = {Celebrities in Frontal Profile (CFP)
\par
Datasets},
  file = {/home/david/Zotero/storage/ZKYRD3D7/Sengupta et al. - 2016 - Frontal to profile face verification in the wild.pdf;/home/david/Zotero/storage/EC37SFAQ/stamp.html}
}

@inproceedings{shiUniversalRepresentationLearning2020,
  title = {Towards {{Universal Representation Learning}} for {{Deep Face Recognition}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Shi, Yichun and Yu, Xiang and Sohn, Kihyuk and Chandraker, Manmohan and Jain, Anil K.},
  year = {2020},
  month = jun,
  pages = {6816--6825},
  publisher = {{IEEE}},
  address = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.00685},
  urldate = {2023-02-28},
  abstract = {Recognizing faces in the wild is extremely hard as they appear with diverse variations. Traditional methods either train with specifically annotated target domain data which contains the variations, or introduce unlabeled target domain data to adapt from the training domain. Instead, we propose a universal representation learning face recognition framework, URFace, that can deal with larger variations unseen in the given training data, without leveraging knowledge of the target domain. We firstly synthesize the training data that corresponds to several semantically meaningful variations, such as low resolution, occlusion and head pose. However, directly using the augmented data hinders training convergence, since the augmented samples are usually hard examples. We propose to split the feature embedding into multiple sub-embeddings and associate different confidence values for each sub-embedding to smooth the training procedure. The sub-embeddings are further decorrelated by regularizing classification loss on variations and adversarial loss on different partitions of them. Experiments show that our method achieves state-of-the-art performance on general face recognition datasets such as LFW and MegaFace, while being significantly better on extreme benchmarks such as TinyFace and IJB-S.},
  isbn = {978-1-72817-168-5},
  langid = {english},
  annotation = {84 citations (Semantic Scholar/DOI) [2023-02-28] 44 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/NTTK982S/Shi et al. - 2020 - Towards Universal Representation Learning for Deep.pdf}
}

@inproceedings{simardBestPracticesConvolutional2003,
  title = {Best Practices for Convolutional Neural Networks Applied to Visual Document Analysis},
  booktitle = {Seventh {{International Conference}} on {{Document Analysis}} and {{Recognition}}, 2003. {{Proceedings}}.},
  author = {Simard, P.Y. and Steinkraus, D. and Platt, J.C.},
  year = {2003},
  volume = {1},
  pages = {958--963},
  publisher = {{IEEE Comput. Soc}},
  address = {{Edinburgh, UK}},
  doi = {10.1109/ICDAR.2003.1227801},
  urldate = {2023-01-25},
  abstract = {Neural networks are a powerful technology for classification of visual inputs arising from documents. However, there is a confusing plethora of different neural network methods that are used in the literature and in industry. This paper describes a set of concrete best practices that document analysis researchers can use to get good results with neural networks. The most important practice is getting a training set as large as possible: we expand the training set by adding a new form of distorted data. The next most important practice is that convolutional neural networks are better suited for visual document tasks than fully connected networks. We propose that a simple ``do-it-yourself'' implementation of convolution with a flexible architecture is suitable for many visual document problems. This simple convolutional neural network does not require complex methods, such as momentum, weight decay, structuredependent learning rates, averaging layers, tangent prop, or even finely-tuning the architecture. The end result is a very simple yet general architecture which can yield state-of-the-art performance for document analysis. We illustrate our claims on the MNIST set of English digit images.},
  isbn = {978-0-7695-1960-9},
  langid = {english},
  file = {/home/david/Zotero/storage/M3Z6FERC/Simard et al. - 2003 - Best practices for convolutional neural networks a.pdf}
}

@inproceedings{sohnUnsupervisedDomainAdaptation2017,
  title = {Unsupervised {{Domain Adaptation}} for {{Face Recognition}} in {{Unlabeled Videos}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Sohn, Kihyuk and Liu, Sifei and Zhong, Guangyu and Yu, Xiang and Yang, Ming-Hsuan and Chandraker, Manmohan},
  year = {2017},
  month = oct,
  pages = {5917--5925},
  publisher = {{IEEE}},
  address = {{Venice}},
  doi = {10.1109/ICCV.2017.630},
  urldate = {2023-02-28},
  abstract = {Despite rapid advances in face recognition, there remains a clear gap between the performance of still image-based face recognition and video-based face recognition, due to the vast difference in visual quality between the domains and the difficulty of curating diverse large-scale video datasets. This paper addresses both of those challenges, through an image to video feature-level domain adaptation approach, to learn discriminative video frame representations. The framework utilizes large-scale unlabeled video data to reduce the gap between different domains while transferring discriminative knowledge from large-scale labeled still images. Given a face recognition network that is pretrained in the image domain, the adaptation is achieved by (i) distilling knowledge from the network to a video adaptation network through feature matching, (ii) performing feature restoration through synthetic data augmentation and (iii) learning a domain-invariant feature through a domain adversarial discriminator. We further improve performance through a discriminator-guided feature fusion that boosts high-quality frames while eliminating those degraded by video domainspecific factors. Experiments on the YouTube Faces and IJB-A datasets demonstrate that each module contributes to our feature-level domain adaptation framework and substantially improves video face recognition performance to achieve state-of-the-art accuracy. We demonstrate qualitatively that the network learns to suppress diverse artifacts in videos such as pose, illumination or occlusion without being explicitly trained for them.},
  isbn = {978-1-5386-1032-9},
  langid = {english},
  annotation = {105 citations (Semantic Scholar/DOI) [2023-02-28] 67 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/G4TYLTZD/Sohn et al. - 2017 - Unsupervised Domain Adaptation for Face Recognitio.pdf}
}

@misc{songOcclusionRobustFace2019,
  title = {Occlusion {{Robust Face Recognition Based}} on {{Mask Learning}} with {{PairwiseDifferential Siamese Network}}},
  author = {Song, Lingxue and Gong, Dihong and Li, Zhifeng and Liu, Changsong and Liu, Wei},
  year = {2019},
  month = aug,
  number = {arXiv:1908.06290},
  eprint = {1908.06290},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-02-28},
  abstract = {Deep Convolutional Neural Networks (CNNs) have been pushing the frontier of the face recognition research in the past years. However, existing general CNN face models generalize poorly to the scenario of occlusions on variable facial areas. Inspired by the fact that a human visual system explicitly ignores occlusions and only focuses on non-occluded facial areas, we propose a mask learning strategy to find and discard the corrupted feature elements for face recognition. A mask dictionary is firstly established by exploiting the differences between the top convoluted features of occluded and occlusion-free face pairs using an innovatively designed Pairwise Differential Siamese Network (PDSN). Each item of this dictionary captures the correspondence between occluded facial areas and corrupted feature elements, which is named Feature Discarding Mask (FDM). When dealing with a face image with random partial occlusions, we generate its FDM by combining relevant dictionary items and then multiply it with the original features to eliminate those corrupted feature elements. Comprehensive experiments on both synthesized and realistic occluded face datasets show that the proposed approach significantly outperforms the state-of-the-arts.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {123 citations (Semantic Scholar/arXiv) [2023-02-28]},
  file = {/home/david/Zotero/storage/I6HHQ9AI/Song et al. - 2019 - Occlusion Robust Face Recognition Based on Mask Le.pdf;/home/david/Zotero/storage/H79PZAG7/1908.html}
}

@article{stallkampManVsComputer2012,
  title = {Man vs. Computer: {{Benchmarking}} Machine Learning Algorithms for Traffic Sign Recognition},
  shorttitle = {Man vs. Computer},
  author = {Stallkamp, J. and Schlipsing, M. and Salmen, J. and Igel, C.},
  year = {2012},
  month = aug,
  journal = {Neural Networks},
  series = {Selected {{Papers}} from {{IJCNN}} 2011},
  volume = {32},
  pages = {323--332},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2012.02.016},
  urldate = {2023-01-25},
  abstract = {Traffic signs are characterized by a wide variability in their visual appearance in real-world environments. For example, changes of illumination, varying weather conditions and partial occlusions impact the perception of road signs. In practice, a large number of different sign classes needs to be recognized with very high accuracy. Traffic signs have been designed to be easily readable for humans, who perform very well at this task. For computer systems, however, classifying traffic signs still seems to pose a challenging pattern recognition problem. Both image processing and machine learning algorithms are continuously refined to improve on this task. But little systematic comparison of such systems exist. What is the status quo? Do today's algorithms reach human performance? For assessing the performance of state-of-the-art machine learning algorithms, we present a publicly available traffic sign dataset with more than 50,000 images of German road signs in 43 classes. The data was considered in the second stage of the German Traffic Sign Recognition Benchmark held at IJCNN 2011. The results of this competition are reported and the best-performing algorithms are briefly described. Convolutional neural networks (CNNs) showed particularly high classification accuracies in the competition. We measured the performance of human subjects on the same data\textemdash and the CNNs outperformed the human test persons.},
  langid = {english},
  keywords = {Benchmarking,Convolutional neural networks,Machine learning,Traffic sign recognition},
  file = {/home/david/Zotero/storage/F7DQ68KG/Stallkamp et al. - 2012 - Man vs. computer Benchmarking machine learning al.pdf;/home/david/Zotero/storage/W7ICGVVM/S0893608012000457.html}
}

@misc{sunDeepID3FaceRecognition2015,
  title = {{{DeepID3}}: {{Face Recognition}} with {{Very Deep Neural Networks}}},
  shorttitle = {{{DeepID3}}},
  author = {Sun, Yi and Liang, Ding and Wang, Xiaogang and Tang, Xiaoou},
  year = {2015},
  month = feb,
  number = {arXiv:1502.00873},
  eprint = {1502.00873},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-02-27},
  abstract = {The state-of-the-art of face recognition has been significantly advanced by the emergence of deep learning. Very deep neural networks recently achieved great success on general object recognition because of their superb learning capacity. This motivates us to investigate their effectiveness on face recognition. This paper proposes two very deep neural network architectures, referred to as DeepID3, for face recognition. These two architectures are rebuilt from stacked convolution and inception layers proposed in VGG net and GoogLeNet to make them suitable to face recognition. Joint face identification-verification supervisory signals are added to both intermediate and final feature extraction layers during training. An ensemble of the proposed two architectures achieves 99.53\% LFW face verification accuracy and 96.0\% LFW rank-1 face identification accuracy, respectively. A further discussion of LFW face verification result is given in the end.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {873 citations (Semantic Scholar/arXiv) [2023-02-28]},
  file = {/home/david/Zotero/storage/F6ILL9Q4/Sun et al. - 2015 - DeepID3 Face Recognition with Very Deep Neural Ne.pdf;/home/david/Zotero/storage/E58NIL9Y/1502.html}
}

@misc{sunDeepLearningFace2014,
  title = {Deep {{Learning Face Representation}} by {{Joint Identification-Verification}}},
  author = {Sun, Yi and Wang, Xiaogang and Tang, Xiaoou},
  year = {2014},
  month = jun,
  number = {arXiv:1406.4773},
  eprint = {1406.4773},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-02-28},
  abstract = {The key challenge of face recognition is to develop effective feature representations for reducing intra-personal variations while enlarging inter-personal differences. In this paper, we show that it can be well solved with deep learning and using both face identification and verification signals as supervision. The Deep IDentification-verification features (DeepID2) are learned with carefully designed deep convolutional networks. The face identification task increases the inter-personal variations by drawing DeepID2 extracted from different identities apart, while the face verification task reduces the intra-personal variations by pulling DeepID2 extracted from the same identity together, both of which are essential to face recognition. The learned DeepID2 features can be well generalized to new identities unseen in the training data. On the challenging LFW dataset, 99.15\% face verification accuracy is achieved. Compared with the best deep learning result on LFW, the error rate has been significantly reduced by 67\%.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {2044 citations (Semantic Scholar/arXiv) [2023-02-28]},
  note = {DeepID2},
  file = {/home/david/Zotero/storage/LDNUHTFS/Sun et al. - 2014 - Deep Learning Face Representation by Joint Identif.pdf;/home/david/Zotero/storage/R3FF7Z4N/1406.html}
}

@inproceedings{sunDeepLearningFace2014a,
  title = {Deep {{Learning Face Representation}} from {{Predicting}} 10,000 {{Classes}}},
  booktitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Sun, Yi and Wang, Xiaogang and Tang, Xiaoou},
  year = {2014},
  month = jun,
  pages = {1891--1898},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2014.244},
  abstract = {This paper proposes to learn a set of high-level feature representations through deep learning, referred to as Deep hidden IDentity features (DeepID), for face verification. We argue that DeepID can be effectively learned through challenging multi-class face identification tasks, whilst they can be generalized to other tasks (such as verification) and new identities unseen in the training set. Moreover, the generalization capability of DeepID increases as more face classes are to be predicted at training. DeepID features are taken from the last hidden layer neuron activations of deep convolutional networks (ConvNets). When learned as classifiers to recognize about 10, 000 face identities in the training set and configured to keep reducing the neuron numbers along the feature extraction hierarchy, these deep ConvNets gradually form compact identity-related features in the top layers with only a small number of hidden neurons. The proposed features are extracted from various face regions to form complementary and over-complete representations. Any state-of-the-art classifiers can be learned based on these high-level representations for face verification. 97:45\% verification accuracy on LFW is achieved with only weakly aligned faces.},
  keywords = {Bayes methods,Biological neural networks,deep learning,Face,face verification,Feature extraction,Joints,Neurons,Training},
  annotation = {1827 citations (Semantic Scholar/DOI) [2023-02-28] 1110 citations (Crossref) [2023-02-28]},
  note = {DeepID},
  file = {/home/david/Zotero/storage/6LC2FE5N/Sun et al. - 2014 - Deep Learning Face Representation from Predicting .pdf;/home/david/Zotero/storage/GLQNSCT3/stamp.html}
}

@misc{sunDeeplyLearnedFace2014,
  title = {Deeply Learned Face Representations Are Sparse, Selective, and Robust},
  author = {Sun, Yi and Wang, Xiaogang and Tang, Xiaoou},
  year = {2014},
  month = dec,
  number = {arXiv:1412.1265},
  eprint = {1412.1265},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-02-28},
  abstract = {This paper designs a high-performance deep convolutional network (DeepID2+) for face recognition. It is learned with the identification-verification supervisory signal. By increasing the dimension of hidden representations and adding supervision to early convolutional layers, DeepID2+ achieves new state-of-the-art on LFW and YouTube Faces benchmarks. Through empirical studies, we have discovered three properties of its deep neural activations critical for the high performance: sparsity, selectiveness and robustness. (1) It is observed that neural activations are moderately sparse. Moderate sparsity maximizes the discriminative power of the deep net as well as the distance between images. It is surprising that DeepID2+ still can achieve high recognition accuracy even after the neural responses are binarized. (2) Its neurons in higher layers are highly selective to identities and identity-related attributes. We can identify different subsets of neurons which are either constantly excited or inhibited when different identities or attributes are present. Although DeepID2+ is not taught to distinguish attributes during training, it has implicitly learned such high-level concepts. (3) It is much more robust to occlusions, although occlusion patterns are not included in the training set.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {868 citations (Semantic Scholar/arXiv) [2023-02-28]},
  note = {DeepID2+},
  file = {/home/david/Zotero/storage/D8AJQV28/Sun et al. - 2014 - Deeply learned face representations are sparse, se.pdf;/home/david/Zotero/storage/9NUGCDF2/1412.html}
}

@inproceedings{sunSparsifyingNeuralNetwork2016,
  title = {Sparsifying {{Neural Network Connections}} for {{Face Recognition}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Sun, Yi and Wang, Xiaogang and Tang, Xiaoou},
  year = {2016},
  month = jun,
  pages = {4856--4864},
  publisher = {{IEEE}},
  address = {{Las Vegas, NV, USA}},
  doi = {10.1109/CVPR.2016.525},
  urldate = {2023-02-28},
  abstract = {This paper proposes to learn high-performance deep ConvNets with sparse neural connections, referred to as sparse ConvNets, for face recognition. The sparse ConvNets are learned in an iterative way, each time one additional layer is sparsified and the entire model is re-trained given the initial weights learned in previous iterations. One important finding is that directly training the sparse ConvNet from scratch failed to find good solutions for face recognition, while using a previously learned denser model to properly initialize a sparser model is critical to continue learning effective features for face recognition. This paper also proposes a new neural correlation-based weight selection criterion and empirically verifies its effectiveness in selecting informative connections from previously learned models in each iteration. When taking a moderately sparse structure (26\%-76\% of weights in the dense model), the proposed sparse ConvNet model significantly improves the face recognition performance of the previous state-of-theart DeepID2+ models given the same training data, while it keeps the performance of the baseline model with only 12\% of the original parameters.},
  isbn = {978-1-4673-8851-1},
  langid = {english},
  annotation = {135 citations (Semantic Scholar/DOI) [2023-02-28] 77 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/CSPVZZVK/Sun et al. - 2016 - Sparsifying Neural Network Connections for Face Re.pdf}
}

@book{szeliskiComputerVisionAlgorithms2022,
  title = {Computer {{Vision}}: {{Algorithms}} and {{Applications}}},
  shorttitle = {Computer {{Vision}}},
  author = {Szeliski, Richard},
  year = {2022},
  series = {Texts in {{Computer Science}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-34372-9},
  urldate = {2023-02-14},
  isbn = {978-3-030-34371-2 978-3-030-34372-9},
  langid = {english},
  keywords = {3D Reconstruction,Computational Photography,Computer Vision,Deep Learning,Feature Detection and Matching,Image Processing,Image Segmentation,Image Stitching,Image-Based Rendering,Motion Estimation,Scene Recognition,Structure from Motion},
  file = {/home/david/Zotero/storage/PXDWRBHL/Szeliski - 2022 - Computer Vision Algorithms and Applications.pdf}
}

@inproceedings{taigmanDeepFaceClosingGap2014,
  title = {{{DeepFace}}: {{Closing}} the {{Gap}} to {{Human-Level Performance}} in {{Face Verification}}},
  shorttitle = {{{DeepFace}}},
  booktitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Taigman, Yaniv and Yang, Ming and Ranzato, Marc'Aurelio and Wolf, Lior},
  year = {2014},
  month = jun,
  pages = {1701--1708},
  publisher = {{IEEE}},
  address = {{Columbus, OH, USA}},
  doi = {10.1109/CVPR.2014.220},
  urldate = {2023-02-13},
  abstract = {In modern face recognition, the conventional pipeline consists of four stages: detect {$\Rightarrow$} align {$\Rightarrow$} represent {$\Rightarrow$} classify. We revisit both the alignment step and the representation step by employing explicit 3D face modeling in order to apply a piecewise affine transformation, and derive a face representation from a nine-layer deep neural network. This deep network involves more than 120 million parameters using several locally connected layers without weight sharing, rather than the standard convolutional layers. Thus we trained it on the largest facial dataset to-date, an identity labeled dataset of four million facial images belonging to more than 4,000 identities. The learned representations coupling the accurate model-based alignment with the large facial database generalize remarkably well to faces in unconstrained environments, even with a simple classifier. Our method reaches an accuracy of 97.35\% on the Labeled Faces in the Wild (LFW) dataset, reducing the error of the current state of the art by more than 27\%, closely approaching human-level performance.},
  isbn = {978-1-4799-5118-5},
  langid = {english},
  file = {/home/david/Zotero/storage/VD86KSFJ/Taigman et al. - 2014 - DeepFace Closing the Gap to Human-Level Performan.pdf}
}

@inproceedings{taigmanDeepFaceClosingGap2014a,
  title = {{{DeepFace}}: {{Closing}} the {{Gap}} to {{Human-Level Performance}} in {{Face Verification}}},
  shorttitle = {{{DeepFace}}},
  booktitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Taigman, Yaniv and Yang, Ming and Ranzato, Marc'Aurelio and Wolf, Lior},
  year = {2014},
  month = jun,
  pages = {1701--1708},
  publisher = {{IEEE}},
  address = {{Columbus, OH, USA}},
  doi = {10.1109/CVPR.2014.220},
  urldate = {2023-02-27},
  abstract = {In modern face recognition, the conventional pipeline consists of four stages: detect {$\Rightarrow$} align {$\Rightarrow$} represent {$\Rightarrow$} classify. We revisit both the alignment step and the representation step by employing explicit 3D face modeling in order to apply a piecewise affine transformation, and derive a face representation from a nine-layer deep neural network. This deep network involves more than 120 million parameters using several locally connected layers without weight sharing, rather than the standard convolutional layers. Thus we trained it on the largest facial dataset to-date, an identity labeled dataset of four million facial images belonging to more than 4,000 identities. The learned representations coupling the accurate model-based alignment with the large facial database generalize remarkably well to faces in unconstrained environments, even with a simple classifier. Our method reaches an accuracy of 97.35\% on the Labeled Faces in the Wild (LFW) dataset, reducing the error of the current state of the art by more than 27\%, closely approaching human-level performance.},
  isbn = {978-1-4799-5118-5},
  langid = {english},
  annotation = {5524 citations (Semantic Scholar/DOI) [2023-02-28] 3028 citations (Crossref) [2023-02-28]},
  note = {DeepFace},
  file = {/home/david/Zotero/storage/3HX85V8C/Taigman et al. - 2014 - DeepFace Closing the Gap to Human-Level Performan.pdf}
}

@inproceedings{taigmanWebscaleTrainingFace2015,
  title = {Web-Scale Training for Face Identification},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Taigman, Yaniv and Yang, Ming and Ranzato, Marc'Aurelio and Wolf, Lior},
  year = {2015},
  month = jun,
  pages = {2746--2754},
  publisher = {{IEEE}},
  address = {{Boston, MA, USA}},
  doi = {10.1109/CVPR.2015.7298891},
  urldate = {2023-02-28},
  abstract = {Scaling machine learning methods to very large datasets has attracted considerable attention in recent years, thanks to easy access to ubiquitous sensing and data from the web. We study face recognition and show that three distinct properties have surprising effects on the transferability of deep convolutional networks (CNN): (1) The bottleneck of the network serves as an important transfer learning regularizer, and (2) in contrast to the common wisdom, performance saturation may exist in CNN's (as the number of training samples grows); we propose a solution for alleviating this by replacing the naive random subsampling of the training set with a bootstrapping process. Moreover, (3) we find a link between the representation norm and the ability to discriminate in a target domain, which sheds lights on how such networks represent faces. Based on these discoveries, we are able to improve face recognition accuracy on the widely used LFW benchmark, both in the verification (1:1) and identification (1:N) protocols, and directly compare, for the first time, with the state of the art Commercially-Off-The-Shelf system and show a sizable leap in performance.},
  isbn = {978-1-4673-6964-0},
  langid = {english},
  annotation = {249 citations (Semantic Scholar/DOI) [2023-02-28] 130 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/27UYXY5B/Taigman et al. - 2015 - Web-scale training for face identification.pdf}
}

@misc{tangPyramidBoxContextassistedSingle2018,
  title = {{{PyramidBox}}: {{A Context-assisted Single Shot Face Detector}}},
  shorttitle = {{{PyramidBox}}},
  author = {Tang, Xu and Du, Daniel K. and He, Zeqiang and Liu, Jingtuo},
  year = {2018},
  month = aug,
  number = {arXiv:1803.07737},
  eprint = {1803.07737},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-14},
  abstract = {Face detection has been well studied for many years and one of remaining challenges is to detect small, blurred and partially occluded faces in uncontrolled environment. This paper proposes a novel context-assisted single shot face detector, named \textbackslash emph\{PyramidBox\} to handle the hard face detection problem. Observing the importance of the context, we improve the utilization of contextual information in the following three aspects. First, we design a novel context anchor to supervise high-level contextual feature learning by a semi-supervised method, which we call it PyramidAnchors. Second, we propose the Low-level Feature Pyramid Network to combine adequate high-level context semantic feature and Low-level facial feature together, which also allows the PyramidBox to predict faces of all scales in a single shot. Third, we introduce a context-sensitive structure to increase the capacity of prediction network to improve the final accuracy of output. In addition, we use the method of Data-anchor-sampling to augment the training samples across different scales, which increases the diversity of training data for smaller faces. By exploiting the value of context, PyramidBox achieves superior performance among the state-of-the-art over the two common face detection benchmarks, FDDB and WIDER FACE. Our code is available in PaddlePaddle: \textbackslash href\{https://github.com/PaddlePaddle/models/tree/develop/fluid/face\_detection\}\{\textbackslash url\{https://github.com/PaddlePaddle/models/tree/develop/fluid/face\_detection\}\}.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: 21 pages, 12 figures},
  file = {/home/david/Zotero/storage/8TJGQUYM/Tang et al. - 2018 - PyramidBox A Context-assisted Single Shot Face De.pdf;/home/david/Zotero/storage/QAK5I7AX/1803.html}
}

@inproceedings{terhorstQMagFaceSimpleAccurate2023,
  title = {{{QMagFace}}: {{Simple}} and {{Accurate Quality-Aware Face Recognition}}},
  shorttitle = {{{QMagFace}}},
  booktitle = {2023 {{IEEE}}/{{CVF Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Terhorst, Philipp and Ihlefeld, Malte and Huber, Marco and Damer, Naser and Kirchbuchner, Florian and Raja, Kiran and Kuijper, Arjan},
  year = {2023},
  month = jan,
  pages = {3473--3483},
  publisher = {{IEEE}},
  address = {{Waikoloa, HI, USA}},
  doi = {10.1109/WACV56688.2023.00348},
  urldate = {2023-02-28},
  abstract = {In this work, we propose QMagFace, a simple and effective face recognition solution (QMagFace) that combines a quality-aware comparison score with a recognition model based on a magnitude-aware angular margin loss. The proposed approach includes model-specific face image qualities in the comparison process to enhance the recognition performance under unconstrained circumstances. Exploiting the linearity between the qualities and their comparison scores induced by the utilized loss, our quality-aware comparison function is simple and highly generalizable. The experiments conducted on several face recognition databases and benchmarks demonstrate that the introduced qualityawareness leads to consistent improvements in the recognition performance. Moreover, the proposed QMagFace approach performs especially well under challenging circumstances, such as cross-pose, cross-age, or cross-quality. Consequently, it leads to state-of-the-art performances on several face recognition benchmarks, such as 98.50\% on AgeDB, 83.95\% on XQLFQ, and 98.74\% on CFP-FP. The code for QMagFace is publicly available1.},
  isbn = {978-1-66549-346-8},
  langid = {english},
  annotation = {6 citations (Semantic Scholar/DOI) [2023-04-18] 0 citations (Crossref) [2023-03-06]},
  file = {/home/david/Zotero/storage/HHSUVU9E/Terhorst et al. - 2023 - QMagFace Simple and Accurate Quality-Aware Face R.pdf}
}

@misc{tompsonEfficientObjectLocalization2015,
  title = {Efficient {{Object Localization Using Convolutional Networks}}},
  author = {Tompson, Jonathan and Goroshin, Ross and Jain, Arjun and LeCun, Yann and Bregler, Christopher},
  year = {2015},
  month = jun,
  number = {arXiv:1411.4280},
  eprint = {1411.4280},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-02-13},
  abstract = {Recent state-of-the-art performance on human-body pose estimation has been achieved with Deep Convolutional Networks (ConvNets). Traditional ConvNet architectures include pooling and sub-sampling layers which reduce computational requirements, introduce invariance and prevent over-training. These benefits of pooling come at the cost of reduced localization accuracy. We introduce a novel architecture which includes an efficient `position refinement' model that is trained to estimate the joint offset location within a small region of the image. This refinement model is jointly trained in cascade with a state-of-the-art ConvNet model to achieve improved accuracy in human joint location estimation. We show that the variance of our detector approaches the variance of human annotations on the FLIC dataset and outperforms all existing approaches on the MPII-human-pose dataset.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: 8 pages with 1 page of citations},
  file = {/home/david/Zotero/storage/GFPQBARK/Tompson et al. - 2015 - Efficient Object Localization Using Convolutional .pdf;/home/david/Zotero/storage/MZWM9I3V/1411.html}
}

@inproceedings{tranDisentangledRepresentationLearning2017,
  title = {Disentangled {{Representation Learning GAN}} for {{Pose-Invariant Face Recognition}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Tran, Luan and Yin, Xi and Liu, Xiaoming},
  year = {2017},
  month = jul,
  pages = {1283--1292},
  publisher = {{IEEE}},
  address = {{Honolulu, HI}},
  doi = {10.1109/CVPR.2017.141},
  urldate = {2023-02-28},
  abstract = {The large pose discrepancy between two face images is one of the key challenges in face recognition. Conventional approaches for pose-invariant face recognition either perform face frontalization on, or learn a pose-invariant representation from, a non-frontal face image. We argue that it is more desirable to perform both tasks jointly to allow them to leverage each other. To this end, this paper proposes Disentangled Representation learning-Generative Adversarial Network (DR-GAN) with three distinct novelties. First, the encoder-decoder structure of the generator allows DR-GAN to learn a generative and discriminative representation, in addition to image synthesis. Second, this representation is explicitly disentangled from other face variations such as pose, through the pose code provided to the decoder and pose estimation in the discriminator. Third, DR-GAN can take one or multiple images as the input, and generate one unified representation along with an arbitrary number of synthetic images. Quantitative and qualitative evaluation on both controlled and in-the-wild databases demonstrate the superiority of DR-GAN over the state of the art.},
  isbn = {978-1-5386-0457-1},
  langid = {english},
  annotation = {788 citations (Semantic Scholar/DOI) [2023-02-28] 493 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/8PYPB7N8/Tran et al. - 2017 - Disentangled Representation Learning GAN for Pose-.pdf}
}

@article{turingCOMPUTINGMACHINERYINTELLIGENCE1950,
  title = {I.\textemdash{{COMPUTING MACHINERY AND INTELLIGENCE}}},
  author = {Turing, A. M.},
  year = {1950},
  month = oct,
  journal = {Mind},
  volume = {LIX},
  number = {236},
  pages = {433--460},
  issn = {1460-2113, 0026-4423},
  doi = {10.1093/mind/LIX.236.433},
  urldate = {2023-01-13},
  langid = {english},
  file = {/storage/Master Thesis/Articles/lix-236-433.pdf}
}

@article{turkEigenfacesRecognition1991,
  title = {Eigenfaces for {{Recognition}}},
  author = {Turk, Matthew and Pentland, Alex},
  year = {1991},
  month = jan,
  journal = {Journal of Cognitive Neuroscience},
  volume = {3},
  number = {1},
  pages = {71--86},
  issn = {0898-929X},
  doi = {10.1162/jocn.1991.3.1.71},
  urldate = {2023-03-07},
  abstract = {We have developed a near-real-time computer system that can locate and track a subject's head, and then recognize the person by comparing characteristics of the face to those of known individuals. The computational approach taken in this system is motivated by both physiology and information theory, as well as by the practical requirements of near-real-time performance and accuracy. Our approach treats the face recognition problem as an intrinsically two-dimensional (2-D) recognition problem rather than requiring recovery of three-dimensional geometry, taking advantage of the fact that faces are normally upright and thus may be described by a small set of 2-D characteristic views. The system functions by projecting face images onto a feature space that spans the significant variations among known face images. The significant features are known as "eigenfaces," because they are the eigenvectors (principal components) of the set of faces; they do not necessarily correspond to features such as eyes, ears, and noses. The projection operation characterizes an individual face by a weighted sum of the eigenface features, and so to recognize a particular face it is necessary only to compare these weights to those of known individuals. Some particular advantages of our approach are that it provides for the ability to learn and later recognize new faces in an unsupervised manner, and that it is easy to implement using a neural network architecture.}
}

@article{ullahNovelDeepMaskNetModel2022,
  title = {A Novel {{DeepMaskNet}} Model for Face Mask Detection and Masked Facial Recognition},
  author = {Ullah, Naeem and Javed, Ali and Ali Ghazanfar, Mustansar and Alsufyani, Abdulmajeed and Bourouis, Sami},
  year = {2022},
  month = nov,
  journal = {Journal of King Saud University - Computer and Information Sciences},
  volume = {34},
  number = {10, Part B},
  pages = {9905--9914},
  issn = {1319-1578},
  doi = {10.1016/j.jksuci.2021.12.017},
  urldate = {2023-02-27},
  abstract = {Coronavirus disease (COVID-19) has significantly affected the daily life activities of people globally. To prevent the spread of COVID-19, the World Health Organization has recommended the people to wear face mask in public places. Manual inspection of people for wearing face masks in public places is a challenging task. Moreover, the use of face masks makes the traditional face recognition techniques ineffective, which are typically designed for unveiled faces. Thus, introduces an urgent need to develop a robust system capable of detecting the people not wearing the face masks and recognizing different persons while wearing the face mask. In this paper, we propose a novel DeepMasknet framework capable of both the face mask detection and masked facial recognition. Moreover, presently there is an absence of a unified and diverse dataset that can be used to evaluate both the face mask detection and masked facial recognition. For this purpose, we also developed a largescale and diverse unified mask detection and masked facial recognition (MDMFR) dataset to measure the performance of both the face mask detection and masked facial recognition methods. Experimental results on multiple datasets including the cross-dataset setting show the superiority of our DeepMasknet framework over the contemporary models.},
  langid = {english},
  keywords = {COVID-19,Deep learning,DeepMaskNet,Face mask detection,Masked facial recognition,MDMFR dataset},
  annotation = {21 citations (Semantic Scholar/DOI) [2023-04-07] 14 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/DAA98AYT/Ullah et al. - 2022 - A novel DeepMaskNet model for face mask detection .pdf;/home/david/Zotero/storage/PSILTEQH/S1319157821003633.html}
}

@inproceedings{uppalTeacherStudentAdversarialDepth2021,
  title = {Teacher-{{Student Adversarial Depth Hallucination}} to {{Improve Face Recognition}}},
  booktitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Uppal, Hardik and {Sepas-Moghaddam}, Alireza and Greenspan, Michael and Etemad, Ali},
  year = {2021},
  month = oct,
  pages = {3651--3660},
  publisher = {{IEEE}},
  address = {{Montreal, QC, Canada}},
  doi = {10.1109/ICCV48922.2021.00365},
  urldate = {2023-02-28},
  abstract = {We present the Teacher-Student Generative Adversarial Network (TS-GAN) to generate depth images from single RGB images in order to boost the performance of face recognition systems. For our method to generalize well across unseen datasets, we design two components in the architecture, a teacher and a student. The teacher, which itself consists of a generator and a discriminator, learns a latent mapping between input RGB and paired depth images in a supervised fashion. The student, which consists of two generators (one shared with the teacher) and a discriminator, learns from new RGB data with no available paired depth information, for improved generalization. The fully trained shared generator can then be used in runtime to hallucinate depth from RGB for downstream applications such as face recognition. We perform rigorous experiments to show the superiority of TS-GAN over other methods in generating synthetic depth images. Moreover, face recognition experiments demonstrate that our hallucinated depth along with the input RGB images boost performance across various architectures when compared to a single RGB modality by average values of +1.2\%, +2.6\%, and +2.6\% for IIITD, EURECOM, and LFW datasets respectively. We make our implementation public at: https://github.com/hardikuppal/teacher-student-gan.git.},
  isbn = {978-1-66542-812-5},
  langid = {english},
  annotation = {3 citations (Semantic Scholar/DOI) [2023-02-28] 2 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/MTHMD3HR/Uppal et al. - 2021 - Teacher-Student Adversarial Depth Hallucination to.pdf}
}

@inproceedings{violaRapidObjectDetection2001,
  title = {Rapid Object Detection Using a Boosted Cascade of Simple Features},
  booktitle = {Proceedings of the 2001 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}}. {{CVPR}} 2001},
  author = {Viola, P. and Jones, M.},
  year = {2001},
  month = dec,
  volume = {1},
  pages = {I-I},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2001.990517},
  abstract = {This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the "integral image" which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers. The third contribution is a method for combining increasingly more complex classifiers in a "cascade" which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.},
  keywords = {Detectors,Face detection,Filters,Focusing,Image representation,Machine learning,Object detection,Pixel,Robustness,Skin},
  file = {/home/david/Zotero/storage/5Q2JLY3C/Viola and Jones - 2001 - Rapid object detection using a boosted cascade of .pdf;/home/david/Zotero/storage/XVP24CEZ/stamp.html}
}

@article{wangAdditiveMarginSoftmax2018,
  title = {Additive {{Margin Softmax}} for {{Face Verification}}},
  author = {Wang, Feng and Liu, Weiyang and Liu, Haijun and Cheng, Jian},
  year = {2018},
  month = jul,
  journal = {IEEE Signal Processing Letters},
  volume = {25},
  number = {7},
  eprint = {1801.05599},
  primaryclass = {cs},
  pages = {926--930},
  issn = {1070-9908, 1558-2361},
  doi = {10.1109/LSP.2018.2822810},
  urldate = {2023-02-28},
  abstract = {In this paper, we propose a conceptually simple and geometrically interpretable objective function, i.e. additive margin Softmax (AM-Softmax), for deep face verification. In general, the face verification task can be viewed as a metric learning problem, so learning large-margin face features whose intra-class variation is small and inter-class difference is large is of great importance in order to achieve good performance. Recently, Large-margin Softmax and Angular Softmax have been proposed to incorporate the angular margin in a multiplicative manner. In this work, we introduce a novel additive angular margin for the Softmax loss, which is intuitively appealing and more interpretable than the existing works. We also emphasize and discuss the importance of feature normalization in the paper. Most importantly, our experiments on LFW BLUFR and MegaFace show that our additive margin softmax loss consistently performs better than the current state-of-the-art methods using the same network architecture and training dataset. Our code has also been made available at https://github.com/happynear/AMSoftmax},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {921 citations (Semantic Scholar/arXiv) [2023-02-28] 921 citations (Semantic Scholar/DOI) [2023-02-28] 615 citations (Crossref) [2023-02-28]},
  note = {AMS Loss},
  file = {/home/david/Zotero/storage/UN54GSL4/Wang et al. - 2018 - Additive Margin Softmax for Face Verification.pdf;/home/david/Zotero/storage/QCWA4G26/1801.html}
}

@inproceedings{wangCoMiningDeepFace2019,
  title = {Co-{{Mining}}: {{Deep Face Recognition With Noisy Labels}}},
  shorttitle = {Co-{{Mining}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Wang, Xiaobo and Wang, Shuo and Shi, Hailin and Wang, Jun and Mei, Tao},
  year = {2019},
  month = oct,
  pages = {9357--9366},
  publisher = {{IEEE}},
  address = {{Seoul, Korea (South)}},
  doi = {10.1109/ICCV.2019.00945},
  urldate = {2023-02-28},
  abstract = {Face recognition has achieved significant progress with the growing scale of collected datasets, which empowers us to train strong convolutional neural networks (CNNs). While a variety of CNN architectures and loss functions have been devised recently, we still have a limited understanding of how to train the CNN models with the label noise inherent in existing face recognition datasets. To address this issue, this paper develops a novel co-mining strategy to effectively train on the datasets with noisy labels. Specifically, we simultaneously use the loss values as the cue to detect noisy labels, exchange the highconfidence clean faces to alleviate the errors accumulated issue caused by the sample-selection bias, and re-weight the predicted clean faces to make them dominate the discriminative model training in a mini-batch fashion. Extensive experiments by training on three popular datasets (i.e., CASIA-WebFace, MS-Celeb-1M and VggFace2) and testing on several benchmarks, including LFW, CALFW, CPLFW, AgeDB, CFP, RFW, and MegaFace, have demonstrated the effectiveness of our new approach over the stateof-the-art alternatives. Our code is available at http: //www.cbsr.ia.ac.cn/users/xiaobowang/.},
  isbn = {978-1-72814-803-8},
  langid = {english},
  annotation = {81 citations (Semantic Scholar/DOI) [2023-02-28] 53 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/2WKM3KMV/Wang et al. - 2019 - Co-Mining Deep Face Recognition With Noisy Labels.pdf}
}

@misc{wangCosFaceLargeMargin2018,
  title = {{{CosFace}}: {{Large Margin Cosine Loss}} for {{Deep Face Recognition}}},
  shorttitle = {{{CosFace}}},
  author = {Wang, Hao and Wang, Yitong and Zhou, Zheng and Ji, Xing and Gong, Dihong and Zhou, Jingchao and Li, Zhifeng and Liu, Wei},
  year = {2018},
  month = apr,
  number = {arXiv:1801.09414},
  eprint = {1801.09414},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-02-28},
  abstract = {Face recognition has made extraordinary progress owing to the advancement of deep convolutional neural networks (CNNs). The central task of face recognition, including face verification and identification, involves face feature discrimination. However, the traditional softmax loss of deep CNNs usually lacks the power of discrimination. To address this problem, recently several loss functions such as center loss, large margin softmax loss, and angular softmax loss have been proposed. All these improved losses share the same idea: maximizing inter-class variance and minimizing intra-class variance. In this paper, we propose a novel loss function, namely large margin cosine loss (LMCL), to realize this idea from a different perspective. More specifically, we reformulate the softmax loss as a cosine loss by \$L\_2\$ normalizing both features and weight vectors to remove radial variations, based on which a cosine margin term is introduced to further maximize the decision margin in the angular space. As a result, minimum intra-class variance and maximum inter-class variance are achieved by virtue of normalization and cosine decision margin maximization. We refer to our model trained with LMCL as CosFace. Extensive experimental evaluations are conducted on the most popular public-domain face recognition datasets such as MegaFace Challenge, Youtube Faces (YTF) and Labeled Face in the Wild (LFW). We achieve the state-of-the-art performance on these benchmarks, which confirms the effectiveness of our proposed approach.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {1587 citations (Semantic Scholar/arXiv) [2023-02-28]},
  note = {Comment: Accepted by CVPR 2018},
  file = {/home/david/Zotero/storage/L2JC95C2/Wang et al. - 2018 - CosFace Large Margin Cosine Loss for Deep Face Re.pdf;/home/david/Zotero/storage/FXZSVZIC/1801.html}
}

@article{wangDeepFaceRecognition2021,
  title = {Deep Face Recognition: {{A}} Survey},
  shorttitle = {Deep Face Recognition},
  author = {Wang, Mei and Deng, Weihong},
  year = {2021},
  month = mar,
  journal = {Neurocomputing},
  volume = {429},
  pages = {215--244},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2020.10.081},
  urldate = {2023-02-27},
  abstract = {Deep learning applies multiple processing layers to learn representations of data with multiple levels of feature extraction. This emerging technique has reshaped the research landscape of face recognition (FR) since 2014, launched by the breakthroughs of DeepFace and DeepID. Since then, deep learning technique, characterized by the hierarchical architecture to stitch together pixels into invariant face representation, has dramatically improved the state-of-the-art performance and fostered successful real-world applications. In this survey, we provide a comprehensive review of the recent developments on deep FR, covering broad topics on algorithm designs, databases, protocols, and application scenes. First, we summarize different network architectures and loss functions proposed in the rapid evolution of the deep FR methods. Second, the related face processing methods are categorized into two classes: ``one-to-many augmentation'' and ``many-to-one normalization''. Then, we summarize and compare the commonly used databases for both model training and evaluation. Third, we review miscellaneous scenes in deep FR, such as cross-factor, heterogenous, multiple-media and industrial scenes. Finally, the technical challenges and several promising directions are highlighted.},
  langid = {english},
  keywords = {Deep face recognition,Deep learning,Deep network architecture,Face processing,Face recognition database,Loss function},
  annotation = {714 citations (Semantic Scholar/DOI) [2023-02-28] 187 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/SU8BH3WW/Wang and Deng - 2021 - Deep face recognition A survey.pdf;/home/david/Zotero/storage/99GWEEZH/S0925231220316945.html}
}

@misc{wangDevilFaceRecognition2018,
  title = {The {{Devil}} of {{Face Recognition}} Is in the {{Noise}}},
  author = {Wang, Fei and Chen, Liren and Li, Cheng and Huang, Shiyao and Chen, Yanjie and Qian, Chen and Loy, Chen Change},
  year = {2018},
  month = jul,
  number = {arXiv:1807.11649},
  eprint = {1807.11649},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-28},
  abstract = {The growing scale of face recognition datasets empowers us to train strong convolutional networks for face recognition. While a variety of architectures and loss functions have been devised, we still have a limited understanding of the source and consequence of label noise inherent in existing datasets. We make the following contributions: 1) We contribute cleaned subsets of popular face databases, i.e., MegaFace and MS-Celeb-1M datasets, and build a new large-scale noise-controlled IMDb-Face dataset. 2) With the original datasets and cleaned subsets, we profile and analyze label noise properties of MegaFace and MS-Celeb-1M. We show that a few orders more samples are needed to achieve the same accuracy yielded by a clean subset. 3) We study the association between different types of noise, i.e., label flips and outliers, with the accuracy of face recognition models. 4) We investigate ways to improve data cleanliness, including a comprehensive user study on the influence of data labeling strategies to annotation accuracy. The IMDb-Face dataset has been released on https://github.com/fwang91/IMDb-Face.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: accepted to ECCV'18},
  file = {/home/david/Zotero/storage/KUKBHIBM/Wang et al. - 2018 - The Devil of Face Recognition is in the Noise.pdf;/home/david/Zotero/storage/QKH4PUFE/1807.html}
}

@inproceedings{wangEfficientTrainingApproach2022,
  title = {An {{Efficient Training Approach}} for {{Very Large Scale Face Recognition}}},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Wang, Kai and Wang, Shuo and Zhang, Panpan and Zhou, Zhipeng and Zhu, Zheng and Wang, Xiaobo and Peng, Xiaojiang and Sun, Baigui and Li, Hao and You, Yang},
  year = {2022},
  month = jun,
  pages = {4073--4082},
  publisher = {{IEEE}},
  address = {{New Orleans, LA, USA}},
  doi = {10.1109/CVPR52688.2022.00405},
  urldate = {2023-02-28},
  abstract = {Face recognition has achieved significant progress in deep learning era due to the ultra-large-scale and welllabeled datasets. However, training on the outsize datasets is time-consuming and takes up a lot of hardware resource. Therefore, designing an efficient training approach is indispensable. The heavy computational and memory costs mainly result from the million-level dimensionality of the fully connected (FC) layer. To this end, we propose a novel training approach, termed Faster Face Classification (F2C), to alleviate time and cost without sacrificing the performance. This method adopts Dynamic Class Pool (DCP) for storing and updating the identities' features dynamically, which could be regarded as a substitute for the FC layer. DCP is efficiently time-saving and cost-saving, as its smaller size with the independence from the whole face identities together. We further validate the proposed F2C method across several face benchmarks and private datasets, and display comparable results, meanwhile the speed is faster than state-of-the-art FC-based methods in terms of recognition accuracy and hardware costs. Moreover, our method is further improved by a well-designed dual data loader including indentity-based and instancebased loaders, which makes it more efficient for updating DCP parameters.},
  isbn = {978-1-66546-946-3},
  langid = {english},
  annotation = {6 citations (Semantic Scholar/DOI) [2023-04-18] 3 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/FT4KJ2EU/Wang et al. - 2022 - An Efficient Training Approach for Very Large Scal.pdf}
}

@article{wangFacialFeaturePoint2018,
  title = {Facial Feature Point Detection: {{A}} Comprehensive Survey},
  author = {Wang, Nannan and Gao, Xinbo and Tao, Dacheng and Yang, Heng and Li, Xuelong},
  year = {2018},
  month = jan,
  journal = {Neurocomputing},
  volume = {275},
  pages = {50--65},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2017.05.013},
  abstract = {This paper presents a comprehensive survey of facial feature point detection with the assistance of abundant manually labeled images. Facial feature point detection favors many applications such as face recognition, animation, tracking, hallucination, expression analysis and 3D face modeling. Existing methods are categorized into two primary categories according to whether there is the need of a parametric shape model: parametric shape model-based methods and nonparametric shape model-based methods. Parametric shape model-based methods are further divided into two secondary classes according to their appearance models: local part model-based methods (e.g. constrained local model) and holistic model-based methods (e.g. active appearance model). Nonparametric shape model-based methods are divided into several groups according to their model construction process: exemplar-based methods, graphical model-based methods, cascaded regression-based methods, and deep learning based methods. Though significant progress has been made, facial feature point detection is still limited in its success by wild and real-world conditions: large variations across poses, expressions, illuminations, and occlusions. A comparative illustration and analysis of representative methods provides us a holistic understanding and deep insight into facial feature point detection, which also motivates us to further explore more promising future schemes.},
  keywords = {Deep learning,Face alignment,Facial feature point detection,Facial landmark localization}
}

@inproceedings{wangFM2uNetFaceMorphological2020,
  title = {{{FM2u-Net}}: {{Face Morphological Multi-Branch Network}} for {{Makeup-Invariant Face Verification}}},
  shorttitle = {{{FM2u-Net}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Wang, Wenxuan and Fu, Yanwei and Qian, Xuelin and Jiang, Yu-Gang and Tian, Qi and Xue, Xiangyang},
  year = {2020},
  month = jun,
  pages = {5729--5739},
  publisher = {{IEEE}},
  address = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.00577},
  urldate = {2023-02-28},
  abstract = {It is challenging in learning a makeup-invariant face verification model, due to (1) insufficient makeup/non-makeup face training pairs, (2) the lack of diverse makeup faces, and (3) the significant appearance changes caused by cosmetics. To address these challenges, we propose a unified Face Morphological Multi-branch Network (FM 2 u-Net) for makeup-invariant face verification, which can simultaneously synthesize many diverse makeup faces through face morphology network (FM-Net) and effectively learn cosmetics-robust face representations using attention-based multi-branch learning network (AttM-Net). For challenges (1) and (2), FM-Net (two stacked auto-encoders) can synthesize realistic makeup face images by transferring specific regions of cosmetics via cycle consistent loss. For challenge (3), AttM-Net, consisting of one global and three local (task-driven on two eyes and mouth) branches, can effectively capture the complementary holistic and detailed information. Unlike DeepID2 which uses simple concatenation fusion, we introduce a heuristic method AttM-FM, attached to AttM-Net, to adaptively weight the features of different branches guided by the holistic information. We conduct extensive experiments on makeup face verification benchmarks (M-501, M-203, and FAM) and general face recognition datasets (LFW and IJB-A). Our framework FM 2 u-Net achieves state-of-the-art performances.},
  isbn = {978-1-72817-168-5},
  langid = {english},
  annotation = {10 citations (Semantic Scholar/DOI) [2023-02-28] 4 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/YYLD7LS8/Wang et al. - 2020 - FM2u-Net Face Morphological Multi-Branch Network .pdf}
}

@inproceedings{wangGenCNNConvolutionalArchitecture2015,
  title = {{{genCNN}}: {{A Convolutional Architecture}} for {{Word Sequence Prediction}}},
  shorttitle = {{{genCNN}}},
  booktitle = {Proceedings of the 53rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 7th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Wang, Mingxuan and Lu, Zhengdong and Li, Hang and Jiang, Wenbin and Liu, Qun},
  year = {2015},
  month = jul,
  pages = {1567--1576},
  publisher = {{Association for Computational Linguistics}},
  address = {{Beijing, China}},
  doi = {10.3115/v1/P15-1151},
  urldate = {2023-02-13},
  file = {/home/david/Zotero/storage/XWPUUAPS/Wang et al. - 2015 - genCNN A Convolutional Architecture for Word Sequ.pdf}
}

@inproceedings{wangHierarchicalPyramidDiverse2020,
  title = {Hierarchical {{Pyramid Diverse Attention Networks}} for {{Face Recognition}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Wang, Qiangchang and Wu, Tianyi and Zheng, He and Guo, Guodong},
  year = {2020},
  month = jun,
  pages = {8323--8332},
  publisher = {{IEEE}},
  address = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.00835},
  urldate = {2023-02-28},
  abstract = {Deep learning has achieved a great success in face recognition (FR), however, few existing models take hierarchical multi-scale local features into consideration. In this work, we propose a hierarchical pyramid diverse attention (HPDA) network. First, it is observed that local patches would play important roles in FR when the global face appearance changes dramatically. Some recent works apply attention modules to locate local patches automatically without relying on face landmarks. Unfortunately, without considering diversity, some learned attentions tend to have redundant responses around some similar local patches, while neglecting other potential discriminative facial parts. Meanwhile, local patches may appear at different scales due to pose variations or large expression changes. To alleviate these challenges, we propose a pyramid diverse attention (PDA) to learn multi-scale diverse local representations automatically and adaptively. More specifically, a pyramid attention is developed to capture multi-scale features. Meanwhile, a diverse learning is developed to encourage models to focus on different local patches and generate diverse local features. Second, almost all existing models focus on extracting features from the last convolutional layer, lacking of local details or small-scale face parts in lower layers. Instead of simple concatenation or addition, we propose to use a hierarchical bilinear pooling (HBP) to fuse information from multiple layers effectively. Thus, the HPDA is developed by integrating the PDA into the HBP. Experimental results on several datasets show the effectiveness of the HPDA, compared to the state-of-the-art methods.},
  isbn = {978-1-72817-168-5},
  langid = {english},
  annotation = {40 citations (Semantic Scholar/DOI) [2023-02-28] 33 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/CA7PEVYI/Wang et al. - 2020 - Hierarchical Pyramid Diverse Attention Networks fo.pdf}
}

@inproceedings{wangMitigatingBiasFace2020,
  title = {Mitigating {{Bias}} in {{Face Recognition Using Skewness-Aware Reinforcement Learning}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Wang, Mei and Deng, Weihong},
  year = {2020},
  month = jun,
  pages = {9319--9328},
  publisher = {{IEEE}},
  address = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.00934},
  urldate = {2023-02-28},
  abstract = {Racial equality is an important theme of international human rights law, but it has been largely obscured when the overall face recognition accuracy is pursued blindly. More facts indicate racial bias indeed degrades the fairness of recognition system and the error rates on non-Caucasians are usually much higher than Caucasians. To encourage fairness, we introduce the idea of adaptive margin to learn balanced performance for different races based on large margin losses. A reinforcement learning based race balance network (RL-RBN) is proposed. We formulate the process of finding the optimal margins for non-Caucasians as a Markov decision process and employ deep Q-learning to learn policies for an agent to select appropriate margin by approximating the Q-value function. Guided by the agent, the skewness of feature scatter between races can be reduced. Besides, we provide two ethnicity aware training datasets, called BUPT-Globalface and BUPT-Balancedface dataset, which can be utilized to study racial bias from both data and algorithm aspects. Extensive experiments on RFW database show that RL-RBN successfully mitigates racial bias and learns more balanced performance.},
  isbn = {978-1-72817-168-5},
  langid = {english},
  annotation = {119 citations (Semantic Scholar/DOI) [2023-02-28] 62 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/M9CGYBAD/Wang and Deng - 2020 - Mitigating Bias in Face Recognition Using Skewness.pdf}
}

@inproceedings{wangNormFaceL2Hypersphere2017,
  title = {{{NormFace}}: {{L2 Hypersphere Embedding}} for {{Face Verification}}},
  shorttitle = {{{NormFace}}},
  booktitle = {Proceedings of the 25th {{ACM}} International Conference on {{Multimedia}}},
  author = {Wang, Feng and Xiang, Xiang and Cheng, Jian and Yuille, Alan L.},
  year = {2017},
  month = oct,
  eprint = {1704.06369},
  primaryclass = {cs},
  pages = {1041--1049},
  doi = {10.1145/3123266.3123359},
  urldate = {2023-02-27},
  abstract = {Thanks to the recent developments of Convolutional Neural Networks, the performance of face verification methods has increased rapidly. In a typical face verification method, feature normalization is a critical step for boosting performance. This motivates us to introduce and study the effect of normalization during training. But we find this is non-trivial, despite normalization being differentiable. We identify and study four issues related to normalization through mathematical analysis, which yields understanding and helps with parameter settings. Based on this analysis we propose two strategies for training using normalized features. The first is a modification of softmax loss, which optimizes cosine similarity instead of inner-product. The second is a reformulation of metric learning by introducing an agent vector for each class. We show that both strategies, and small variants, consistently improve performance by between 0.2\% to 0.4\% on the LFW dataset based on two models. This is significant because the performance of the two models on LFW dataset is close to saturation at over 98\%. Codes and models are released on https://github.com/happynear/NormFace},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {577 citations (Semantic Scholar/arXiv) [2023-02-28] 577 citations (Semantic Scholar/DOI) [2023-02-28] 275 citations (Crossref) [2023-02-28]},
  note = {Comment: camera-ready version},
  file = {/home/david/Zotero/storage/DR52T2FL/Wang et al. - 2017 - NormFace L2 Hypersphere Embedding for Face Verifi.pdf;/home/david/Zotero/storage/YTAB5765/1704.html}
}

@inproceedings{wangPatchNetSimpleFace2022,
  title = {{{PatchNet}}: {{A Simple Face Anti-Spoofing Framework}} via {{Fine-Grained Patch Recognition}}},
  shorttitle = {{{PatchNet}}},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Wang, Chien-Yi and Lu, Yu-Ding and Yang, Shang-Ta and Lai, Shang-Hong},
  year = {2022},
  month = jun,
  pages = {20249--20258},
  publisher = {{IEEE}},
  address = {{New Orleans, LA, USA}},
  doi = {10.1109/CVPR52688.2022.01964},
  urldate = {2023-02-28},
  abstract = {Face anti-spoofing (FAS) plays a critical role in securing face recognition systems from different presentation attacks. Previous works leverage auxiliary pixel-level supervision and domain generalization approaches to address unseen spoof types. However, the local characteristics of image captures, i.e., capturing devices and presenting materials, are ignored in existing works and we argue that such information is required for networks to discriminate between live and spoof images. In this work, we propose PatchNet which reformulates face anti-spoofing as a fine-grained patch-type recognition problem. To be specific, our framework recognizes the combination of capturing devices and presenting materials based on the patches cropped from non-distorted face images. This reformulation can largely improve the data variation and enforce the network to learn discriminative feature from local capture patterns. In addition, to further improve the generalization ability of the spoof feature, we propose the novel Asymmetric Margin-based Classification Loss and Self-supervised Similarity Loss to regularize the patch embedding space. Our experimental results verify our assumption and show that the model is capable of recognizing unseen spoof types robustly by only looking at local regions. Moreover, the fine-grained and patch-level reformulation of FAS outperforms the existing approaches on intra-dataset, cross-dataset, and domain generalization benchmarks. Furthermore, our PatchNet framework can enable practical applications like FewShot Reference-based FAS and facilitate future exploration of spoof-related intrinsic cues.},
  isbn = {978-1-66546-946-3},
  langid = {english},
  annotation = {13 citations (Semantic Scholar/DOI) [2023-04-07] 4 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/VZBW9WPJ/Wang et al. - 2022 - PatchNet A Simple Face Anti-Spoofing Framework vi.pdf}
}

@misc{wangSupportVectorGuided2018,
  title = {Support {{Vector Guided Softmax Loss}} for {{Face Recognition}}},
  author = {Wang, Xiaobo and Wang, Shuo and Zhang, Shifeng and Fu, Tianyu and Shi, Hailin and Mei, Tao},
  year = {2018},
  month = dec,
  number = {arXiv:1812.11317},
  eprint = {1812.11317},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-02-28},
  abstract = {Face recognition has witnessed significant progresses due to the advances of deep convolutional neural networks (CNNs), the central challenge of which, is feature discrimination. To address it, one group tries to exploit mining-based strategies (\textbackslash textit\{e.g.\}, hard example mining and focal loss) to focus on the informative examples. The other group devotes to designing margin-based loss functions (\textbackslash textit\{e.g.\}, angular, additive and additive angular margins) to increase the feature margin from the perspective of ground truth class. Both of them have been well-verified to learn discriminative features. However, they suffer from either the ambiguity of hard examples or the lack of discriminative power of other classes. In this paper, we design a novel loss function, namely support vector guided softmax loss (SV-Softmax), which adaptively emphasizes the mis-classified points (support vectors) to guide the discriminative features learning. So the developed SV-Softmax loss is able to eliminate the ambiguity of hard examples as well as absorb the discriminative power of other classes, and thus results in more discrimiantive features. To the best of our knowledge, this is the first attempt to inherit the advantages of mining-based and margin-based losses into one framework. Experimental results on several benchmarks have demonstrated the effectiveness of our approach over state-of-the-arts.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {34 citations (Semantic Scholar/arXiv) [2023-02-28]},
  note = {Loss functions},
  file = {/home/david/Zotero/storage/6TPIRSBQ/Wang et al. - 2018 - Support Vector Guided Softmax Loss for Face Recogn.pdf;/home/david/Zotero/storage/FR3CLQ3R/1812.html}
}

@article{weizenbaumELIZAComputerProgram1966,
  title = {{{ELIZA}}\textemdash a Computer Program for the Study of Natural Language Communication between Man and Machine},
  author = {Weizenbaum, Joseph},
  year = {1966},
  month = jan,
  journal = {Communications of the ACM},
  volume = {9},
  number = {1},
  pages = {36--45},
  issn = {0001-0782},
  doi = {10.1145/365153.365168},
  urldate = {2023-01-18},
  file = {/home/david/Zotero/storage/3SAM3JPS/Weizenbaum - 1966 - ELIZA—a computer program for the study of natural .pdf}
}

@incollection{wenDiscriminativeFeatureLearning2016,
  title = {A {{Discriminative Feature Learning Approach}} for {{Deep Face Recognition}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2016},
  author = {Wen, Yandong and Zhang, Kaipeng and Li, Zhifeng and Qiao, Yu},
  editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
  year = {2016},
  volume = {9911},
  pages = {499--515},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  urldate = {2023-02-28},
  abstract = {Convolutional neural networks (CNNs) have been widely used in computer vision community, significantly improving the stateof-the-art. In most of the available CNNs, the softmax loss function is used as the supervision signal to train the deep model. In order to enhance the discriminative power of the deeply learned features, this paper proposes a new supervision signal, called center loss, for face recognition task. Specifically, the center loss simultaneously learns a center for deep features of each class and penalizes the distances between the deep features and their corresponding class centers. More importantly, we prove that the proposed center loss function is trainable and easy to optimize in the CNNs. With the joint supervision of softmax loss and center loss, we can train a robust CNNs to obtain the deep features with the two key learning objectives, inter-class dispension and intra-class compactness as much as possible, which are very essential to face recognition. It is encouraging to see that our CNNs (with such joint supervision) achieve the state-of-the-art accuracy on several important face recognition benchmarks, Labeled Faces in the Wild (LFW), YouTube Faces (YTF), and MegaFace Challenge. Especially, our new approach achieves the best results on MegaFace (the largest public domain face benchmark) under the protocol of small training set (contains under 500000 images and under 20000 persons), significantly improving the previous results and setting new state-of-the-art for both face recognition and face verification tasks.},
  isbn = {978-3-319-46477-0 978-3-319-46478-7},
  langid = {english},
  annotation = {959 citations},
  note = {Center Loss},
  file = {/home/david/Zotero/storage/F22GLH39/Wen et al. - 2016 - A Discriminative Feature Learning Approach for Dee.pdf}
}

@inproceedings{wengCresceptronSelforganizingNeural1992,
  title = {Cresceptron: A Self-Organizing Neural Network Which Grows Adaptively},
  shorttitle = {Cresceptron},
  booktitle = {[{{Proceedings}} 1992] {{IJCNN International Joint Conference}} on {{Neural Networks}}},
  author = {Weng, J. and Ahuja, N. and Huang, T.S.},
  year = {1992},
  month = jun,
  volume = {1},
  pages = {576-581 vol.1},
  doi = {10.1109/IJCNN.1992.287150},
  abstract = {Cresceptron uses a hierarchical framework to grow neural networks automatically, adaptively, and incrementally through learning. At every level of the hierarchy, new concepts are detected automatically and the network grows by creating new neurons and synapses which memorize the new concepts and their context. The training samples are generalized to other perceptually equivalent items through hierarchical tolerance of deviation. The neural network recognizes the learned items and their variations by hierarchically associating the learned knowledge with the input. It segments the recognized items from the input through back training along the response paths.{$<>$}},
  keywords = {Backpropagation,Electric breakdown,Humans,Input variables,Learning systems,Neural networks,Neurons,Unsupervised learning},
  file = {/home/david/Zotero/storage/PXM2V4HZ/Weng et al. - 1992 - Cresceptron a self-organizing neural network whic.pdf;/home/david/Zotero/storage/9EQZD6A9/stamp.html}
}

@inproceedings{winsteadRemoteMicroelectronicsLaboratory2022,
  title = {Remote {{Microelectronics Laboratory Education}} in the {{COVID-19 Pandemic}}},
  booktitle = {2022 {{Intermountain Engineering}}, {{Technology}} and {{Computing}} ({{IETC}})},
  author = {Winstead, Chris J},
  year = {2022},
  month = may,
  pages = {1--6},
  doi = {10.1109/IETC54973.2022.9796805},
  abstract = {In response to emergency conditions brought on by the COVID-19 pandemic, universities were obliged to rapidly transition their mode of education to a remote format. This paper describes a rapidly deployed platform for remote laboratory education in a junior-level undergraduate microelectronics course. The paper describes physical circuit design, server organization, remote access procedures and educational outcomes.},
  keywords = {Circuit synthesis,COVID-19,Education,laboratory education,microelectronics,Microelectronics,Organizations,Pandemics,Remote laboratories,remote learning},
  annotation = {0 citations (Semantic Scholar/DOI) [2023-04-07]},
  file = {/home/david/Zotero/storage/CHG7QI9P/Winstead - 2022 - Remote Microelectronics Laboratory Education in th.pdf;/home/david/Zotero/storage/LCPCL55P/stamp.html}
}

@misc{wuLightCNNDeep2018,
  title = {A {{Light CNN}} for {{Deep Face Representation}} with {{Noisy Labels}}},
  author = {Wu, Xiang and He, Ran and Sun, Zhenan and Tan, Tieniu},
  year = {2018},
  month = aug,
  number = {arXiv:1511.02683},
  eprint = {1511.02683},
  publisher = {{arXiv}},
  urldate = {2023-02-28},
  abstract = {The volume of convolutional neural network (CNN) models proposed for face recognition has been continuously growing larger to better fit large amount of training data. When training data are obtained from internet, the labels are likely to be ambiguous and inaccurate. This paper presents a Light CNN framework to learn a compact embedding on the large-scale face data with massive noisy labels. First, we introduce a variation of maxout activation, called Max-Feature-Map (MFM), into each convolutional layer of CNN. Different from maxout activation that uses many feature maps to linearly approximate an arbitrary convex activation function, MFM does so via a competitive relationship. MFM can not only separate noisy and informative signals but also play the role of feature selection between two feature maps. Second, three networks are carefully designed to obtain better performance meanwhile reducing the number of parameters and computational costs. Lastly, a semantic bootstrapping method is proposed to make the prediction of the networks more consistent with noisy labels. Experimental results show that the proposed framework can utilize large-scale noisy data to learn a Light model that is efficient in computational costs and storage spaces. The learned single network with a 256-D representation achieves state-of-the-art results on various face benchmarks without fine-tuning. The code is released on https://github.com/AlfredXiangWu/LightCNN.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {862 citations (Semantic Scholar/arXiv) [2023-02-28]},
  note = {light-CNN},
  file = {/home/david/Zotero/storage/5STFJ2P7/Wu et al. - 2018 - A Light CNN for Deep Face Representation with Nois.pdf;/home/david/Zotero/storage/25NNZ27W/1511.html}
}

@misc{wuLookBoundaryBoundaryAware2018,
  title = {Look at {{Boundary}}: {{A Boundary-Aware Face Alignment Algorithm}}},
  shorttitle = {Look at {{Boundary}}},
  author = {Wu, Wayne and Qian, Chen and Yang, Shuo and Wang, Quan and Cai, Yici and Zhou, Qiang},
  year = {2018},
  month = may,
  number = {arXiv:1805.10483},
  eprint = {1805.10483},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-15},
  abstract = {We present a novel boundary-aware face alignment algorithm by utilising boundary lines as the geometric structure of a human face to help facial landmark localisation. Unlike the conventional heatmap based method and regression based method, our approach derives face landmarks from boundary lines which remove the ambiguities in the landmark definition. Three questions are explored and answered by this work: 1. Why using boundary? 2. How to use boundary? 3. What is the relationship between boundary estimation and landmarks localisation? Our boundary- aware face alignment algorithm achieves 3.49\% mean error on 300-W Fullset, which outperforms state-of-the-art methods by a large margin. Our method can also easily integrate information from other datasets. By utilising boundary information of 300-W dataset, our method achieves 3.92\% mean error with 0.39\% failure rate on COFW dataset, and 1.25\% mean error on AFLW-Full dataset. Moreover, we propose a new dataset WFLW to unify training and testing across different factors, including poses, expressions, illuminations, makeups, occlusions, and blurriness. Dataset and model will be publicly available at https://wywu.github.io/projects/LAB/LAB.html},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Accepted to CVPR 2018. Project page: https://wywu.github.io/projects/LAB/LAB.html},
  file = {/home/david/Zotero/storage/LCXW94P4/Wu et al. - 2018 - Look at Boundary A Boundary-Aware Face Alignment .pdf;/home/david/Zotero/storage/X92FLFRN/1805.html}
}

@inproceedings{wuRecursiveSpatialTransformer2017,
  title = {Recursive {{Spatial Transformer}} ({{ReST}}) for {{Alignment-Free Face Recognition}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Wu, Wanglong and Kan, Meina and Liu, Xin and Yang, Yi and Shan, Shiguang and Chen, Xilin},
  year = {2017},
  month = oct,
  pages = {3792--3800},
  publisher = {{IEEE}},
  address = {{Venice}},
  doi = {10.1109/ICCV.2017.407},
  urldate = {2023-02-28},
  abstract = {Convolutional Neural Network (CNN) has led to significant progress in face recognition. Currently most CNNbased face recognition methods follow a two-step pipeline, i.e. a detected face is first aligned to a canonical one predefined by a mean face shape, and then it is fed into a CNN to extract features for recognition. The alignment step transforms all faces to the same shape, which can cause loss of geometrical information which is helpful in distinguishing different subjects. Moreover, it is hard to define a single optimal shape for the following recognition, since faces have large diversity in facial features, e.g. poses, illumination, etc. To be free from the above problems with an independent alignment step, we introduce a Recursive Spatial Transformer (ReST) module into CNN, allowing face alignment to be jointly learned with face recognition in an end-to-end fashion. The designed ReST has an intrinsic recursive structure and is capable of progressively aligning faces to a canonical one, even those with large variations. To model non-rigid transformation, multiple ReST modules are organized in a hierarchical structure to account for different parts of faces. Overall, the proposed ReST can handle large face variations and non-rigid transformation, and is end-to-end learnable and adaptive to input, making it an effective alignment-free face recognition solution. Extensive experiments are performed on LFW and YTF datasets, and the proposed ReST outperforms those two-step methods, demonstrating its effectiveness.},
  isbn = {978-1-5386-1032-9},
  langid = {english},
  annotation = {53 citations (Semantic Scholar/DOI) [2023-02-28] 29 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/JJCUKH6G/Wu et al. - 2017 - Recursive Spatial Transformer (ReST) for Alignment.pdf}
}

@inproceedings{wuRotationConsistentMargin2020,
  title = {Rotation {{Consistent Margin Loss}} for {{Efficient Low-Bit Face Recognition}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Wu, Yudong and Wu, Yichao and Gong, Ruihao and Lv, Yuanhao and Chen, Ken and Liang, Ding and Hu, Xiaolin and Liu, Xianglong and Yan, Junjie},
  year = {2020},
  month = jun,
  pages = {6865--6875},
  publisher = {{IEEE}},
  address = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.00690},
  urldate = {2023-02-28},
  abstract = {In this paper, we consider the low-bit quantization problem of face recognition (FR) under the open-set protocol. Different from well explored low-bit quantization on closedset image classification task, the open-set task is more sensitive to quantization errors (QEs). We redefine the QEs in angular space and disentangle it into class error and individual error. These two parts correspond to inter-class separability and intra-class compactness, respectively. Instead of eliminating the entire QEs, we propose the rotation consistent margin (RCM) loss to minimize the individual error, which is more essential to feature discriminative power. Extensive experiments on popular benchmark datasets such as MegaFace Challenge, Youtube Faces (YTF), Labeled Face in the Wild (LFW) and IJB-C show the superiority of proposed loss in low-bit (e.g., 4-, 3-bit) FR quantization tasks.},
  isbn = {978-1-72817-168-5},
  langid = {english},
  annotation = {28 citations (Semantic Scholar/DOI) [2023-02-28] 19 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/8JBVRNN6/Wu et al. - 2020 - Rotation Consistent Margin Loss for Efficient Low-.pdf}
}

@article{xiangConvolutionalNeuralNetworkbased2020,
  title = {A Convolutional Neural Network-Based Linguistic Steganalysis for Synonym Substitution Steganography},
  author = {Xiang, Lingyun and Guo, Guoqing and Yu, Jingming and Sheng, Victor S. and Yang, Peng and Xiang, Lingyun and Guo, Guoqing and Yu, Jingming and Sheng, Victor S. and Yang, Peng},
  year = {2020},
  journal = {Mathematical Biosciences and Engineering},
  volume = {17},
  number = {2},
  pages = {1041--1058},
  issn = {1551-0018},
  doi = {10.3934/mbe.2020055},
  urldate = {2023-02-13},
  abstract = {In this paper, a linguistic steganalysis method based on two-level cascaded convolutional neural networks (CNNs) is proposed to improve the system's ability to detect stego texts, which are generated via synonym substitutions. The first-level network, sentence-level CNN, consists of one convolutional layer with multiple convolutional kernels in different window sizes, one pooling layer to deal with variable sentence lengths, and one fully connected layer with dropout as well as a softmax output, such that two final steganographic features are obtained for each sentence. The unmodified and modified sentences, along with their words, are represented in the form of pre-trained dense word embeddings, which serve as the input of the network. Sentence-level CNN provides the representation of a sentence, and can thus be utilized to predict whether a sentence is unmodified or has been modified by synonym substitutions. In the second level, a text-level CNN exploits the predicted representations of sentences obtained from the sentence-level CNN to determine whether the detected text is a stego text or cover text. Experimental results indicate that the proposed sentence-level CNN can effectively extract sentence features for sentence-level steganalysis tasks and reaches an average accuracy of 82.245\%. Moreover, the proposed steganalysis method achieves greatly improved detection performance when distinguishing stego texts from cover texts.},
  copyright = {2020 The Author(s)},
  langid = {english},
  annotation = {Cc\_license\_type: cc\_by Primary\_atype: Mathematical Biosciences and Engineering Subject\_term: Research article Subject\_term\_id: Research article}
}

@inproceedings{xiangyuzhuHighfidelityPoseExpression2015,
  title = {High-Fidelity {{Pose}} and {{Expression Normalization}} for Face Recognition in the Wild},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {{Xiangyu Zhu} and Lei, Zhen and {Junjie Yan} and Yi, Dong and Li, Stan Z.},
  year = {2015},
  month = jun,
  pages = {787--796},
  publisher = {{IEEE}},
  address = {{Boston, MA, USA}},
  doi = {10.1109/CVPR.2015.7298679},
  urldate = {2023-02-28},
  abstract = {Pose and expression normalization is a crucial step to recover the canonical view of faces under arbitrary conditions, so as to improve the face recognition performance. An ideal normalization method is desired to be automatic, database independent and high-fidelity, where the face appearance should be preserved with little artifact and information loss. However, most normalization methods fail to satisfy one or more of the goals. In this paper, we propose a High-fidelity Pose and Expression Normalization (HPEN) method with 3D Morphable Model (3DMM) which can automatically generate a natural face image in frontal pose and neutral expression. Specifically, we firstly make a landmark marching assumption to describe the non-correspondence between 2D and 3D landmarks caused by pose variations and propose a pose adaptive 3DMM fitting algorithm. Secondly, we mesh the whole image into a 3D object and eliminate the pose and expression variations using an identity preserving 3D transformation. Finally, we propose an inpainting method based on Possion Editing to fill the invisible region caused by self occlusion. Extensive experiments on Multi-PIE and LFW demonstrate that the proposed method significantly improves face recognition performance and outperforms state-of-the-art methods in both constrained and unconstrained environments.},
  isbn = {978-1-4673-6964-0},
  langid = {english},
  annotation = {503 citations (Semantic Scholar/DOI) [2023-02-28] 16 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/B3XI3NSX/Xiangyu Zhu et al. - 2015 - High-fidelity Pose and Expression Normalization fo.pdf}
}

@inproceedings{xiaoImprovingTransferabilityAdversarial2021,
  title = {Improving {{Transferability}} of {{Adversarial Patches}} on {{Face Recognition}} with {{Generative Models}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Xiao, Zihao and Gao, Xianfeng and Fu, Chilin and Dong, Yinpeng and Gao, Wei and Zhang, Xiaolu and Zhou, Jun and Zhu, Jun},
  year = {2021},
  month = jun,
  pages = {11840--11849},
  publisher = {{IEEE}},
  address = {{Nashville, TN, USA}},
  doi = {10.1109/CVPR46437.2021.01167},
  urldate = {2023-02-28},
  abstract = {Face recognition is greatly improved by deep convolutional neural networks (CNNs). Recently, these face recognition models have been used for identity authentication in security sensitive applications. However, deep CNNs are vulnerable to adversarial patches, which are physically realizable and stealthy, raising new security concerns on the real-world applications of these models. In this paper, we evaluate the robustness of face recognition models using adversarial patches based on transferability, where the attacker has limited accessibility to the target models. First, we extend the existing transfer-based attack techniques to generate transferable adversarial patches. However, we observe that the transferability is sensitive to initialization and degrades when the perturbation magnitude is large, indicating the overfitting to the substitute models. Second, we propose to regularize the adversarial patches on the low dimensional data manifold. The manifold is represented by generative models pre-trained on legitimate human face images. Using face-like features as adversarial perturbations through optimization on the manifold, we show that the gaps between the responses of substitute models and the target models dramatically decrease, exhibiting a better transferability. Extensive digital world experiments are conducted to demonstrate the superiority of the proposed method in the black-box setting. We apply the proposed method in the physical world as well.},
  isbn = {978-1-66544-509-2},
  langid = {english},
  annotation = {31 citations (Semantic Scholar/DOI) [2023-02-28] 12 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/CSRUW79T/Xiao et al. - 2021 - Improving Transferability of Adversarial Patches o.pdf}
}

@inproceedings{xiaoRecurrent3D2DDual2017,
  title = {Recurrent {{3D-2D Dual Learning}} for {{Large-Pose Facial Landmark Detection}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Xiao, Shengtao and Feng, Jiashi and Liu, Luoqi and Nie, Xuecheng and Wang, Wei and Yan, Shuicheng and Kassim, Ashraf},
  year = {2017},
  month = oct,
  pages = {1642--1651},
  publisher = {{IEEE}},
  address = {{Venice}},
  doi = {10.1109/ICCV.2017.181},
  urldate = {2023-04-15},
  abstract = {Despite remarkable progress of face analysis techniques, detecting landmarks on large-pose faces is still difficult due to self-occlusion, subtle landmark difference and incomplete information. To address these challenging issues, we introduce a novel recurrent 3D-2D dual learning model that alternatively performs 2D-based 3D face model refinement and 3D-to-2D projection based 2D landmark refinement to reliably reason about self-occluded landmarks, precisely capture the subtle landmark displacement and accurately detect landmarks even in presence of extremely large poses. The proposed model presents the first loop-closed learning framework that effectively exploits the informative feedback from the 3D-2D learning and its dual 2D-3D refinement tasks in a recurrent manner. Benefiting from these two mutual-boosting steps, our proposed model demonstrates appealing robustness to large poses (up to profile pose) and outstanding ability to capture fine-scale landmark displacement compared with existing 3D models. It achieves new state-of-the-art on the challenging AFLW benchmark. Moreover, our proposed model introduces a new architectural design that economically utilizes intermediate features and achieves 4\texttimes{} faster speed than its deep learning based counterparts.},
  isbn = {978-1-5386-1032-9},
  langid = {english},
  file = {/home/david/Zotero/storage/K9EKPWCJ/Xiao et al. - 2017 - Recurrent 3D-2D Dual Learning for Large-Pose Facia.pdf}
}

@misc{xuCenterFaceJointFace2019,
  title = {{{CenterFace}}: {{Joint Face Detection}} and {{Alignment Using Face}} as {{Point}}},
  shorttitle = {{{CenterFace}}},
  author = {Xu, Yuanyuan and Yan, Wan and Sun, Haixin and Yang, Genke and Luo, Jiliang},
  year = {2019},
  month = nov,
  number = {arXiv:1911.03599},
  eprint = {1911.03599},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-13},
  abstract = {Face detection and alignment in unconstrained environment is always deployed on edge devices which have limited memory storage and low computing power. This paper proposes a one-stage method named CenterFace to simultaneously predict facial box and landmark location with real-time speed and high accuracy. The proposed method also belongs to the anchor free category. This is achieved by: (a) learning face existing possibility by the semantic maps, (b) learning bounding box, offsets and five landmarks for each position that potentially contains a face. Specifically, the method can run in real-time on a single CPU core and 200 FPS using NVIDIA 2080TI for VGA-resolution images, and can simultaneously achieve superior accuracy (WIDER FACE Val/Test-Easy: 0.935/0.932, Medium: 0.924/0.921, Hard: 0.875/0.873 and FDDB discontinuous: 0.980, continuous: 0.732). A demo of CenterFace can be available at https://github.com/Star-Clouds/CenterFace.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: 11 pages, 3 figures. A demo of CenterFace can be available at https://github.com/Star-Clouds/CenterFace},
  file = {/home/david/Zotero/storage/KY2DRTI3/Xu et al. - 2019 - CenterFace Joint Face Detection and Alignment Usi.pdf;/home/david/Zotero/storage/4UHN2B9A/1911.html}
}

@article{xuMachineLearningConstruction2021,
  title = {Machine Learning in Construction: {{From}} Shallow to Deep Learning},
  shorttitle = {Machine Learning in Construction},
  author = {Xu, Yayin and Zhou, Ying and Sekula, Przemyslaw and Ding, Lieyun},
  year = {2021},
  month = may,
  journal = {Developments in the Built Environment},
  volume = {6},
  pages = {100045},
  issn = {26661659},
  doi = {10.1016/j.dibe.2021.100045},
  abstract = {The development of artificial intelligence technology is currently bringing about new opportunities in construction. Machine learning is a major area of interest within the field of artificial intelligence, playing a pivotal role in the process of making construction ``smart''. The application of machine learning in construction has the potential to open up an array of opportunities such as site supervision, automatic detection, and intelligent maintenance. However, the implementation of machine learning faces a range of challenges due to the difficulties in acquiring labeled data, especially when applied in a highly complex construction site environment. This paper reviews the history of machine learning development from shallow to deep learning and its applications in construction. The strengths and weaknesses of machine learning technology in construction have been analyzed in order to foresee the future direction of machine learning applications in this sphere. Furthermore, this paper presents suggestions which may benefit researchers in terms of combining specific knowledge domains in construction with machine learning algorithms so as to develop dedicated deep network models for the industry.},
  langid = {english},
  file = {/home/david/Zotero/storage/GTU5IIL7/1-s2.0-S2666165921000041-main.pdf}
}

@article{yamashitaConvolutionalNeuralNetworks2018,
  title = {Convolutional Neural Networks: An Overview and Application in Radiology},
  shorttitle = {Convolutional Neural Networks},
  author = {Yamashita, Rikiya and Nishio, Mizuho and Do, Richard Kinh Gian and Togashi, Kaori},
  year = {2018},
  month = aug,
  journal = {Insights into Imaging},
  volume = {9},
  number = {4},
  pages = {611--629},
  publisher = {{SpringerOpen}},
  issn = {1869-4101},
  doi = {10.1007/s13244-018-0639-9},
  urldate = {2023-02-09},
  abstract = {Convolutional neural network (CNN), a class of artificial neural networks that has become dominant in various computer vision tasks, is attracting interest across a variety of domains, including radiology. CNN is designed to automatically and adaptively learn spatial hierarchies of features through backpropagation by using multiple building blocks, such as convolution layers, pooling layers, and fully connected layers. This review article offers a perspective on the basic concepts of CNN and its application to various radiological tasks, and discusses its challenges and future directions in the field of radiology. Two challenges in applying CNN to radiological tasks, small dataset and overfitting, will also be covered in this article, as well as techniques to minimize them. Being familiar with the concepts and advantages, as well as limitations, of CNN is essential to leverage its potential in diagnostic radiology, with the goal of augmenting the performance of radiologists and improving patient care. \textbullet{} Convolutional neural network is a class of deep learning methods which has become dominant in various computer vision tasks and is attracting interest across a variety of domains, including radiology. \textbullet{} Convolutional neural network is composed of multiple building blocks, such as convolution layers, pooling layers, and fully connected layers, and is designed to automatically and adaptively learn spatial hierarchies of features through a backpropagation algorithm. \textbullet{} Familiarity with the concepts and advantages, as well as limitations, of convolutional neural network is essential to leverage its potential to improve radiologist performance and, eventually, patient care.},
  copyright = {2018 The Author(s)},
  langid = {english},
  file = {/home/david/Zotero/storage/UCQ9XZCM/Yamashita et al. - 2018 - Convolutional neural networks an overview and app.pdf}
}

@inproceedings{Yang2009,
  type = {Conference Paper},
  title = {Detecting Human Actions in Surveillance Videos},
  author = {Yang, Ming and Ji, Shuiwang and Xu, Wei and Wang, Jinjun and Lv, Fengjun and Yu, Kai and Gong, Yihong and Dikmen, Mert and Lin, Dennis J. and Huang, Thomas S.},
  year = {2009},
  series = {2009 {{TREC Video Retrieval Evaluation Notebook Papers}}},
  publication_stage = {Final},
  source = {Scopus},
  note = {Cited by: 26}
}

@misc{yangAggregateChannelFeatures2014,
  title = {Aggregate Channel Features for Multi-View Face Detection},
  author = {Yang, Bin and Yan, Junjie and Lei, Zhen and Li, Stan Z.},
  year = {2014},
  month = sep,
  number = {arXiv:1407.4023},
  eprint = {1407.4023},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-08},
  abstract = {Face detection has drawn much attention in recent decades since the seminal work by Viola and Jones. While many subsequences have improved the work with more powerful learning algorithms, the feature representation used for face detection still can't meet the demand for effectively and efficiently handling faces with large appearance variance in the wild. To solve this bottleneck, we borrow the concept of channel features to the face detection domain, which extends the image channel to diverse types like gradient magnitude and oriented gradient histograms and therefore encodes rich information in a simple form. We adopt a novel variant called aggregate channel features, make a full exploration of feature design, and discover a multi-scale version of features with better performance. To deal with poses of faces in the wild, we propose a multi-view detection approach featuring score re-ranking and detection adjustment. Following the learning pipelines in Viola-Jones framework, the multi-view face detector using aggregate channel features shows competitive performance against state-of-the-art algorithms on AFW and FDDB testsets, while runs at 42 FPS on VGA images.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: 8 pages, 6 figures. Submitted to International Joint Conference on Biometrics, 2014},
  file = {/home/david/Zotero/storage/SZFI3HIU/Yang et al. - 2014 - Aggregate channel features for multi-view face det.pdf;/home/david/Zotero/storage/XYLS8QFD/1407.html}
}

@inproceedings{yangStackedHourglassNetwork2017,
  title = {Stacked {{Hourglass Network}} for {{Robust Facial Landmark Localisation}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Yang, Jing and Liu, Qingshan and Zhang, Kaihua},
  year = {2017},
  month = jul,
  pages = {2025--2033},
  publisher = {{IEEE}},
  address = {{Honolulu, HI, USA}},
  doi = {10.1109/CVPRW.2017.253},
  urldate = {2023-04-15},
  abstract = {With the increasing number of public available training data for face alignment, the regression-based methods attracted much attention and have become the dominant methods to solve this problem. There are two main factors, the variance of the regression target and the capacity of regression model, affecting the performance of the regression task. In this paper, we present a Stacked Hourglass Network for robust facial landmark localisation. We first adopt a supervised face transformation to remove the translation, scale and rotation variation of each face, in order to reduce the variance of the regression target. Then we employ a deep convolutional neural network named Stacked Hourglass Network to increase the capacity of the regression model. To better evaluate the proposed method, we reimplement two popular cascade shape regression models, SDM and LBF, for comparison. Extensive experiments on four challenging datasets, COFW, IBUG, 300W and the Menpo Benchmark, prove the effectiveness of the proposed method.},
  isbn = {978-1-5386-0733-6},
  langid = {english},
  file = {/home/david/Zotero/storage/E4WQEPEY/Yang et al. - 2017 - Stacked Hourglass Network for Robust Facial Landma.pdf}
}

@misc{yanVarGFaceNetEfficientVariable2019,
  title = {{{VarGFaceNet}}: {{An Efficient Variable Group Convolutional Neural Network}} for {{Lightweight Face Recognition}}},
  shorttitle = {{{VarGFaceNet}}},
  author = {Yan, Mengjia and Zhao, Mengao and Xu, Zining and Zhang, Qian and Wang, Guoli and Su, Zhizhong},
  year = {2019},
  month = nov,
  number = {arXiv:1910.04985},
  eprint = {1910.04985},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-02-27},
  abstract = {To improve the discriminative and generalization ability of lightweight network for face recognition, we propose an efficient variable group convolutional network called VarGFaceNet. Variable group convolution is introduced by VarGNet to solve the conflict between small computational cost and the unbalance of computational intensity inside a block. We employ variable group convolution to design our network which can support large scale face identification while reduce computational cost and parameters. Specifically, we use a head setting to reserve essential information at the start of the network and propose a particular embedding setting to reduce parameters of fully-connected layer for embedding. To enhance interpretation ability, we employ an equivalence of angular distillation loss to guide our lightweight network and we apply recursive knowledge distillation to relieve the discrepancy between the teacher model and the student model. The champion of deepglint-light track of LFR (2019) challenge demonstrates the effectiveness of our model and approach. Implementation of VarGFaceNet will be released at https://github.com/zma-c-137/VarGFaceNet soon.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {51 citations (Semantic Scholar/arXiv) [2023-02-28]},
  note = {Comment: 8 pages,2 figures. In Proceedings of the IEEE International Conference on Computer Vision Workshop, 2019},
  file = {/home/david/Zotero/storage/KQWZCTDY/Yan et al. - 2019 - VarGFaceNet An Efficient Variable Group Convoluti.pdf;/home/david/Zotero/storage/X58MXGBC/1910.html}
}

@misc{yiLearningFaceRepresentation2014,
  title = {Learning {{Face Representation}} from {{Scratch}}},
  author = {Yi, Dong and Lei, Zhen and Liao, Shengcai and Li, Stan Z.},
  year = {2014},
  month = nov,
  number = {arXiv:1411.7923},
  eprint = {1411.7923},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-25},
  abstract = {Pushing by big data and deep convolutional neural network (CNN), the performance of face recognition is becoming comparable to human. Using private large scale training datasets, several groups achieve very high performance on LFW, i.e., 97\% to 99\%. While there are many open source implementations of CNN, none of large scale face dataset is publicly available. The current situation in the field of face recognition is that data is more important than algorithm. To solve this problem, this paper proposes a semi-automatical way to collect face images from Internet and builds a large scale dataset containing about 10,000 subjects and 500,000 images, called CASIAWebFace. Based on the database, we use a 11-layer CNN to learn discriminative representation and obtain state-of-theart accuracy on LFW and YTF. The publication of CASIAWebFace will attract more research groups entering this field and accelerate the development of face recognition in the wild.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/david/Zotero/storage/58Q26GCS/Yi et al. - 2014 - Learning Face Representation from Scratch.pdf;/home/david/Zotero/storage/Y9ZEP7RQ/1411.html}
}

@inproceedings{yinFeatureTransferLearning2019,
  title = {Feature {{Transfer Learning}} for {{Face Recognition With Under-Represented Data}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Yin, Xi and Yu, Xiang and Sohn, Kihyuk and Liu, Xiaoming and Chandraker, Manmohan},
  year = {2019},
  month = jun,
  pages = {5697--5706},
  publisher = {{IEEE}},
  address = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.00585},
  urldate = {2023-02-28},
  abstract = {Despite the large volume of face recognition datasets, there is a significant portion of subjects, of which the samples are insufficient and thus under-represented. Ignoring such significant portion results in insufficient training data. Training with under-represented data leads to biased classifiers in conventionally-trained deep networks. In this paper, we propose a center-based feature transfer framework to augment the feature space of under-represented subjects from the regular subjects that have sufficiently diverse samples. A Gaussian prior of the variance is assumed across all subjects and the variance from regular ones are transferred to the under-represented ones. This encourages the under-represented distribution to be closer to the regular distribution. Further, an alternating training regimen is proposed to simultaneously achieve less biased classifiers and a more discriminative feature representation. We conduct ablative study to mimic the under-represented datasets by varying the portion of under-represented classes on the MSCeleb-1M dataset. Advantageous results on LFW, IJB-A and MS-Celeb-1M demonstrate the effectiveness of our feature transfer and training strategy, compared to both general baselines and state-of-the-art methods. Moreover, our feature transfer successfully presents smooth visual interpolation, which conducts disentanglement to preserve identity of a class while augmenting its feature space with non-identity variations such as pose and lighting.},
  isbn = {978-1-72813-293-8},
  langid = {english},
  annotation = {178 citations (Semantic Scholar/DOI) [2023-02-28] 116 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/FGL5FHIR/Yin et al. - 2019 - Feature Transfer Learning for Face Recognition Wit.pdf}
}

@inproceedings{yinInterpretableFaceRecognition2019,
  title = {Towards {{Interpretable Face Recognition}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Yin, Bangjie and Tran, Luan and Li, Haoxiang and Shen, Xiaohui and Liu, Xiaoming},
  year = {2019},
  month = oct,
  pages = {9347--9356},
  publisher = {{IEEE}},
  address = {{Seoul, Korea (South)}},
  doi = {10.1109/ICCV.2019.00944},
  urldate = {2023-02-28},
  abstract = {Deep CNNs have been pushing the frontier of visual recognition over past years. Besides recognition accuracy, strong demands in understanding deep CNNs in the research community motivate developments of tools to dissect pre-trained models to visualize how they make predictions. Recent works further push the interpretability in the network learning stage to learn more meaningful representations. In this work, focusing on a specific area of visual recognition, we report our efforts towards interpretable face recognition. We propose a spatial activation diversity loss to learn more structured face representations. By leveraging the structure, we further design a feature activation diversity loss to push the interpretable representations to be discriminative and robust to occlusions. We demonstrate on three face recognition benchmarks that our proposed method is able to achieve the state-of-art face recognition accuracy with easily interpretable face representations.},
  isbn = {978-1-72814-803-8},
  langid = {english},
  annotation = {48 citations (Semantic Scholar/DOI) [2023-02-28] 39 citations (Crossref) [2023-02-28]},
  note = {Loss functions},
  file = {/home/david/Zotero/storage/ULX2GFLW/Yin et al. - 2019 - Towards Interpretable Face Recognition.pdf}
}

@inproceedings{z.caoFaceRecognitionLearningbased2010,
  title = {Face Recognition with Learning-Based Descriptor},
  booktitle = {2010 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {{Z. Cao} and {Q. Yin} and {X. Tang} and {J. Sun}},
  year = {2010},
  month = jun,
  pages = {2707--2714},
  doi = {10.1109/CVPR.2010.5539992},
  isbn = {1063-6919}
}

@article{zafeiriouSurveyFaceDetection2015,
  title = {A Survey on Face Detection in the Wild: {{Past}}, Present and Future},
  author = {Zafeiriou, Stefanos and Zhang, Cha and Zhang, Zhengyou},
  year = {2015},
  month = sep,
  journal = {Computer Vision and Image Understanding},
  volume = {138},
  pages = {1--24},
  issn = {1077-3142},
  doi = {10.1016/j.cviu.2015.03.015},
  abstract = {Face detection is one of the most studied topics in computer vision literature, not only because of the challenging nature of face as an object, but also due to the countless applications that require the application of face detection as a first step. During the past 15years, tremendous progress has been made due to the availability of data in unconstrained capture conditions (so-called `in-the-wild') through the Internet, the effort made by the community to develop publicly available benchmarks, as well as the progress in the development of robust computer vision algorithms. In this paper, we survey the recent advances in real-world face detection techniques, beginning with the seminal Viola\textendash Jones face detector methodology. These techniques are roughly categorized into two general schemes: rigid templates, learned mainly via boosting based methods or by the application of deep neural networks, and deformable models that describe the face by its parts. Representative methods will be described in detail, along with a few additional successful methods that we briefly go through at the end. Finally, we survey the main databases used for the evaluation of face detection algorithms and recent benchmarking efforts, and discuss the future of face detection.},
  keywords = {Boosting,Deep neural networks,Deformable models,Face detection,Feature extraction}
}

@inproceedings{zellSimulationNeuronalerNetze1994,
  title = {Simulation Neuronaler {{Netze}}},
  author = {Zell, Andreas},
  year = {1994}
}

@inproceedings{zhangAdaptiveLabelNoise2021,
  title = {Adaptive {{Label Noise Cleaning}} with {{Meta-Supervision}} for {{Deep Face Recognition}}},
  booktitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Zhang, Yaobin and Deng, Weihong and Zhong, Yaoyao and Hu, Jiani and Li, Xian and Zhao, Dongyue and Wen, Dongchao},
  year = {2021},
  month = oct,
  pages = {15045--15055},
  issn = {2380-7504},
  doi = {10.1109/ICCV48922.2021.01479},
  abstract = {The training of a deep face recognition system usually faces the interference of label noise in the training data. However, it is difficult to obtain a high-precision cleaning model to remove these noises. In this paper, we propose an adaptive label noise cleaning algorithm based on meta-learning for face recognition datasets, which can learn the distribution of the data to be cleaned and make automatic adjustments based on class differences. It first learns re-liable cleaning knowledge from well-labeled noisy data, then gradually transfers it to the target data with meta-supervision to improve performance. A threshold adapter module is also proposed to address the drift problem in transfer learning methods. Extensive experiments clean two noisy in-the-wild face recognition datasets and show the effectiveness of the proposed method to reach state-of-the-art performance on the IJB-C face recognition benchmark.},
  keywords = {Biometrics,Computational modeling,Computer vision,Face recognition,Faces,Interference,Training,Training data,Transfer learning},
  annotation = {7 citations (Semantic Scholar/DOI) [2023-02-28]},
  file = {/home/david/Zotero/storage/9PHK4FL2/Zhang et al. - 2021 - Adaptive Label Noise Cleaning with Meta-Supervisio.pdf;/home/david/Zotero/storage/LM465HK4/stamp.html}
}

@misc{zhangFaceBoxesCPURealtime2018,
  title = {{{FaceBoxes}}: {{A CPU Real-time Face Detector}} with {{High Accuracy}}},
  shorttitle = {{{FaceBoxes}}},
  author = {Zhang, Shifeng and Zhu, Xiangyu and Lei, Zhen and Shi, Hailin and Wang, Xiaobo and Li, Stan Z.},
  year = {2018},
  month = dec,
  number = {arXiv:1708.05234},
  eprint = {1708.05234},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-14},
  abstract = {Although tremendous strides have been made in face detection, one of the remaining open challenges is to achieve real-time speed on the CPU as well as maintain high performance, since effective models for face detection tend to be computationally prohibitive. To address this challenge, we propose a novel face detector, named FaceBoxes, with superior performance on both speed and accuracy. Specifically, our method has a lightweight yet powerful network structure that consists of the Rapidly Digested Convolutional Layers (RDCL) and the Multiple Scale Convolutional Layers (MSCL). The RDCL is designed to enable FaceBoxes to achieve real-time speed on the CPU. The MSCL aims at enriching the receptive fields and discretizing anchors over different layers to handle faces of various scales. Besides, we propose a new anchor densification strategy to make different types of anchors have the same density on the image, which significantly improves the recall rate of small faces. As a consequence, the proposed detector runs at 20 FPS on a single CPU core and 125 FPS using a GPU for VGA-resolution images. Moreover, the speed of FaceBoxes is invariant to the number of faces. We comprehensively evaluate this method and present state-of-the-art detection performance on several face detection benchmark datasets, including the AFW, PASCAL face, and FDDB. Code is available at https://github.com/sfzhang15/FaceBoxes},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Accepted by IJCB 2017; Added references; Released codes},
  file = {/home/david/Zotero/storage/LK5B9QC2/Zhang et al. - 2018 - FaceBoxes A CPU Real-time Face Detector with High.pdf;/home/david/Zotero/storage/YYSPM5Q4/1708.html}
}

@misc{zhangFaceDetectionUsing2018,
  title = {Face {{Detection Using Improved Faster RCNN}}},
  author = {Zhang, Changzheng and Xu, Xiang and Tu, Dandan},
  year = {2018},
  month = feb,
  number = {arXiv:1802.02142},
  eprint = {1802.02142},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-13},
  abstract = {Faster RCNN has achieved great success for generic object detection including PASCAL object detection and MS COCO object detection. In this report, we propose a detailed designed Faster RCNN method named FDNet1.0 for face detection. Several techniques were employed including multi-scale training, multi-scale testing, light-designed RCNN, some tricks for inference and a vote-based ensemble method. Our method achieves two 1th places and one 2nd place in three tasks over WIDER FACE validation dataset (easy set, medium set, hard set).},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/david/Zotero/storage/C5G37YI7/Zhang et al. - 2018 - Face Detection Using Improved Faster RCNN.pdf;/home/david/Zotero/storage/TPEUC7HX/1802.html}
}

@inproceedings{zhangGlobalLocalGCNLargeScale2020,
  title = {Global-{{Local GCN}}: {{Large-Scale Label Noise Cleansing}} for {{Face Recognition}}},
  shorttitle = {Global-{{Local GCN}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zhang, Yaobin and Deng, Weihong and Wang, Mei and Hu, Jiani and Li, Xian and Zhao, Dongyue and Wen, Dongchao},
  year = {2020},
  month = jun,
  pages = {7728--7737},
  publisher = {{IEEE}},
  address = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.00775},
  urldate = {2023-02-28},
  abstract = {In the field of face recognition, large-scale web-collected datasets are essential for learning discriminative representations, but they suffer from noisy identity labels, such as outliers and label flips. It is beneficial to automatically cleanse their label noise for improving recognition accuracy. Unfortunately, existing cleansing methods cannot accurately identify noise in the wild. To solve this problem, we propose an effective automatic label noise cleansing framework for face recognition datasets, FaceGraph. Using two cascaded graph convolutional networks, FaceGraph performs global-to-local discrimination to select useful data in a noisy environment. Extensive experiments show that cleansing widely used datasets, such as CASIA-WebFace, VGGFace2, MegaFace2, and MS-Celeb-1M, using the proposed method can improve the recognition performance of state-of-the-art representation learning methods like Arcface. Further, we cleanse massive self-collected celebrity data, namely MillionCelebs, to provide 18.8M images of 636K identities. Training with the new data, Arcface surpasses state-of-the-art performance by a notable margin to reach 95.62\% TPR at 1e-5 FPR on the IJB-C benchmark.},
  isbn = {978-1-72817-168-5},
  langid = {english},
  annotation = {36 citations (Semantic Scholar/DOI) [2023-02-28] 24 citations (Crossref) [2023-02-28]},
  note = {Dataset's cleansing},
  file = {/home/david/Zotero/storage/34DEN368/Zhang et al. - 2020 - Global-Local GCN Large-Scale Label Noise Cleansin.pdf}
}

@article{zhangImprovedBreastCancer2021,
  title = {Improved {{Breast Cancer Classification Through Combining Graph Convolutional Network}} and {{Convolutional Neural Network}}},
  author = {Zhang, Yu-Dong and Satapathy, Suresh Chandra and Guttery, David S. and G{\'o}rriz, Juan Manuel and Wang, Shui-Hua},
  year = {2021},
  month = mar,
  journal = {Information Processing \& Management},
  volume = {58},
  number = {2},
  pages = {102439},
  issn = {0306-4573},
  doi = {10.1016/j.ipm.2020.102439},
  abstract = {Aim In a pilot study to improve detection of malignant lesions in breast mammograms, we aimed to develop a new method called BDR-CNN-GCN, combining two advanced neural networks: (i) graph convolutional network (GCN); and (ii) convolutional neural network (CNN). Method We utilised a standard 8-layer CNN, then integrated two improvement techniques: (i) batch normalization (BN) and (ii) dropout (DO). Finally, we utilized rank-based stochastic pooling (RSP) to substitute the traditional max pooling. This resulted in BDR-CNN, which is a combination of CNN, BN, DO, and RSP. This BDR-CNN was hybridized with a two-layer GCN, and yielded our BDR-CNN-GCN model which was then utilized for analysis of breast mammograms as a 14-way data augmentation method. Results As proof of concept, we ran our BDR-CNN-GCN algorithm 10 times on the breast mini-MIAS dataset (containing 322 mammographic images), achieving a sensitivity of 96.20{$\pm$}2.90\%, a specificity of 96.00{$\pm$}2.31\% and an accuracy of 96.10{$\pm$}1.60\%. Conclusion Our BDR-CNN-GCN showed improved performance compared to five proposed neural network models and 15 state-of-the-art breast cancer detection approaches, proving to be an effective method for data augmentation and improved detection of malignant breast masses.},
  keywords = {Artificial intelligence,Breast cancer classification,Convolutional neural network,Data augmentation,Deep learning,Graph convolutional network,Mammogram,Rank-based stochastic pooling}
}

@article{zhangJointFaceDetection2016a,
  title = {Joint {{Face Detection}} and {{Alignment}} Using {{Multi-task Cascaded Convolutional Networks}}},
  author = {Zhang, Kaipeng and Zhang, Zhanpeng and Li, Zhifeng and Qiao, Yu},
  year = {2016},
  month = oct,
  journal = {IEEE Signal Processing Letters},
  volume = {23},
  number = {10},
  eprint = {1604.02878},
  primaryclass = {cs},
  pages = {1499--1503},
  issn = {1070-9908, 1558-2361},
  doi = {10.1109/LSP.2016.2603342},
  urldate = {2023-04-13},
  abstract = {Face detection and alignment in unconstrained environment are challenging due to various poses, illuminations and occlusions. Recent studies show that deep learning approaches can achieve impressive performance on these two tasks. In this paper, we propose a deep cascaded multi-task framework which exploits the inherent correlation between them to boost up their performance. In particular, our framework adopts a cascaded structure with three stages of carefully designed deep convolutional networks that predict face and landmark location in a coarse-to-fine manner. In addition, in the learning process, we propose a new online hard sample mining strategy that can improve the performance automatically without manual sample selection. Our method achieves superior accuracy over the state-of-the-art techniques on the challenging FDDB and WIDER FACE benchmark for face detection, and AFLW benchmark for face alignment, while keeps real time performance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Submitted to IEEE Signal Processing Letters},
  file = {/home/david/Zotero/storage/38E5IQNC/Zhang et al. - 2016 - Joint Face Detection and Alignment using Multi-tas.pdf;/home/david/Zotero/storage/KYMENL53/1604.html}
}

@inproceedings{zhangRangeLossDeep2017,
  title = {Range {{Loss}} for {{Deep Face Recognition}} with {{Long-Tailed Training Data}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Zhang, Xiao and Fang, Zhiyuan and Wen, Yandong and Li, Zhifeng and Qiao, Yu},
  year = {2017},
  month = oct,
  pages = {5419--5428},
  publisher = {{IEEE}},
  address = {{Venice}},
  doi = {10.1109/ICCV.2017.578},
  urldate = {2023-02-28},
  abstract = {Deep convolutional neural networks have achieved significant improvements on face recognition task due to their ability to learn highly discriminative features from tremendous amounts of face images. Many large scale face datasets exhibit long-tail distribution where a small number of entities (persons) have large number of face images while a large number of persons only have very few face samples (long tail). Most of the existing works alleviate this problem by simply cutting the tailed data and only keep identities with enough number of examples. Unlike these work, this paper investigated how long-tailed data impact the training of face CNNs and develop a novel loss function, called range loss, to effectively utilize the tailed data in training process. More specifically, range loss is designed to reduce overall intrapersonal variations while enlarge interpersonal differences simultaneously. Extensive experiments on two face recognition benchmarks, Labeled Faces in the Wild (LFW) [11] and YouTube Faces (YTF) [33], demonstrate the effectiveness of the proposed range loss in overcoming the long tail effect, and show the good generalization ability of the proposed methods.},
  isbn = {978-1-5386-1032-9},
  langid = {english},
  annotation = {340 citations (Semantic Scholar/DOI) [2023-02-28] 171 citations (Crossref) [2023-02-28]},
  note = {Range Loss},
  file = {/home/david/Zotero/storage/I6PB4JZ7/Zhang et al. - 2017 - Range Loss for Deep Face Recognition with Long-Tai.pdf}
}

@article{zhangStudyArtificialIntelligence2021,
  title = {Study on Artificial Intelligence: {{The}} State of the Art and Future Prospects},
  shorttitle = {Study on Artificial Intelligence},
  author = {Zhang, Caiming and Lu, Yang},
  year = {2021},
  month = sep,
  journal = {Journal of Industrial Information Integration},
  volume = {23},
  pages = {100224},
  issn = {2452414X},
  doi = {10.1016/j.jii.2021.100224},
  urldate = {2023-01-11},
  abstract = {In the world, the technological and industrial revolution is accelerating by the widespread application of new generation information and communication technologies, such as AI, IoT (the Internet of Things), and blockchain technology. Artificial intelligence has attracted much attention from government, industry, and academia. In this study, popular articles published in recent years that relate to artificial intelligence are selected and explored. This study aims to provide a review of artificial intelligence based on industry information integration. It presents an overview of the scope of artificial intelligence using background, drivers, technologies, and applications, as well as logical opinions regarding the development of artificial intelligence. This paper may play a role in AIrelated research and should provide important insights for practitioners in the real world.The main contribu\- tion of this study is that it clarifies the state of the art of AI for future study.},
  langid = {english},
  file = {/storage/Master Thesis/Articles/1-s2.0-S2452414X21000248-main.pdf}
}

@inproceedings{zhangUnifyingMarginBasedSoftmax2023,
  title = {Unifying {{Margin-Based Softmax Losses}} in {{Face Recognition}}},
  booktitle = {2023 {{IEEE}}/{{CVF Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Zhang, Yang and Herdade, Simao and Thadani, Kapil and Dodds, Eric and Culpepper, Jack and Ku, Yueh-Ning},
  year = {2023},
  month = jan,
  pages = {3537--3546},
  publisher = {{IEEE}},
  address = {{Waikoloa, HI, USA}},
  doi = {10.1109/WACV56688.2023.00354},
  urldate = {2023-02-28},
  abstract = {In this work, we develop a theoretical and experimental framework to study the effect of margin penalties on angular softmax losses, which have led to state-of-the-art performance in face recognition. We also introduce a new multiplicative margin which performs comparably to previously proposed additive margins when the model is trained to convergence. A regime of the margin parameters can lead to degenerate minima, but these can be reliably avoided through the use of two regularization techniques that we propose. Our theory predicts the minimal angular distance between sample embeddings and the correct and wrong class prototype vectors learned during training, and it suggests a new method to identify optimal margin parameters without expensive tuning. Finally, we conduct a thorough ablation study of the margin parameters in our proposed framework, and we characterize the sensitivity of generalization to each parameter both theoretically and through experiments on standard face recognition benchmarks.},
  isbn = {978-1-66549-346-8},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/DOI) [2023-04-18] 0 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/KTBDKXAF/Zhang et al. - 2023 - Unifying Margin-Based Softmax Losses in Face Recog.pdf}
}

@article{zhaoAgeInvariantFaceRecognition2022,
  title = {Towards {{Age-Invariant Face Recognition}}},
  author = {Zhao, Jian and Yan, Shuicheng and Feng, Jiashi},
  year = {2022},
  month = jan,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {44},
  number = {1},
  pages = {474--487},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2020.3011426},
  abstract = {Despite the remarkable progress in face recognition related technologies, reliably recognizing faces across ages remains a big challenge. The appearance of a human face changes substantially over time, resulting in significant intra-class variations. As opposed to current techniques for age-invariant face recognition, which either directly extract age-invariant features for recognition, or first synthesize a face that matches target age before feature extraction, we argue that it is more desirable to perform both tasks jointly so that they can leverage each other. To this end, we propose a deep Age-Invariant Model (AIM) for face recognition in the wild with three distinct novelties. First, AIM presents a novel unified deep architecture jointly performing cross-age face synthesis and recognition in a mutual boosting way. Second, AIM achieves continuous face rejuvenation/aging with remarkable photorealistic and identity-preserving properties, avoiding the requirement of paired data and the true age of testing samples. Third, effective and novel training strategies are developed for end-to-end learning of the whole deep architecture, which generates powerful age-invariant face representations explicitly disentangled from the age variation. Moreover, we construct a new large-scale Cross-Age Face Recognition (CAFR) benchmark dataset to facilitate existing efforts and push the frontiers of age-invariant face recognition research. Extensive experiments on both our CAFR dataset and several other cross-age datasets (MORPH, CACD, and FG-NET) demonstrate the superiority of the proposed AIM model over the state-of-the-arts. Benchmarking our model on the popular unconstrained face recognition datasets YTF and IJB-C additionally verifies its promising generalization ability in recognizing faces in the wild.},
  keywords = {Age-invariant face recognition,age-invariant model,Aging,benchmark dataset,Benchmark testing,Face,Face recognition,Feature extraction,generative adversarial networks,Robustness,Training},
  file = {/home/david/Zotero/storage/FJTZY94I/Zhao et al. - 2022 - Towards Age-Invariant Face Recognition.pdf;/home/david/Zotero/storage/R9LBBCQB/stamp.html}
}

@inproceedings{zhaoPoseInvariantFace2018,
  title = {Towards {{Pose Invariant Face Recognition}} in the {{Wild}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Zhao, Jian and Cheng, Yu and Xu, Yan and Xiong, Lin and Li, Jianshu and Zhao, Fang and Jayashree, Karlekar and Pranata, Sugiri and Shen, Shengmei and Xing, Junliang and Yan, Shuicheng and Feng, Jiashi},
  year = {2018},
  month = jun,
  pages = {2207--2216},
  publisher = {{IEEE}},
  address = {{Salt Lake City, UT}},
  doi = {10.1109/CVPR.2018.00235},
  urldate = {2023-02-28},
  abstract = {Pose variation is one key challenge in face recognition. As opposed to current techniques for pose invariant face recognition, which either directly extract pose invariant features for recognition, or first normalize profile face images to frontal pose before feature extraction, we argue that it is more desirable to perform both tasks jointly to allow them to benefit from each other. To this end, we propose a Pose Invariant Model (PIM) for face recognition in the wild, with three distinct novelties. First, PIM is a novel and unified deep architecture, containing a Face Frontalization sub-Net (FFN) and a Discriminative Learning sub-Net (DLN), which are jointly learned from end to end. Second, FFN is a well-designed dual-path Generative Adversarial Network (GAN) which simultaneously perceives global structures and local details, incorporated with an unsupervised cross-domain adversarial training and a ``learning to learn'' strategy for high-fidelity and identity-preserving frontal view synthesis. Third, DLN is a generic Convolutional Neural Network (CNN) for face recognition with our enforced cross-entropy optimization strategy for learning discriminative yet generalized feature representation. Qualitative and quantitative experiments on both controlled and in-the-wild benchmarks demonstrate the superiority of the proposed model over the state-of-the-arts.},
  isbn = {978-1-5386-6420-9},
  langid = {english},
  annotation = {163 citations (Semantic Scholar/DOI) [2023-02-28] 117 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/V4E2WUG5/Zhao et al. - 2018 - Towards Pose Invariant Face Recognition in the Wil.pdf}
}

@inproceedings{zhaoRDCFaceRadialDistortion2020,
  title = {{{RDCFace}}: {{Radial Distortion Correction}} for {{Face Recognition}}},
  shorttitle = {{{RDCFace}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zhao, He and Ying, Xianghua and Shi, Yongjie and Tong, Xin and Wen, Jingsi and Zha, Hongbin},
  year = {2020},
  month = jun,
  pages = {7718--7727},
  publisher = {{IEEE}},
  address = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.00774},
  urldate = {2023-02-28},
  abstract = {The effects of radial lens distortion often appear in wideangle cameras of surveillance and safeguard systems, which may severely degrade performances of previous face recognition algorithms. Traditional methods for radial lens distortion correction usually employ line features in scenarios that are not suitable for face images. In this paper, we propose a distortion-invariant face recognition system called RDCFace, which only utilize the distorted images of faces, to directly alleviate the effects of radial lens distortion. RDCFace is an end-to-end trainable cascade network, which can learn rectification and alignment parameters to achieve a better face recognition performance without requiring supervision of facial landmarks and distortion parameters. We design sequential spatial transformer layers to optimize the correction, alignment, and recognition modules jointly. The feasibility of our method comes from implicitly using the statistics of the layout of face features learned from the large-scale face data. Extensive experiments indicate that our method is robust to distortion and gains significant improvements on several benchmarks including LFW, YTF, CFP, and RadialFace, a real distorted face dataset compared with state-of-the-art methods.},
  isbn = {978-1-72817-168-5},
  langid = {english},
  annotation = {9 citations (Semantic Scholar/DOI) [2023-02-28] 6 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/GJPAEKM9/Zhao et al. - 2020 - RDCFace Radial Distortion Correction for Face Rec.pdf}
}

@inproceedings{zhaoRegularFaceDeepFace2019,
  title = {{{RegularFace}}: {{Deep Face Recognition}} via {{Exclusive Regularization}}},
  shorttitle = {{{RegularFace}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zhao, Kai and Xu, Jingyi and Cheng, Ming-Ming},
  year = {2019},
  month = jun,
  pages = {1136--1144},
  publisher = {{IEEE}},
  address = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.00123},
  urldate = {2023-02-28},
  abstract = {We consider the face recognition task where facial images of the same identity (person) is expected to be closer in the representation space, while different identities be far apart. Several recent studies encourage the intra-class compactness by developing loss functions that penalize the variance of representations of the same identity. In this paper, we propose the `exclusive regularization' that focuses on the other aspect of discriminability \textendash{} the inter-class separability, which is neglected in many recent approaches. The proposed method, named RegularFace, explicitly distances identities by penalizing the angle between an identity and its nearest neighbor, resulting in discriminative face representations. Our method has intuitive geometric interpretation and presents unique benefits that are absent in previous works. Quantitative comparisons against prior methods on several open benchmarks demonstrate the superiority of our method. In addition, our method is easy to implement and requires only a few lines of python code on modern deep learning frameworks.},
  isbn = {978-1-72813-293-8},
  langid = {english},
  annotation = {85 citations (Semantic Scholar/DOI) [2023-02-28] 60 citations (Crossref) [2023-02-28]},
  note = {Very important},
  file = {/home/david/Zotero/storage/956Y47VN/Zhao et al. - 2019 - RegularFace Deep Face Recognition via Exclusive R.pdf}
}

@misc{zhengRingLossConvex2018,
  title = {Ring Loss: {{Convex Feature Normalization}} for {{Face Recognition}}},
  shorttitle = {Ring Loss},
  author = {Zheng, Yutong and Pal, Dipan K. and Savvides, Marios},
  year = {2018},
  month = feb,
  number = {arXiv:1803.00130},
  eprint = {1803.00130},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-02-28},
  abstract = {We motivate and present Ring loss, a simple and elegant feature normalization approach for deep networks designed to augment standard loss functions such as Softmax. We argue that deep feature normalization is an important aspect of supervised classification problems where we require the model to represent each class in a multi-class problem equally well. The direct approach to feature normalization through the hard normalization operation results in a non-convex formulation. Instead, Ring loss applies soft normalization, where it gradually learns to constrain the norm to the scaled unit circle while preserving convexity leading to more robust features. We apply Ring loss to large-scale face recognition problems and present results on LFW, the challenging protocols of IJB-A Janus, Janus CS3 (a superset of IJB-A Janus), Celebrity Frontal-Profile (CFP) and MegaFace with 1 million distractors. Ring loss outperforms strong baselines, matches state-of-the-art performance on IJB-A Janus and outperforms all other results on the challenging Janus CS3 thereby achieving state-of-the-art. We also outperform strong baselines in handling extremely low resolution face matching.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {172 citations (Semantic Scholar/arXiv) [2023-02-28]},
  note = {Comment: Accepted at CVPR 2018},
  file = {/home/david/Zotero/storage/W6C9MDNC/Zheng et al. - 2018 - Ring loss Convex Feature Normalization for Face R.pdf;/home/david/Zotero/storage/UVKQ7WE7/1803.html}
}

@inproceedings{zhengUncertaintyModelingContextualConnections2019,
  title = {Uncertainty {{Modeling}} of {{Contextual-Connections Between Tracklets}} for {{Unconstrained Video-Based Face Recognition}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Zheng, Jingxiao and Yu, Ruichi and Chen, Jun-Cheng and Lu, Boyu and Castillo, Carlos and Chellappa, Rama},
  year = {2019},
  month = oct,
  pages = {703--712},
  publisher = {{IEEE}},
  address = {{Seoul, Korea (South)}},
  doi = {10.1109/ICCV.2019.00079},
  urldate = {2023-02-28},
  abstract = {Unconstrained video-based face recognition is a challenging problem due to significant within-video variations caused by pose, occlusion and blur. To tackle this problem, an effective idea is to propagate the identity from highquality faces to low-quality ones through contextual connections, which are constructed based on context such as body appearance. However, previous methods have often propagated erroneous information due to lack of uncertainty modeling of the noisy contextual connections. In this paper, we propose the Uncertainty-Gated Graph (UGG), which conducts graph-based identity propagation between tracklets, which are represented by nodes in a graph. UGG explicitly models the uncertainty of the contextual connections by adaptively updating the weights of the edge gates according to the identity distributions of the nodes during inference. UGG is a generic graphical model that can be applied at only inference time or with end-to-end training. We demonstrate the effectiveness of UGG with state-of-theart results in the recently released challenging Cast Search in Movies and IARPA Janus Surveillance Video Benchmark dataset.},
  isbn = {978-1-72814-803-8},
  langid = {english},
  annotation = {9 citations (Semantic Scholar/DOI) [2023-02-28] 6 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/CMV557AD/Zheng et al. - 2019 - Uncertainty Modeling of Contextual-Connections Bet.pdf}
}

@inproceedings{zhongUnequalTrainingDeepFace2019,
  title = {Unequal-{{Training}} for {{Deep Face Recognition With Long-Tailed Noisy Data}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zhong, Yaoyao and Deng, Weihong and Wang, Mei and Hu, Jiani and Peng, Jianteng and Tao, Xunqiang and Huang, Yaohai},
  year = {2019},
  month = jun,
  pages = {7804--7813},
  issn = {2575-7075},
  doi = {10.1109/CVPR.2019.00800},
  abstract = {Large-scale face datasets usually exhibit a massive number of classes, a long-tailed distribution, and severe label noise, which undoubtedly aggravate the difficulty of training. In this paper, we propose a training strategy that treats the head data and the tail data in an unequal way, accompanying with noise-robust loss functions, to take full advantage of their respective characteristics. Specifically, the unequal-training framework provides two training data streams: the first stream applies the head data to learn discriminative face representation supervised by Noise Resistance loss; the second stream applies the tail data to learn auxiliary information by gradually mining the stable discriminative information from confusing tail classes. Consequently, both training streams offer complementary information to deep feature learning. Extensive experiments have demonstrated the effectiveness of the new unequal-training framework and loss functions. Better yet, our method could save a significant amount of GPU memory. With our method, we achieve the best result on MegaFace Challenge 2 (MF2) given a large-scale noisy training data set.},
  keywords = {and Body Pose,Deep Learning,Face,Face recognition,Gesture,Graphics processing units,Image and Video Synthesis,Memory management,Representation learning,Representation Learning,Resistance,Training,Training data},
  annotation = {90 citations (Semantic Scholar/DOI) [2023-02-28] 59 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/2UHS9QQA/Zhong et al. - 2019 - Unequal-Training for Deep Face Recognition With Lo.pdf;/home/david/Zotero/storage/4L9VAF56/stamp.html}
}

@misc{zhuBCNNBranchConvolutional2017,
  title = {B-{{CNN}}: {{Branch Convolutional Neural Network}} for {{Hierarchical Classification}}},
  shorttitle = {B-{{CNN}}},
  author = {Zhu, Xinqi and Bain, Michael},
  year = {2017},
  month = oct,
  number = {arXiv:1709.09890},
  eprint = {1709.09890},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1709.09890},
  urldate = {2023-02-13},
  abstract = {Convolutional Neural Network (CNN) image classifiers are traditionally designed to have sequential convolutional layers with a single output layer. This is based on the assumption that all target classes should be treated equally and exclusively. However, some classes can be more difficult to distinguish than others, and classes may be organized in a hierarchy of categories. At the same time, a CNN is designed to learn internal representations that abstract from the input data based on its hierarchical layered structure. So it is natural to ask if an inverse of this idea can be applied to learn a model that can predict over a classification hierarchy using multiple output layers in decreasing order of class abstraction. In this paper, we introduce a variant of the traditional CNN model named the Branch Convolutional Neural Network (B-CNN). A B-CNN model outputs multiple predictions ordered from coarse to fine along the concatenated convolutional layers corresponding to the hierarchical structure of the target classes, which can be regarded as a form of prior knowledge on the output. To learn with B-CNNs a novel training strategy, named the Branch Training strategy (BT-strategy), is introduced which balances the strictness of the prior with the freedom to adjust parameters on the output layers to minimize the loss. In this way we show that CNN based models can be forced to learn successively coarse to fine concepts in the internal layers at the output stage, and that hierarchical prior knowledge can be adopted to boost CNN models' classification performance. Our models are evaluated to show that the B-CNN extensions improve over the corresponding baseline CNN on the benchmark datasets MNIST, CIFAR-10 and CIFAR-100.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: 9 pages, 8 figures},
  file = {/home/david/Zotero/storage/GUGKM2JP/Zhu and Bain - 2017 - B-CNN Branch Convolutional Neural Network for Hie.pdf;/home/david/Zotero/storage/GIA2V3AC/1709.html}
}

@inproceedings{zhuLocalAdaptiveFaceRecognition2022,
  title = {Local-{{Adaptive Face Recognition}} via {{Graph-based Meta-Clustering}} and {{Regularized Adaptation}}},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zhu, Wenbin and Wang, Chien-Yi and Tseng, Kuan-Lun and Lai, Shang-Hong and Wang, Baoyuan},
  year = {2022},
  month = jun,
  pages = {20269--20278},
  publisher = {{IEEE}},
  address = {{New Orleans, LA, USA}},
  doi = {10.1109/CVPR52688.2022.01966},
  urldate = {2023-02-28},
  abstract = {Due to the rising concern of data privacy, it's reasonable to assume the local client data can't be transferred to a centralized server, nor their associated identity label is provided. To support continuous learning and fill the last-mile quality gap, we introduce a new problem setup called ``local-adaptive face recognition (LaFR)''. Leveraging the environment-specific local data after the deployment of the initial global model, LaFR aims at getting optimal performance by training local-adapted models automatically and un-supervisely, as opposed to fixing their initial global model. We achieve this by a newly proposed embedding cluster model based on Graph Convolution Network (GCN), which is trained via meta-optimization procedure. Compared with previous works, our meta-clustering model can generalize well in unseen local environments. With the pseudo identity labels from the clustering results, we further introduce novel regularization techniques to improve the model adaptation performance. Extensive experiments on racial and internal sensor adaptation demonstrate that our proposed solution is more effective for adapting face recognition models in each specific environment. Meanwhile, we show that LaFR can further improve the global model by a simple federated aggregation over the updated local models.},
  isbn = {978-1-66546-946-3},
  langid = {english},
  annotation = {5 citations (Semantic Scholar/DOI) [2023-04-18] 2 citations (Crossref) [2023-02-28]},
  file = {/home/david/Zotero/storage/UDBMBAGB/Zhu et al. - 2022 - Local-Adaptive Face Recognition via Graph-based Me.pdf}
}

@inproceedings{zhuWebFace260MBenchmarkUnveiling2021,
  title = {{{WebFace260M}}: {{A Benchmark Unveiling}} the {{Power}} of {{Million-Scale Deep Face Recognition}}},
  shorttitle = {{{WebFace260M}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zhu, Zheng and Huang, Guan and Deng, Jiankang and Ye, Yun and Huang, Junjie and Chen, Xinze and Zhu, Jiagang and Yang, Tian and Lu, Jiwen and Du, Dalong and Zhou, Jie},
  year = {2021},
  month = jun,
  pages = {10487--10497},
  publisher = {{IEEE}},
  address = {{Nashville, TN, USA}},
  doi = {10.1109/CVPR46437.2021.01035},
  urldate = {2023-02-28},
  abstract = {In this paper, we contribute a new million-scale face benchmark containing noisy 4M identities/260M faces (WebFace260M) and cleaned 2M identities/42M faces (WebFace42M) training data, as well as an elaborately designed time-constrained evaluation protocol. Firstly, we collect 4M name list and download 260M faces from the Internet. Then, a Cleaning Automatically utilizing SelfTraining (CAST) pipeline is devised to purify the tremendous WebFace260M, which is efficient and scalable. To the best of our knowledge, the cleaned WebFace42M is the largest public face recognition training set and we expect to close the data gap between academia and industry. Referring to practical scenarios, Face Recognition Under Inference Time conStraint (FRUITS) protocol and a test set are constructed to comprehensively evaluate face matchers.},
  isbn = {978-1-66544-509-2},
  langid = {english},
  annotation = {81 citations (Semantic Scholar/DOI) [2023-02-28] 40 citations (Crossref) [2023-02-28]},
  note = {Datasets},
  file = {/home/david/Zotero/storage/28AEPNHM/Zhu et al. - 2021 - WebFace260M A Benchmark Unveiling the Power of Mi.pdf}
}
