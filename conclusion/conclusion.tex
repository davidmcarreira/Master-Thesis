\documentclass[class=report, crop=false, a4paper, 12pt]{standalone}

%Packages import
\usepackage{../pkgs}


\begin{document}
\section{Main Outcomes}
\par This work studied different neural networks models in order to propose a better approach to TrustID's facial verification module. The main objective was to find a good compromise between the maximum accurate performance and the minimum computational cost possible without compromising safety. To that extent, four different sized models were suggested: MobileFaceNet, FaceNet, iResnet-18 and iResnet-SE-50. First they were compared in terms of their specifications: number of trainable parameters, mult-adds, number of trainable layers, embedding size, inference time, loss function and training dataset. Because the system is to be applied on a student monitoring scenario, the capturing device will induce high data variations, hence the model must be invariant to poses, illumination, quality, etc. With that in mind, appropriate benchmarks were designed to test the methods in a wide range of possible scenarios.

\par Initial tests, studied the pre-trained models in order to pick the best one to then verify if it can be refined. Analyzing the accuracies on all benchmarks, the ROC curves, TAR at different FAR values, DET curves and EER points, clearly showcased the best three models that could substitute TrustID0s solution: iResnet-SE-50, iResnet-18 and MobileFaceNet. iResnet-SE-50 and iResnet-18 are two contenders that performed very similarly and MobileFaceNet consistently placed third place, but is very close. Balancing MobileFaceNet's performance with its inherent lightweight characteristics, it was selected as the best starting point for fine-tuning, since it has much less trainable parameters, number of mult-adds operations and inference time for a small performance trade-off.

\par Two datasets were picked to fine-tune, each one with an objective in mind. QMUL-SurvFace was proposed as a way of improving the performance on the very difficult XQLFW benchmark. Then DigiFace-1M served as an experiment to test how the model would react to fully synthetic ethically sourced data, and if it would improve the performance on pose related benchmarks. First all the layers of the network, aside from the batch norm ones, were trained for three different ArcFace margins ($0.5$, $0.4$ and $0.3$). This approach revealed to be successful in increasing the XQLFW accuracy performance and discriminative power at lower FAR ($1e-3$ and $1e-2$ for $m=0.5$) for the pose group and XQLFW when trained with QMUL-SurvFace. However, that came along with worse results on the other benchmarks. On the same experiment, DigiFace-1M didn't produce any results worth mentioning, since it only deteriorated the model's results.  

\par With the performance degradation verified on the benchmarks for both datasets, two new training approaches that envolved freezing layers were employed. First, five initial layers were frozen, and the reamining were trained. Then, all the layers aside from the last two were frozen. The main intention would be to update less the weights responsible for capturing more general information and train only more detail specific layers, ultimately leading to the improvements verified on whole network training approach with results closer to the original model where they became worse. These tests proved that to not be the case. We concluded that, as would be theoretically expected, the lesser layers were trained, the closer the results are to the pre-trained model, but the XQLFW performance didn't improve, and again DigiFace-1M didn't show any enhancement whatsoever. This behaviour also leads us to believe that, because the model trained with less layers still shows performs degradation while not improving in the same domains where the model trained through all the layers does, that cand indicate that the model needs to be more complex in order to be able to adapt to the dataset's intricacies without tuning it over all the layers.

\par All in all, MobileFaceNet is a good trade-off between computational overhead and accurate results that performs better than TrustID on the designed benchmarks. 

\section{Future work}
After this work has been concluded, a better solution was suggested. However, there are some experiments that are worth investigating. Following the ethically sourced data philosophy, it would be interesting to train MobileFaceNet from scratch using only DigiFace-1M or an ensemble of datasets of synthetic data and consensual images to evaluate how the model would perform on the same benchmarks geared toward student monitoring scenarios.
\end{document}