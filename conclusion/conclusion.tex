\documentclass[class=report, crop=false, a4paper, 12pt]{standalone}

%Packages import
\usepackage{../pkgs}


\begin{document}
\section{Main Outcomes}
\par This work studies different neural networks models in order to propose an improved approach to TrustID's facial verification module. The main objective is to find an appropriate tradeoff between accurate performance and computational cost, without compromising safety. To that extent, four different models were suggested: MobileFaceNet, FaceNet, iResnet-18 and iResnet-SE-50. Firstly, they were compared in terms of their specifications: number of trainable parameters, mult-adds, number of trainable layers, embedding size, inference time, loss function and training dataset. Since the system is to be applied on an image-based student monitoring scenario, the capturing device will induce high data variations, hence the model must be invariant to poses, illumination, quality, etc. With that in mind, appropriate benchmarks were designed to test the methods in a wide range of possible scenarios.

\par With initial tests, we evaluated some pre-trained models in order to select one to then verify if it could be further refined. Analyzing the accuracies on all benchmarks, the ROC curves, TAR at different FAR values, DET curves and EER points, clearly showcased the best three models that could substitute TrustID's solution: iResnet-SE-50, iResnet-18 and MobileFaceNet. iResnet-SE-50 and iResnet-18 are two contenders that performed similarly and MobileFaceNet consistently scored third, but at a close distance. Balancing MobileFaceNet's performance with its inherent lightweight characteristics, it was selected for fine-tuning, since it has much less trainable parameters, number of mult-adds operations, inference time and NPUA for a minimal performance trade-off.

\par Two datasets were chosen for fine-tuning, each one with an objective in mind. QMUL-SurvFace was proposed as a way of improving the performance on the very challenging XQLFW benchmark. Then DigiFace-1M was utilized to test how the model would react to fully synthetic ethically sourced data, and if it would improve the performance on pose related benchmarks. First, all the layers of the network, aside from the batch norm ones, were trained for three different ArcFace margins ($0.5$, $0.4$ and $0.3$). This approach revealed to be successful in increasing the XQLFW accuracy performance and discriminative power at lower FAR ($1e-3$ and $1e-2$ for $m=0.5$) for the pose group and XQLFW when trained with QMUL-SurvFace. However, that came along with inferior results on the other benchmarks. In the same experiment, DigiFace-1M did not improve any accuracy results.  

\par With the performance degradation verified on the benchmarks for both datasets, two new training approaches that envolved freezing layers were employed. We concluded that, as would be theoretically expected, the lesser layers were trained, the closer the results are to the pre-trained model, but the XQLFW performance did not improve, and again DigiFace-1M did not show any enhancement whatsoever. This behaviour also leads us to believe that, since the model trained with less layers still shows performs degradation while not improving in the same domains where the model trained through all the layers does, that can indicate that the model needs to be more complex in order to be able to adapt to the dataset's intricacies without tuning it over all the layers.

\par All in all, MobileFaceNet is an adequate trade-off between computational overhead and accurate results, as proven by NPUA, that performs better than TrustID on the designed benchmarks. 

\section{Future work}
To further improve the current work, there are some open issues that are worth investigating. Following the ethically sourced data philosophy, it would be interesting to train MobileFaceNet from scratch using only DigiFace-1M or an ensemble of datasets of synthetic data and consensual images to evaluate how the model would perform on the same benchmarks geared toward student monitoring scenarios. 
\end{document}