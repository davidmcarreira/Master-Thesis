\documentclass[12pt]{article}
\linespread{1.4}

%Packages
% \usepackage{standalone}
% \usepackage{import}
\usepackage{hyperref}
\usepackage{indentfirst}
\usepackage{biblatex}
\usepackage[bottom]{footmisc}


%Custom commands
\newcommand{\red}[1]{\textcolor{red}{#1}} %To color red portions of text when needed

%Custom settings


%Bibtex resources
\addbibresource{Master-Thesis.bib}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% End of Preamble %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

% \import{cover/}{cover.tex}

\section{Introduction}
The following sections present a broad overview of the history of Artificial Intelligence (AI) without specifing or detailing too much on particular topics of this theme. The main objective is to present some context by presenting important articles in order for the reader to be able to have a notion of the progress that has been made over the past decades, the hardships encountered and how important AI is in our lifes. 
% - Turing test => Importance and evolution of machine learning => What led to CNN
%Introductory paragraph
\subsection{Philosophy}
On October 1950, in his article \textit{Computing Machinery and Intelligence}, Alan Turing questioned: "Can machines think?" \autocite{turingCOMPUTINGMACHINERYINTELLIGENCE1950}. At the time, the question was too meaningless to answer since not only the theory but also the technology available weren't devoleped enough. Noneotherless, Turing still predicted that in the future there would be computers that could effectively display human-like intelligence and discernment under the conditions proposed on the aforementioned article.

%Although Turing himself considered this specific inquiry too meaningless to answer, if the problem was viewed as whether a computer could perform adequately in a proposed game called \textit{The Imitation Game}, then machines could effectively be seen as a thought capable system. This game, nowadays also referred to as \textit{The Turing Test}, consists of typewritten questions and answers between 3 operators (two humans and one computer) to determine whom's the machine. Finally, and most importantly, Turing also predicted that in the future there would be computers that could play well said game, i.e, display human-like intelligence and discernment. Approximately seventy-three years have passed since the publication of the aforementioned article and, as predicted by Alan Turing, a number of systems have since passed the \textit{Turing Test} (insert citations here). 

\subsection{Relevant events to the birth of AI}
%Paragraph leading to the birth of AI
\par The breakthroughs of AI are predominant, and its importance in our everyday life is undeniable, but the theory behind it has several early roots. The interest in the area grew immensely with, for example, all the Turing's theoretical research, the proposal of the first mathematical Artificial Neuron model in 1943 by Warren McCulloch and Walter Pitts (based of binary inputs and output) \autocite{mccullochLOGICALCALCULUSIDEAS} and in 1949 Donald Hebb revolutionized the way the artificial neurons were treated by proposing what is known as the Hebb's rule\footnote{"When an axon of cell A is near enough to excite a cell B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that Aâ€™s efficiency, as one of the cells firing B, is increased." \autocite{hebbOrganizationBehaviorNeuropsychological1949}, meaning that when two neurons fire together their relation is strengthned.}. Taking into consideration the latter two, but specially Hebb's proposals, Belmont Farley and Westley Clark implementated in 1954 one of the first successful Artificial Neural Networks, composed of two layers of 128 artificial neurons with weighted inputs \autocite{farleySimulationSelforganizingSystems1954}. Over the span of approximately ten years, multiple researches were performed attempting to computerize the human brain. However, only in 1956, during the \textit{Dartmouth Summer Research Project on Artificial Intelligence} \autocite{mccarthyPROPOSALDARTMOUTHSUMMER}, was the term "Artificial Intelligence" firstly proposed by John McCarthy \textit{et al.}, beginning what is now considered to be the birth of AI \autocite{zhangStudyArtificialIntelligence2021}.  

\subsection{The stagnation of progress}
%Paragraph talking about the plateau of AI, leading  to CNN and DNN 
\par The succeeding two decades following the Dartmouth conference were filled with important developments, with special emphasis in the works published in 1958 by Frank Rosenblatt (generalized the Farley and Clark training to multi-layer networks rather than only two) \autocite{rosenblattPerceptronProbabilisticModel1958}, the 1959 General Problem Solver implemented by Allen Newel \textit{et al.} (a program intended to work as a universal problem solver that was capable of solving exercises such as the Towers of Hanoi\footnote{\textit{The Towers of Hanoi} is a game with 3 stacks of increasingly smaller disks. The goal is to stack them one at a time, so that they are arranged in a decreasing radius manner.}) \autocite{newell1959report} and the ELIZA a natural language processing tool program developed by Joseph Weizenbaum between 1964 and 1966 \autocite{weizenbaumELIZAComputerProgram1966}. Unfortunately, all the interest and development around AI met an unforseen halt in 1969 when Marvin Minksy and Seymour Papert released a book that reported on the problems of the Perceptron network. The overal sentiment regarding this topic of research was of doubt, due to mainly two issues raised by Minsky and Papert: the ANN couldn't stolve linear inseperatable problems (with special significance the XOR and XNOR functions) and there were limitations due to a lack of sufficient computing power to handle the processing of multi-layer large networks \autocite{minsky69perceptrons}.

\subsection{Better solutions to bring back AI}
Seventeen years after the publishings of Minksy, there was hope for AI again. Hinton \textit{et al.} (Backpropagation), Waibe \textit{et al.} (Time Delay Neural Network for speech recognition, considered a one-dimensional convolutional neural network), Zhang (first two-dimensional CNN - SIANN), LeCun \textit{et al.} (network for handwritten zipcode recognition and used the term "convolution" for the first time which is the original version of LeNet)

%Paragraph talking about the importance of AI/Deep Learning nowadays (specifically in image recognition)

%Paragraph talking about the id problem ==> TrustID

\newpage
\section{Background Section}

% - Deep Learning vs Shallow learning/Conventional Machine Learning
%     - What is deep learning
%         - Examples of techniques
%     - What is shallow learning/Conventional Machine Learning
%         - Examples of techniques
%     - Table comparing advantages and disadvantages (concluding why deep learning is better)

%     - Neural Networks

\subsection{The domains of Artificial Intelligence}
Artificial Intelligence is an extensive term that can be broadly described as the ability of a computer to simulate or mimic human-like behaviors, such as decision-making, judgement and, most importantly, learning \autocite{zhangStudyArtificialIntelligence2021}.

\newpage
\printbibliography

\end{document}