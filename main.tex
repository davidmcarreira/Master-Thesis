\documentclass[12pt]{article}
\linespread{1.4}

%Packages
% \usepackage{standalone}
% \usepackage{import}
\usepackage[bottom]{footmisc} %Footmisc needs to be loaded before hyperreff, otherwise the latter gets broken
\usepackage{hyperref}
\usepackage{indentfirst}
\usepackage[backref=true]{biblatex}
\usepackage{footnotebackref}
\usepackage{xcolor}
\usepackage{amsfonts}



%Custom commands
\newcommand{\red}[1]{\textcolor{red}{#1}} %To color red portions of text when needed

%Custom settings


%Bibtex resources
\addbibresource{Master-Thesis.bib}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% End of Preamble %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\tableofcontents

% \import{cover/}{cover.tex}

\newpage
\section{Background Section}

\subsection{History of AI}
The following sections present a broad overview of the history of Artificial Intelligence (AI) without specifing or detailing too much on particular topics of this theme. The main objective is to present some context by presenting important articles in order for the reader to be able to have a notion of the progress that has been made over the past decades, the hardships encountered and how important AI is in our lifes. 
% - Turing test => Importance and evolution of machine learning => What led to CNN
%Introductory paragraph
\subsubsection{Philosophy}
On October 1950, in his article \textit{Computing Machinery and Intelligence}, Alan Turing questioned: "Can machines think?" \autocite{turingCOMPUTINGMACHINERYINTELLIGENCE1950}. At the time, the question was too meaningless to answer since not only the theory but also the technology available weren't devoleped enough. Noneotherless, Turing still predicted that in the future there would be computers that could effectively display human-like intelligence and discernment under the conditions proposed on the aforementioned article.

%Although Turing himself considered this specific inquiry too meaningless to answer, if the problem was viewed as whether a computer could perform adequately in a proposed game called \textit{The Imitation Game}, then machines could effectively be seen as a thought capable system. This game, nowadays also referred to as \textit{The Turing Test}, consists of typewritten questions and answers between 3 operators (two humans and one computer) to determine whom's the machine. Finally, and most importantly, Turing also predicted that in the future there would be computers that could play well said game, i.e, display human-like intelligence and discernment. Approximately seventy-three years have passed since the publication of the aforementioned article and, as predicted by Alan Turing, a number of systems have since passed the \textit{Turing Test} (insert citations here). 

\subsubsection{Relevant events to the birth of AI}
%Paragraph leading to the birth of AI
\par The breakthroughs of AI are predominant, and its importance in our everyday life is undeniable, but the theory behind it has several early roots. The interest in the area grew immensely with, for example, all the Turing's theoretical research, the proposal of the first mathematical Artificial Neuron model in 1943 by Warren McCulloch and Walter Pitts (based of binary inputs and output) \autocite{mccullochLOGICALCALCULUSIDEAS} and in 1949 Donald Hebb revolutionized the way the artificial neurons were treated by proposing what is known as the Hebb's rule\footnote{"When an axon of cell A is near enough to excite a cell B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that A’s efficiency, as one of the cells firing B, is increased." \autocite{hebbOrganizationBehaviorNeuropsychological1949}, meaning that when two neurons fire together their relation is strengthned.}. Taking into consideration the latter two, but specially Hebb's proposals, Belmont Farley and Westley Clark implementated in 1954 one of the first successful Artificial Neural Networks (ANN), also called Perceptron, composed of two layers of 128 artificial neurons with weighted inputs \autocite{farleySimulationSelforganizingSystems1954}. Over the span of approximately ten years, multiple researches were performed attempting to computerize the human brain. However, only in 1956, during the \textit{Dartmouth Summer Research Project on Artificial Intelligence} \autocite{mccarthyPROPOSALDARTMOUTHSUMMER}, was the term "Artificial Intelligence" firstly proposed by John McCarthy \textit{et al.}, beginning what is now considered to be the birth of AI \autocite{zhangStudyArtificialIntelligence2021}.  

\subsubsection{The fading of general interest}
%Paragraph talking about the plateau of AI, leading to CNN and DNN 
\par The succeeding two decades following the Dartmouth conference were filled with important developments, with special emphasis in the works published in 1958 by Frank Rosenblatt (generalized the Farley and Clark training to multi-layer networks rather than only two) \autocite{rosenblattPerceptronProbabilisticModel1958}, the 1959 General Problem Solver implemented by Allen Newel \textit{et al.} (a program intended to work as a universal problem solver that was capable of solving exercises such as the Towers of Hanoi\footnote{\textit{The Towers of Hanoi} is a game with 3 stacks of increasingly smaller disks. The goal is to stack them one at a time, so that they are arranged in a decreasing radius manner.}) \autocite{newell1959report} and the ELIZA a natural language processing tool program developed by Joseph Weizenbaum between 1964 and 1966 \autocite{weizenbaumELIZAComputerProgram1966}. Unfortunately, part of the interest and development around AI met an unforseen fade after criticisms about the exagerated public funding \autocite{haenleinBriefHistoryArtificial2019} and the Marvin Minksy and Seymour Papert 1969 book \textit{The Perceptron: A Probabilistic Model for Infmation Storage and Organization in the Brain} that reported on the problems of the Perceptron network. The overal sentiments regarding this topic of research was of doubt and fear of no progress, mainly due to the spending and two issues raised by Minsky and Papert: the ANN couldn't solve linear inseperatable problems\footnote{That is, if two sets $X$ and $Y$ in $\mathbb{R}^d$ can't be divided by a hyperplane such that the elements of $X$ and $Y$ stay on opposing sides, then we're dealing with linear inseparable classes \autocite{elizondoLinearSeparabilityProblem2006}} and there were limitations due to a lack of sufficient computing power to handle the processing of multi-layer large networks \autocite{minsky69perceptrons}.

\subsubsection{Deep Learning - A better approach}
Minksy and Papert raised important questions, but it shouldn't have discouraged other researchers from further trying, since they failed to acknowledge alternative approaches that had already solved those exact problems. As previously stated, the model proposed by McCulloch and Pitts, later improved by the Farley-Clark implementation and, finally, Rosenblatt, couldn't handle linearly inseparable classes. A possible solution for cases like this started being studied in the 1960s \autocite{josephContributionsPerceptronTheory1960,rosenblattPrinciplesNeurodynamicsPerceptrons1962} and, although it didn't produce relevant results, in 1965 Alexey Ivakhnenko and Valentin Lapa \autocite{ivakhnenkoCyberneticPredictingDevices} were, indeed, successful in implementing what is nowadays considered to be the first deep learning network of its kind \autocite{schmidhuberDeepLearningNeural2015}. In 1971 Ivakhnenko also published an article describing a deep learning network with 8 layers that was already able to create hierarchical internal representations \autocite{4308320}.
\par The years progressed, in 1979 Kunihiko Fukushima introduced the first Convolutional Neural Network (CNN) in a structural sense, due to its similarity to the architecture of modern ones of this category. Ten years later, Yann LeCun \textit{et al.} applied for the first time a revolutionizing training algorithm called Backpropagation to a CNN \autocite{6795724}, creating what is now a pilar for most of the modern competition winning networks in computer vision \autocite{schmidhuberDeepLearningNeural2015} and employing the term "convolution" for the first known time \autocite{liSurveyConvolutionalNeural2020}. He also introduced the MNIST (\textbf{M}odified \textbf{N}ational \textbf{I}nstitute of \textbf{S}tandards and \textbf{T}echnology) dataset, a collection of handwritten digits \autocite{lecunGradientBasedLearningApplied1998}, that to this day is still one of the most famous benchmarks in Machine Learning. Backpropagation can be traced back many decades, but the modern version was first described by Seppo Linnainmaa (1970) \autocite{linnainmaa1970representation}, implemented for the first time by Stuart Dreyfus (1973) \autocite{Dreyfus1973383} and, finally in 1986, David Rummelhart \textit{et al.} popularized it in the Neural Network's (NN) domain by demonstrating the growing usefulness of internal representations \autocite{rumelhart1986learning}.

\subsubsection{The importance of Convolutional Neural Networks}
The study on Neural Networks continued and there were improvements on all types of architectures \autocite{hochreiterLongShortTermMemory1997,wengCresceptronSelforganizingNeural1992} with special highlight to pioneering Neural Networks processed by GPUs\footnote{Graphics Processing Unit} (standard NN in 2004 by \autocite{ohGPUImplementationNeural2004} and CNN in 2006 by \autocite{chellapillaHighPerformanceConvolutional}). But there's a well deserved particular attention related to the developments of CNNs due to their great performance in image related tasks when compared to others, as proven by LeCun in his 1998 paper \autocite{lecunGradientBasedLearningApplied1998}. Some relevant examples: in 2003 the MNIST record was broken by Patrice Simard \textit{et al.} \autocite{simardBestPracticesConvolutional2003}, achieving an error rate of 0.4$\%$ (whereas a non-convolutional neural network by the same authors took the second place with 0.7$\%$); three years later, the same benchmark had a new set low of 0.39$\%$ by Marc’Aurelio Ranzato \textit{et al.} \autocite{ranzatoEfficientLearningSparse2006}; in 2009 a CNN by Yang \textit{et al.} was the first network of this type to win an official international competition (TRECVid) \autocite{Yang2009}; a GPU implementation of a CNN \autocite{ciresanCommitteeNeuralNetworks2011} achieved superhuman vision performance in a competition (IJCNN 2011) in the \textit{German Traffic Sign Recognition Benchmark} with a 0.56$\%$ error rate (0.78$\%$ for the best human performance, 1.69$\%$ for the second-best neural network contestant and 3.86$\%$ for the best non-neural method \autocite{stallkampManVsComputer2012}). This last example conjoined with non-convolutional methods \autocite{rainaLargescaleDeepUnsupervised2009b,ciresanDeepBigSimple2010} and the previously cited \autocite{chellapillaHighPerformanceConvolutional,ohGPUImplementationNeural2004}, reinforces how fundamental GPUs were to further develop neural networks. To supplement even more the importance of CNNs and GPUs, only a year later, Alex Krizhevsky \textit{et al.} proposed a Deep CNN trained by GPUs that was the first one to win the ImageNet LSVRC-2012, achieving an error rate of 15.3$\%$ while the second place obtained 26.2$\%$ \autocite{krizhevskyImageNetClassificationDeep2012}. The year of 2012 was very important for Deep Learning, CNNs and Computer Vision, due to all the attention brought to many researches on this topic after several systems of this kind won image analysis competitions (\autocite{ciresanDeepNeuralNetworks2012,ciresanMitosisDetectionBreast2013} and the very important previously mentioned \autocite{krizhevskyImageNetClassificationDeep2012}), beginning what's considered to be the start of the new wave we're currently in of interest in Artificial Intelligence, specially in the aforesaid topics \autocite{liSurveyConvolutionalNeural2020}. 



%Paragraph talking about the importance of AI/Deep Learning nowadays (specifically in image recognition)

%Paragraph talking about the id problem ==> TrustID

% - Deep Learning vs Conventional Machine Learning
%     - What is deep learning
%         - Examples of techniques
%     - What is shallow learning/Conventional Machine Learning
%         - Examples of techniques
%     - Table comparing advantages and disadvantages (concluding why deep learning is better)

%     - Neural Networks

\subsection{Definitions and concepts}
\section{Related work}

\newpage
\printbibliography

\end{document}